[
{
	"uri": "https://asapphat.github.io/tanphat.worklog.github.io/5-workshop/5-workshop/5.1-workshop-overview/",
	"title": "Workshop Overview",
	"tags": [],
	"description": "",
	"content": "InsightHR Platform Overview InsightHR is a comprehensive serverless HR automation platform that demonstrates modern cloud-native application development on AWS. This workshop will guide you through building a production-ready application from scratch.\nProject Scope The InsightHR platform provides:\nEmployee Management System: Complete CRUD operations for employee records with advanced filtering by department, position, and status Performance Tracking: Quarterly performance scores with automatic calculation based on KPIs, completed tasks, and 360-degree feedback Attendance Management: Real-time check-in/check-out system with historical tracking and status monitoring AI-Powered Chatbot: Natural language query interface using AWS Bedrock (Claude 3 Haiku) for intelligent data insights Analytics Dashboard: Interactive visualizations with charts, tables, and export capabilities Role-Based Access Control: Three-tier access system (Admin, Manager, Employee) with appropriate data filtering Secure Authentication: Email/password and Google OAuth integration via AWS Cognito Architecture Overview â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\râ”‚ User Browser â”‚\râ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\râ”‚\râ–¼\râ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\râ”‚ CloudFront CDN â”‚\râ”‚ Custom Domain: insight-hr.io.vn â”‚\râ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\râ”‚\râ–¼\râ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\râ”‚ S3 Static Website â”‚\râ”‚ React SPA (Vite + TypeScript) â”‚\râ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\râ”‚\râ–¼\râ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\râ”‚ API Gateway (REST) â”‚\râ”‚ Cognito Authorizer â”‚\râ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\râ”‚\râ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\râ–¼ â–¼ â–¼\râ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\râ”‚ Lambda â”‚ â”‚ Lambda â”‚ â”‚ Lambda â”‚\râ”‚ Auth â”‚ â”‚ Employees â”‚ â”‚ Chatbot â”‚\râ””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\râ”‚ â”‚ â”‚\râ–¼ â–¼ â–¼\râ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\râ”‚ DynamoDB â”‚\râ”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\râ”‚ â”‚ Users â”‚ â”‚Employees â”‚ â”‚ Scores â”‚ â”‚Attendanceâ”‚ â”‚\râ”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚\râ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\râ”‚\râ–¼\râ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\râ”‚ AWS Bedrock â”‚\râ”‚ (Claude 3 Haiku Model) â”‚\râ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ Technology Stack Frontend:\nReact 18 with TypeScript Vite 7.2 (build tool) Tailwind CSS 3.4 (Frutiger Aero theme) Zustand 5.0 (state management) React Hook Form + Zod (form validation) Recharts 3.4 (data visualization) Backend:\nPython 3.11 (Lambda runtime) AWS Lambda (serverless compute) AWS API Gateway (REST API) AWS DynamoDB (NoSQL database) AWS Cognito (authentication) AWS Bedrock (AI/ML) Infrastructure:\nAWS S3 (static hosting) AWS CloudFront (CDN) AWS Route53 (DNS) AWS CloudWatch (monitoring) AWS IAM (security) Key Features 1. Fully Serverless Architecture No EC2 instances to manage Automatic scaling based on demand Pay-per-use pricing model High availability built-in 2. Modern Development Stack TypeScript for type safety React for responsive UI Python for backend logic Infrastructure as Code principles 3. Production-Ready Custom domain with SSL CloudWatch monitoring Synthetic canaries for testing Role-based access control 4. Cost-Effective DynamoDB on-demand pricing Lambda free tier eligible Minimal monthly costs (~$2-5) No idle resource charges Workshop Structure This workshop is divided into 11 modules:\nWorkshop Overview (Current) - Understanding the project scope Prerequisites - Setting up your environment Project Architecture - Deep dive into system design Setup AWS Environment - Configuring AWS account and credentials Database Setup - Creating and populating DynamoDB tables Authentication Service - Implementing Cognito and auth Lambda functions Backend Services - Building employee, performance, and chatbot APIs Frontend Development - Creating the React application Deployment - Deploying to S3 and CloudFront Testing \u0026amp; Monitoring - Setting up CloudWatch and canaries Cleanup - Removing resources to avoid charges Learning Objectives By the end of this workshop, you will be able to:\nDesign and implement serverless architectures on AWS Build RESTful APIs using Lambda and API Gateway Model data effectively in DynamoDB Implement authentication with AWS Cognito Integrate AI capabilities using AWS Bedrock Deploy static websites with S3 and CloudFront Monitor applications with CloudWatch Apply security best practices with IAM Optimize costs for serverless applications Prerequisites Check Before proceeding, ensure you have:\nâœ… AWS Account with admin access âœ… AWS CLI installed and configured âœ… Node.js 18+ and npm installed âœ… Python 3.11+ installed âœ… Basic understanding of React and TypeScript âœ… Familiarity with REST APIs âœ… Text editor or IDE (VS Code recommended) Estimated Costs Running this workshop will incur minimal costs:\nService Estimated Cost DynamoDB $0.50/month Lambda Free tier S3 + CloudFront $1-2/month API Gateway $0.10/month Bedrock $0.0004/query Total $2-5/month Remember to complete the cleanup module at the end to avoid ongoing charges.\nNext Steps Ready to begin? Let\u0026rsquo;s move to the Prerequisites section to set up your development environment.\n"
},
{
	"uri": "https://asapphat.github.io/tanphat.worklog.github.io/",
	"title": "Internship Report",
	"tags": [],
	"description": "",
	"content": "Internship Report Student Information: Full Name: BÃ¹i Táº¥n PhÃ¡t\nPhone Number: 0815928038\nEmail: btfat3103@gmail.com\nUniversity: FPT University\nMajor: Artificial Inteligent\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 12/08/2025 to 12/11/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "
},
{
	"uri": "https://asapphat.github.io/tanphat.worklog.github.io/1-worklog/",
	"title": "Worklog",
	"tags": [],
	"description": "",
	"content": "On this page, you will find my complete worklog documenting my AWS First Cloud Journey internship from September 8 to December 19, 2025. This represents a 12-week comprehensive learning experience covering AWS fundamentals, advanced services, and real-world application development through the InsightHR-1 project.\nWeek 1: Onboarding and AWS Foundation\nWeek 2: Development Environment Setup and AWS Cloud Day 2025\nWeek 3: VPC and EC2 Fundamentals with Static Website Deployment\nWeek 4: AWS Security Services and Identity Management\nWeek 5: Data Protection, Backup Services, and Messaging\nWeek 6: RDS Database Management, Auto Scaling, and CloudWatch Monitoring\nWeek 7: Serverless Architecture - Lambda, S3, and DynamoDB\nWeek 8: Midterm Review and Architectural Design\nWeek 9: Advanced Monitoring with CloudWatch and Grafana\nWeek 10: AWS Lambda with Python and Serverless Dashboard Development\nWeek 11: Lambda Functions Development and Docker Containerization\nWeek 12: Amazon Bedrock Chatbot and Project Finalization\n"
},
{
	"uri": "https://asapphat.github.io/tanphat.worklog.github.io/3-blogstranslated/3.1-blog1/",
	"title": "AWS Weekly Roundup: Strands Agents ",
	"tags": [],
	"description": "",
	"content": "AWS News Blog AWS Weekly Roundup: Strands Agents 1M+ downloads, Cloud Club Captain, AI Agent Hackathon, and more (September 15, 2025) by Channy Yun (ìœ¤ì„ì°¬) on 15 SEP 2025 in Amazon CloudFront, Amazon EC2 Mac Instances, AWS Cloud Development Kit, AWS CloudTrail, AWS Lambda, AWS Trainium, News, Open Source, Startup\nLast week, Strands Agents, AWS open source for agentic AI SDK, just hit 1 million downloads and earned 3,000+ GitHub Stars less than 4 months since launching as a preview in May 2025. With Strands Agents, you can build production-ready, multi-agent AI systems in a few lines of code.\nWe\u0026rsquo;ve continuously improved features including support for multi-agent patterns, A2A protocol, and Amazon Bedrock AgentCore. You can use a collection of sample implementations to help you get started with building intelligent agents using Strands Agents. We always welcome your contribution and feedback to our project including bug reports, new features, corrections, or additional documentation.\nHere is the latest research article of Amazon Science about the future of agentic AI and questions that scientists are asking about agent-to-agent communications, contextual understanding, common sense reasoning, and more. You can understand the technical topic of agentic AI with relatable examples, including one about our personal behaviors about leaving doors open or closed, locked or unlocked.\nLast week\u0026rsquo;s Launches Here are some launches that got my attention:\nAmazon EC2 M4 and M4 Pro Mac instances â€“ New M4 Mac instances offer up to 20% better application build performance compared to M2 Mac instances, while M4 Pro Mac instances deliver up to 15% better application build performance compared to M2 Pro Mac instances. These instances are ideal for building and testing applications for Apple platforms such as iOS, macOS, iPadOS, tvOS, watchOS, visionOS, and Safari. LocalStack integration in Visual Studio Code (VS Code) â€“ You can use LocalStack to locally emulate and test your serverless applications using the familiar VS Code interface without switching between tools or managing complex setup, thus simplifying your local serverless development process. AWS Cloud Development Kit (AWS CDK) Refactor (Preview) â€“ You can rename constructs, move resources between stacks, and reorganize CDK applications while preserving the state of deployed resources. By using AWS CloudFormation\u0026rsquo;s refactor capabilities with automated mapping computation, CDK Refactor eliminates the risk of unintended resource replacement during code restructuring. AWS CloudTrail MCP Server â€“ New AWS CloudTrail MCP server allows AI assistants to analyze API calls, track user activities, and perform advanced security analysis across your AWS environment through natural language interactions. You can explore more AWS MCP servers for working with AWS service resources. Amazon CloudFront support for IPv6 origins â€“ Your applications can send IPv6 traffic all the way to their origins, allowing them to meet their architectural and regulatory requirements for IPv6 adoption. End-to-end IPv6 support improves network performance for end users connecting over IPv6 networks, and also removes concerns for IPv4 address exhaustion for origin infrastructure. For a full list of AWS announcements, be sure to keep an eye on the Whatâ€™s New with AWS? page.\nOther AWS News Here are some additional news items that you might find interesting:\nA city in the palm of your hand â€“ Check out this interactive feature that explains how our AWS Trainium chip designers think like city planners, optimizing every nanometer to move data at near light speed. Measuring the effectiveness of software development tools and practices â€“ Read how Amazon developers that identified specific challenges before adopting AI tools cut costs by 15.9% year-over-year using our cost-to-serve-software framework (CTS-SW). They deployed more frequently and reduced manual interventions by 30.4% by focusing on the right problems first. Become an AWS Cloud Club Captain â€“ Join a growing network of student cloud enthusiasts by becoming an AWS Cloud Club Captain! As a Captain, youâ€™ll get to organize events and building cloud communities while developing leadership skills. Application window is open September 1-28, 2025. Upcoming AWS Events Check your calendars and sign up for these upcoming AWS events as well as AWS re:Invent and AWS Summits:\nAWS AI Agent Global Hackathon â€“ This is your chance to dive deep into our powerful generative AI stack and create something truly awesome. From September 8 to October 20, you have the opportunity to create AI agents using AWS suite of AI services, competing for over $45,000 in prizes and exclusive go-to-market opportunities. AWS Gen AI Lofts â€“ You can learn AWS AI products and services with exclusive sessions and meet industry-leading experts, and have valuable networking opportunities with investors and peers. Register in your nearest city: Mexico City (September 30â€“October 2), Paris (October 7â€“21), London (Oct 13â€“21), and Tel Aviv (November 11â€“19). AWS Community Days â€“ Join community-led conferences that feature technical discussions, workshops, and hands-on labs led by expert AWS users and industry leaders from around the world: Aotearoa and Poland (September 18), South Africa (September 20), Bolivia (September 20), Portugal (September 27), Germany (October 7), and Hungary (October 16). You can browse all upcoming AWS events and AWS startup events.\nThatâ€™s all for this week. Check back next Monday for another Weekly Roundup!\nâ€” Channy\nTAGS: Week in Review\nChanny Yun (ìœ¤ì„ì°¬) Channy is a Lead Blogger of AWS News Blog and Principal Developer Advocate for AWS Cloud. As an open web enthusiast and blogger at heart, he loves community-driven learning and sharing of technology.\n"
},
{
	"uri": "https://asapphat.github.io/tanphat.worklog.github.io/1-worklog/1.1-week1/",
	"title": "Week 1 Worklog",
	"tags": [],
	"description": "",
	"content": "Week Duration: September 8 - 14, 2025\nWeek 1 Objectives: Connect and get acquainted with members of First Cloud Journey. Set up development environment and understand FCJ program workflows. Create AWS Free Tier account and explore core AWS services. Establish documentation workflow for the internship journey. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Monday - Start Module 1: - Lab 1 Setting up AWS Account: 09/08/2025 09/08/2025 https://youtu.be/HxYZAK1coOI?si=QzTgLWe-kpZEJ4m8 https://000001.awsstudygroup.com/vi/ Tuesday - Lab 7 Cost Management with AWS Account: - Lab 9 Request Support with Amazon Support: 09/09/2025 09/09/2025 https://000007.awsstudygroup.com/ https://000009.awsstudygroup.com/ Wednesday - Module 2 Theory: 09/10/2025 09/10/2025 https://youtu.be/O9Ac_vGHquM?si=NX9x5PJxBIFDO4dL Thursday - Lab 3: Amazon VPC and AWS Site-to-Site VPN Workshop 09/11/2025 09/14/2025 https://000003.awsstudygroup.com/ Friday - Finalize lab exercises 09/12/2025 09/12/2025 AWS Labs Documentation Saturday - Finalize lab exercises and consolidate learnings 09/13/2025 09/13/2025 AWS Workshop Materials Week 1 Achievements: Completed Module 1 - AWS Foundation:\nSuccessfully completed Module 1 theory and fundamentals Understood AWS cloud infrastructure basics Learned core AWS concepts and terminology Established foundation for advanced AWS topics Completed Lab 1 - Setting up AWS Account:\nCreated AWS Free Tier account successfully Configured AWS management console Set up initial security settings and access keys Understood AWS billing and cost management basics Completed Lab 7 - Cost Management with AWS Account:\nLearned AWS cost management strategies Configured billing alerts and notifications Understood cost optimization principles Practiced budget tracking and forecasting Completed Lab 9 - Request Support with Amazon Support:\nLearned AWS support plans and options Understood support case management Practiced creating and tracking support tickets Gained knowledge of AWS Support resources Completed Module 2 - Advanced AWS Concepts:\nMastered advanced AWS infrastructure concepts Deepened understanding of AWS architecture patterns Built upon Module 1 foundational knowledge Prepared for hands-on lab exercises Completed Lab 3 - Amazon VPC and AWS Site-to-Site VPN Workshop:\nSuccessfully set up Virtual Private Cloud (VPC) Configured VPC subnets and routing Established Site-to-Site VPN connections Practiced hybrid network connectivity Understood VPC security groups and network ACLs Key Technical Skills Acquired:\nAWS account setup and configuration AWS management console navigation Cost management and optimization VPC and networking fundamentals Support and troubleshooting processes AWS security best practices for beginners Documentation and Organization:\nSet up comprehensive worklog and documentation structure Created organized learning material repository Established workflow for tracking progress Prepared foundation for project work Foundation Established:\nStrong baseline knowledge of core AWS services Ready for project-based learning in Week 2 Prepared for EC2 and networking deep-dive Next Week Goals: Deep dive into specific AWS services based on project requirements. Continue building documentation and refining workflow processes. Explore more advanced AWS features and best practices. "
},
{
	"uri": "https://asapphat.github.io/tanphat.worklog.github.io/4-eventparticipated/4.1-event1/",
	"title": "Event 1",
	"tags": [],
	"description": "",
	"content": "Vietnam Cloud Day 2025: Ho Chi Minh City Connect Edition for Builders - GenAI and Data Track Location: AWS Event Hall, L26 Bitexco Tower, HCMC\nDate: Thursday, September 18, 2025\nLearning Report: \u0026ldquo;Vietnam Cloud Day 2025: GenAI and Data Track\u0026rdquo; Event Objectives Provide an overview of Agentic AI and AWS\u0026rsquo;s strategic vision Understand how to build a Unified Data Foundation to effectively support AI activities and Data Analytics on AWS Analyze in detail the GenAI deployment roadmap, AI Agent Architecture models, and challenges in moving them to production Study the AI-Driven Development Lifecycle (AI-DLC) model Master core principles of Security, Risk Management, and Responsible AI in Generative AI Introduce new AWS services designed to support AI Agents and maximize business productivity Speaker List Jun Kai Loke - Solution Architecture Expert for AI/ML, AWS Kien Nguyen - Solution Architect, AWS Tamelly Lim - Storage Solution Architecture Expert, AWS Binh Tran - Senior Solution Architect, AWS Taiki Dang - Solution Architect, AWS Christal Poon - Solution Architecture Expert, AWS Key Content Highlights Overview of Agentic AI â€“ Jun Kai Loke Agentic AI is an important strategic trend focusing on creating systems with autonomous capabilities, minimizing human intervention, and automating complex processes at a deep level Real-world examples of successful applications: Katalon, Apero, Techcom Securities Amazon Bedrock serves as the core platform for AI development, supporting: Secure deployment at scale Integration of tools and memory capabilities End-to-end monitoring Building a Unified Data Foundation on AWS â€“ Kien Nguyen Current Challenges: Many businesses struggle with GenAI deployment due to unprepared data platforms (only 52% of CDOs assess their data platforms as ready, according to HBR), mainly due to data silos, people silos, and separate business units End-to-End Data Strategy includes three interacting components: Producers, Foundations, and Consumers Essential AWS Data Services: Amazon Bedrock (GenAI platform) Databases (RDS and specialized services supporting vector search) Analytics \u0026amp; ML (SageMaker, Unified Studio) Data \u0026amp; AI Governance Lake House Architecture (S3, Redshift Managed Storage, Iceberg Open API) Amazon DataZone (Data management and sharing) GenAI Roadmap \u0026amp; AI Agent Architecture â€“ Jun Kai Loke \u0026amp; Tamelly Lim AI Agents building blueprint includes: Model \u0026amp; application capabilities, and tool framework AWS introduces Amazon Bedrock AgentCore to address challenges in moving Agents to production AgentCore includes: Agent Core Runtime, Agent Core Gateway, Memory, Agent Browser and Code Interpreter, aiming to enhance security and scalability AI-Driven Development Lifecycle (AI-DLC) â€“ Binh Tran AI-DLC is a new software development model, maximally automated by AI, consisting of 3 stages:\nInception: Define context, outline user stories, and plan with work units Construction: Code + test, supplement architecture, deploy Infrastructure as Code (IaC) and test Operation: Deploy to production using IaC and incident management Security of Generative AI Applications â€“ Taiki Dang Essential Security Elements: Compliance \u0026amp; Governance, Legal \u0026amp; Privacy, Controls, Risk Management, and Resilience Risk Analysis by Layer: End-user risks (Hallucination, IP, Legal), fine-tuning risks (data retention), and model provider risks (training data, model construction) Risk Mitigation Strategies: Use Prompt engineering, Fine-tuning, Retrieval-Augmented Generation (RAG), parameter adjustment, Bedrock Guardrails, and prompt security Must apply standards such as AWS Well-Architected, MITRE ATLAS, OWASP Top 10 for LLM Apps, NIST AI 600-1, ISO 42001, and EU AI Act AI Agents: Boosting Business Productivity â€“ Christal Poon Introduction to AI Agent types: Specialized Agents, Fully-managed Agents, and DIY Agents Productivity support services: Amazon QuickSight (for business analytics) and Amazon Q (providing Dashboards, Reports, Executive summaries, and AI Agent scenarios) What I Learned Mindset \u0026amp; Strategy Agentic AI is the next evolution of automation, moving towards autonomous systems with reduced human oversight Strong data platforms (based on S3, Lake House, Bedrock, SageMaker) are prerequisites for successful GenAI deployment AI-DLC provides a modern methodology that automates the entire development cycle from planning, coding, testing to deployment Architecture \u0026amp; Technology Understanding AI Agent architecture and Amazon Bedrock AgentCore solutions addresses challenges around security and scalability in production environments Security must be integrated at every level of the AI stack, from data, model to end-user application, while complying with international standards and regulations Technology Application AWS is significantly investing and expanding the AI Agents \u0026amp; Enterprise AI ecosystem through services like Amazon Q and QuickSuite (coming soon) Application to Work Process Optimization: Research and integrate AI Agents into repetitive business tasks to increase efficiency Quality Management: Use Amazon Bedrock, Amazon Q, and Guardrails to control quality, ensure safety, and minimize model \u0026ldquo;hallucination\u0026rdquo; Infrastructure Building: Ensure building a unified data platform with clear governance before launching GenAI projects Applying AI-DLC: Experiment with AI-DLC software development model in internal projects to accelerate deployment Business Analysis: Use QuickSight and Amazon Q to quickly create dashboards and insights for leadership and business teams Event Experience The workshop provided clear and practical insight into the transition from traditional automation to the Agentic AI era. Speakers\u0026rsquo; presentations provided specific guidance for the GenAI adoption journey in Vietnam. The combination of AgentCore, Bedrock, AI-DLC, and Amazon Q painted a comprehensive and powerful picture of the next generation of AI for enterprises, from data platforms, application development, to security and operations.\nAdd your event photos here Overall, the event not only provided in-depth technical knowledge but also helped shift thinking about how to strategically, responsibly, and safely integrate AI into all aspects of business.\nSpeakers Jignesh Shah â€“ Director, Open Source Databases Erica Liu â€“ Sr. GTM Specialist, AppMod Fabrianne Effendi â€“ Assc. Specialist SA, Serverless Amazon Web Services Key Highlights Identifying the drawbacks of legacy application architecture Long product release cycles â†’ Lost revenue/missed opportunities Inefficient operations â†’ Reduced productivity, higher costs Non-compliance with security regulations â†’ Security breaches, loss of reputation Transitioning to modern application architecture â€“ Microservices Migrating to a modular system â€” each function is an independent service communicating via events, built on three core pillars:\nQueue Management: Handle asynchronous tasks Caching Strategy: Optimize performance Message Handling: Flexible inter-service communication Domain-Driven Design (DDD) Four-step method: Identify domain events â†’ arrange timeline â†’ identify actors â†’ define bounded contexts Bookstore case study: Demonstrates real-world DDD application Context mapping: 7 patterns for integrating bounded contexts Event-Driven Architecture 3 integration patterns: Publish/Subscribe, Point-to-point, Streaming Benefits: Loose coupling, scalability, resilience Sync vs async comparison: Understanding the trade-offs Compute Evolution Shared Responsibility Model: EC2 â†’ ECS â†’ Fargate â†’ Lambda Serverless benefits: No server management, auto-scaling, pay-for-value Functions vs Containers: Criteria for appropriate choice Amazon Q Developer SDLC automation: From planning to maintenance Code transformation: Java upgrade, .NET modernization AWS Transform agents: VMware, Mainframe, .NET migration Key Takeaways Design Mindset Business-first approach: Always start from the business domain, not the technology Ubiquitous language: Importance of a shared vocabulary between business and tech teams Bounded contexts: Identifying and managing complexity in large systems Technical Architecture Event storming technique: Practical method for modeling business processes Use event-driven communication instead of synchronous calls Integration patterns: When to use sync, async, pub/sub, streaming Compute spectrum: Criteria for choosing between VM, containers, and serverless Modernization Strategy Phased approach: No rushing â€” follow a clear roadmap 7Rs framework: Multiple modernization paths depending on the application ROI measurement: Cost reduction + business agility Applying to Work Apply DDD to current projects: Event storming sessions with business teams Refactor microservices: Use bounded contexts to define service boundaries Implement event-driven patterns: Replace some sync calls with async messaging Adopt serverless: Pilot AWS Lambda for suitable use cases Try Amazon Q Developer: Integrate into the dev workflow to boost productivity Event Experience Attending the â€œGenAI-powered App-DB Modernizationâ€ workshop was extremely valuable, giving me a comprehensive view of modernizing applications and databases using advanced methods and tools. Key experiences included:\nLearning from highly skilled speakers Experts from AWS and major tech organizations shared best practices in modern application design. Through real-world case studies, I gained a deeper understanding of applying DDD and Event-Driven Architecture to large projects. Hands-on technical exposure Participating in event storming sessions helped me visualize how to model business processes into domain events. Learned how to split microservices and define bounded contexts to manage large-system complexity. Understood trade-offs between synchronous and asynchronous communication and integration patterns like pub/sub, point-to-point, streaming. Leveraging modern tools Explored Amazon Q Developer, an AI tool for SDLC support from planning to maintenance. Learned to automate code transformation and pilot serverless with AWS Lambda to improve productivity. Networking and discussions The workshop offered opportunities to exchange ideas with experts, peers, and business teams, enhancing the ubiquitous language between business and tech. Real-world examples reinforced the importance of the business-first approach rather than focusing solely on technology. Lessons learned Applying DDD and event-driven patterns reduces coupling while improving scalability and resilience. Modernization requires a phased approach with ROI measurement; rushing the process can be risky. AI tools like Amazon Q Developer can significantly boost productivity when integrated into the current workflow. Some event photos Overall, the event not only provided technical knowledge but also helped me reshape my thinking about application design, system modernization, and cross-team collaboration.\n"
},
{
	"uri": "https://asapphat.github.io/tanphat.worklog.github.io/5-workshop/5-workshop/5.2-prerequisite/",
	"title": "Prerequisites",
	"tags": [],
	"description": "",
	"content": "Prerequisites for InsightHR Workshop Before starting this workshop, ensure you have the following tools and accounts set up.\n1. AWS Account You\u0026rsquo;ll need an AWS account with appropriate permissions to create and manage resources.\nRequired AWS Services Access:\nIAM (Identity and Access Management) DynamoDB Lambda API Gateway S3 CloudFront Cognito Bedrock CloudWatch Route53 (optional, for custom domain) Estimated Costs: $2-5/month during development\nIf you\u0026rsquo;re using AWS Free Tier, many services in this workshop are covered. However, some services like Bedrock may incur small charges.\n2. AWS CLI Install and configure the AWS Command Line Interface.\nInstallation:\nWindows:\nmsiexec.exe /i https://awscli.amazonaws.com/AWSCLIV2.msi macOS:\ncurl \u0026#34;https://awscli.amazonaws.com/AWSCLI2.pkg\u0026#34; -o \u0026#34;AWSCLIV2.pkg\u0026#34; sudo installer -pkg AWSCLIV2.pkg -target / Linux:\ncurl \u0026#34;https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\u0026#34; -o \u0026#34;awscliv2.zip\u0026#34; unzip awscliv2.zip sudo ./aws/install Configuration:\naws configure Enter your:\nAWS Access Key ID AWS Secret Access Key Default region (e.g., ap-southeast-1) Default output format (e.g., json) Verify Installation:\naws --version # Expected output: aws-cli/2.x.x Python/3.x.x ... 3. Node.js and npm Required for frontend development.\nMinimum Version: Node.js 18+\nInstallation:\nDownload from nodejs.org or use a version manager:\nUsing nvm (recommended):\n# Install nvm curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.0/install.sh | bash # Install Node.js nvm install 18 nvm use 18 Verify Installation:\nnode --version # Expected: v18.x.x or higher npm --version # Expected: 9.x.x or higher 4. Python Required for Lambda function development.\nMinimum Version: Python 3.11+\nInstallation:\nDownload from python.org or use your system\u0026rsquo;s package manager.\nVerify Installation:\npython --version # or python3 --version # Expected: Python 3.11.x or higher Install pip (if not included):\npython -m ensurepip --upgrade 5. Text Editor / IDE Choose your preferred development environment:\nRecommended: Visual Studio Code\nDownload from code.visualstudio.com Install recommended extensions: AWS Toolkit Python ESLint Prettier Tailwind CSS IntelliSense Alternatives:\nPyCharm Sublime Text Atom WebStorm 6. Git (Optional but Recommended) For version control and accessing code repositories.\nInstallation:\nDownload from git-scm.com\nVerify Installation:\ngit --version # Expected: git version 2.x.x 7. Required Knowledge JavaScript/TypeScript:\nBasic syntax and ES6+ features Async/await and Promises React fundamentals (components, hooks, state) Python:\nBasic syntax and data structures Functions and error handling Working with JSON AWS Concepts:\nBasic understanding of cloud computing Familiarity with AWS Console Understanding of serverless architecture (helpful but not required) Web Development:\nHTML/CSS basics REST API concepts HTTP methods (GET, POST, PUT, DELETE) 8. AWS Account Setup Create IAM User For security best practices, create an IAM user instead of using root credentials:\nSign in to AWS Console\nNavigate to IAM service\nClick \u0026ldquo;Users\u0026rdquo; â†’ \u0026ldquo;Add users\u0026rdquo;\nEnter username (e.g., insighthr-admin)\nSelect \u0026ldquo;Programmatic access\u0026rdquo; and \u0026ldquo;AWS Management Console access\u0026rdquo;\nAttach policies:\nAdministratorAccess (for workshop purposes) Or create a custom policy with required permissions Download credentials (Access Key ID and Secret Access Key)\nConfigure AWS CLI with these credentials\nEnable Required Services Ensure the following services are available in your region:\nâœ… AWS Lambda âœ… Amazon DynamoDB âœ… Amazon API Gateway âœ… Amazon S3 âœ… Amazon CloudFront âœ… Amazon Cognito âœ… Amazon Bedrock (check regional availability) âœ… Amazon CloudWatch Recommended Region: ap-southeast-1 (Singapore) - All services are available and latency is good for Southeast Asia.\n9. Bedrock Model Access AWS Bedrock requires explicit model access request.\nSteps to Enable:\nGo to AWS Console â†’ Amazon Bedrock Navigate to \u0026ldquo;Model access\u0026rdquo; in the left sidebar Click \u0026ldquo;Manage model access\u0026rdquo; Find \u0026ldquo;Claude 3 Haiku\u0026rdquo; by Anthropic Check the box and click \u0026ldquo;Request model access\u0026rdquo; Wait for approval (usually instant for Haiku) Without Bedrock access, the AI chatbot feature won\u0026rsquo;t work. However, you can still complete the rest of the workshop.\n10. Optional: Domain Name If you want to use a custom domain (like insight-hr.io.vn):\nPurchase a domain from a registrar (e.g., Route53, GoDaddy, Namecheap) Have access to DNS management Budget for SSL certificate (free with AWS Certificate Manager) Pre-Workshop Checklist Before proceeding to the next section, verify you have:\nAWS account with admin access AWS CLI installed and configured Node.js 18+ and npm installed Python 3.11+ installed Text editor/IDE set up Git installed (optional) Basic knowledge of JavaScript/TypeScript and Python Understanding of REST APIs AWS Bedrock model access requested Familiarity with AWS Console Troubleshooting AWS CLI Configuration Issues:\n# Check current configuration aws configure list # Test AWS access aws sts get-caller-identity Node.js Version Issues:\n# Check installed versions nvm list # Switch to correct version nvm use 18 Python Version Issues:\n# Check Python path which python3 # Create virtual environment python3 -m venv venv source venv/bin/activate # On Windows: venv\\Scripts\\activate Next Steps Once you\u0026rsquo;ve completed all prerequisites, proceed to Project Architecture to understand the system design.\nAdditional Resources AWS CLI Documentation Node.js Documentation Python Documentation AWS Free Tier AWS Bedrock Documentation "
},
{
	"uri": "https://asapphat.github.io/tanphat.worklog.github.io/2-proposal/",
	"title": "Proposal",
	"tags": [],
	"description": "",
	"content": "ğŸ“¥ Download Proposal\nAWS First Cloud AI Journey â€“ Project Plan [SKYLINE2] â€“ [University] â€“ [INSIGHTHR]\n[09-12-2025]\nTable of Contents BACKGROUND and motivation\n1.1 Executive Summary 1.2 Project Success Criteria 1.3 Assumptions SOLUTION ARCHITECTURE / ARCHITECTURAL DIAGRAM\n2.1 Technical Architecture Diagram 2.2 Technical Plan 2.3 Project Plan 2.4 Security Considerations Activities AND Deliverables\n3.1 Activities and deliverables 3.2 Out of Scope 3.3 Path to Production EXPECTED AWS COST BREAKDOWN BY SERVICES\nTEAM\nRESOURCES \u0026amp; COST ESTIMATES\nACCEPTANCE\nBACKGROUND AND MOTIVATION 1.1 Executive Summary Organization faces HR evaluation inefficiencies due to manual data handling, lack of transparency in evaluation processes and metrics tracking.\nInsightHR delivers HR automation through flexible evaluation management, automated scoring, and AI insights. AWS provides serverless scalability, cost efficiency, security for sensitive data, AI Chatbot via Bedrock, and rapid deployment.\nCustom KPI, automated performance scoring, multi-level dashboards, AI assistant for natural-language queries, automated notifications, role-based access (Admin/Manager/Employee), multi-tenant support.\nEnd-to-end delivery including Well-Architected design, serverless backend (Lambda, DynamoDB, API Gateway), frontend (S3 + CloudFront), authentication/security (Cognito, IAM), KPI/formula builder, AI chatbot (Bedrock + Lambda query data from info tables), notifications (SNS, SES), CI/CD, monitoring, and knowledge transfer.\n1.2 Project Success Criteria Success is defined by demonstrating a functional MVP that proves the platform\u0026rsquo;s capability to automate HR evaluations and deliver measurable business value.\n1. Functional Criteria:\nAuthentication with role-based access (Admin/HR, Manager, Employee) HR creates custom KPIs without technical support CSV upload triggers automated Lambda scoring Dashboard displays individual/team performance with charts AI chatbot answers natural language queries from DynamoDB data SES sends automated email notifications 2. Technical Criteria:\n99.9%+ uptime \u0026lt;300ms API latency (95th percentile) 95%+ scoring accuracy vs manual calculations 90%+ AI response relevance Zero critical security vulnerabilities 3. Performance \u0026amp; Cost:\n~$33.14/month AWS cost End-to-end workflow (upload â†’ score â†’ visualize) completes in \u0026lt;5 minutes 4. Business Impact:\nDemonstrates 60%+ HR time reduction potential Non-technical users operate KPI builder and chatbot independently 5. Delivery:\nWeek 8: MVP (authentication, KPI/formula management, scoring, basic dashboard) Week 12: Full features (chatbot, notifications, advanced dashboard) 1.3 Assumptions 1. Assumptions:\nThe current AWS cost estimate of approximately $33.14/month is accurate for the projected initial load and usage. The required data format and mapping logic for employee performance data can be clearly defined and provided by the HR team for the automated scoring engine. The Large Language Model provided by Amazon Bedrock to support HR. The automated scoring system is trained locally. The technical evaluation files of each team are assessed according to the companies\u0026rsquo; criteria and must follow the format provided by the customer. 2. Constraints:\nThe project delivery must adhere to the 12-week timeline utilizing the Agile Scrum framework. The solution must be built entirely on serverless AWS services to meet the objectives of scalability, cost efficiency, and reduced operational overhead. The final production AWS cost must remain around the ~$33.14/month target. 3. Risks:\nData Security/Compliance: Failure to fully understand or implement all of the customer\u0026rsquo;s specific regulatory control validation requirements could impact the project\u0026rsquo;s ability to meet security objectives. Feature Creep: Requests for features identified as \u0026ldquo;Out of Scope\u0026rdquo; could derail the 12-week MVP delivery timeline. SOLUTION ARCHITECTURE / ARCHITECTURAL DIAGRAM 2.1 Technical Architecture Diagram The InsightHR platform is built on a serverless architecture using AWS services, providing scalability, cost-effectiveness, and high availability. The architecture includes:\n1. Frontend \u0026amp; Content Delivery:\nAmazon S3: Hosts the static website and stores user-uploaded files (CSV, AI models). S3 Vector to store vectors (embeddings for text-data), S3 Standard for storing raw documents. Amazon CloudFront: Distributes static and dynamic content globally with low latency. 2. Backend \u0026amp; Compute:\nAWS Lambda: Executes all business logic, including authentication, custom scoring, and chatbot functions. Amazon API Gateway: Manages APIs as the communication gateway between frontend and backend. 3. Data Storage:\nAmazon DynamoDB: Stores structured data such as user/employee information, company KPIs, scoring formulas, and performance evaluation results. 4. AI \u0026amp; Machine Learning:\nAmazon Bedrock: Provides Large Language Models (LLMs) for the HR assistant chatbot. The ML Model system is trained locally for scoring function. 5. Security \u0026amp; Identity:\nAmazon Cognito: Manages user authentication, registration, and identity workflows. AWS IAM: Manages access control and permissions for AWS services. AWS KMS: Encrypts sensitive data in DynamoDB and S3. 6. Monitoring \u0026amp; Notifications:\nAmazon CloudWatch \u0026amp; CloudWatch Logs: Monitors Lambda functions, API Gateway, and database access. Amazon SNS: Sends notifications (e.g., reminders, result notifications) to employees. 7. Architecture Benefits:\nServerless: No server management and automatic scaling. Cost-Effective: Mostly pay-as-you-go services. High Availability: Built-in redundancy across AWS regions. Scalable: Can handle growth from small teams to large enterprises. Flexible: Easy to modify and extend functionality. 8. Proposed Architecture Diagram:\n9. Tools Proposed for This Project:\nAmazon CloudFront: For global content delivery and caching of static and dynamic web content. Amazon S3: To host static web assets and store documents, vector embeddings, and other files processed by the system. Amazon API Gateway: To provide a secure RESTful interface, acting as the communication layer between frontend clients and backend services. AWS Lambda: To run backend business logic including user dashboard, auto scoring, HR assistant, and data management workflows. Amazon DynamoDB: To store application data such as user information, HR records, scoring results, and vector metadata with low-latency performance. Amazon Cognito: To manage user authentication, authorization, registration, MFA, and secure access to APIs and frontend applications. AWS Identity and Access Management (IAM): To define fine-grained access policies and control permissions between services and users. AWS Key Management Service (KMS): To manage encryption keys used for securing sensitive data stored in S3, DynamoDB, and logs. Amazon ECR: To store containerized model assets and application dependencies in a secure and version-controlled repository. Amazon Bedrock / Large Language Model (LLM): To provide AI capabilities for chat, data extraction, summarisation, and intelligent HR workflows. Amazon Simple Email Service (SES): To send automated email notifications such as onboarding alerts and HR communications. Amazon Simple Notification Service (SNS): To publish notifications and trigger downstream processes; integrates with email, SMS, and microservices. Amazon CloudWatch \u0026amp; CloudWatch Logs: For monitoring performance, logging, tracing, and operational troubleshooting across Lambda, API Gateway, and AI components. 2.2 Technical Plan The partner will develop automated deployment scripts using AWS CloudFormation and Infrastructure as Code (IaC) practices.\nThis will allow for quick and repeatable deployments into AWS accounts. Some additional configurations such as WAF rules on CloudFront for enhanced security may require approval and will follow standard DevOps change management processes.\nApplication Feature Implementation:\n1. Authentication \u0026amp; Security Module\nUser Management: Cognito manages user lifecycle Registration, login, password reset workflows Access Control: IAM and RBAC enforce role-based permissions Admin/HR, Manager, and Employee access levels API Security: API Gateway implements JWT-protected endpoints Token validation before Lambda processing 2. Administration Module (HR Panel)\nKPI Management: HR creates, edits, and deletes custom metrics Examples: Tasks Completed, Code Quality, Customer Satisfaction Definitions stored in DynamoDB Auto scoring by employee\u0026rsquo;s technical score for each team with ML model. 3. Core User Functions\nData Upload \u0026amp; Mapping: Upload performance data files (CSV) to DynamoDB Scoring Engine: Lambda triggered on upload Retrieves active formula from DynamoDB Calculates employee scores Stores results in DynamoDB Flow: Upload â†’ Validate â†’ Map â†’ Calculate â†’ Store Dashboard: Visualize individual and department performance Line graphs, bar charts, trend analysis AI Chatbot: Bedrock (LLM) integration Natural language queries (e.g., \u0026ldquo;Summarize Team A Q4 performance\u0026rdquo;) Queries and summarizes DynamoDB data Notifications: SES sends automated alerts Performance milestones, review reminders, custom triggers 2.3 Project Plan The partner will adopt the Agile Scrum framework over 12 one-week sprints totaling a 12-week delivery timeline.\n1. Team Responsibilities\nProduct Owner: Prioritizes backlog (KPIs, formulas, analytics) Final authority on feature acceptance Development Team: Implements Cognito authentication Builds admin portal and formula builder Develops scoring engine and dashboard Integrates Bedrock chatbot and SNS with SES notifications via Email. QA Personnel: Conducts functional, performance, and security testing Facilitates UAT Ensures compliance and quality standards 2. Communication Cadences\nDaily Standups (30 min - 1 hr): Progress review and blocker identification Retrospectives (Weekly, 1 hr): Process improvement and delivery optimization Executive Updates (Weekly): Written reports on progress, risks, KPIs, roadmap Leadership decisions required 3. Knowledge Transfer\nSessions conducted by the development team covering AWS serverless fundamentals KPI and formula configuration Data workflows and column-mapping Dashboard navigation and analytics System monitoring (CloudWatch, Cognito, DynamoDB) 2.4 Security Considerations The partner will implement AWS security best practices based on the Well-Architected Framework, prioritizing protection of sensitive HR data while ensuring high operational availability. Security implementation covers five key categories:\n1. Access Control\nCognito manages user identities Enforces strong password policies and MFA support IAM implements RBAC Admin/HR access Admin Panel and KPI/Formula configurations Employees view only their own performance data API Gateway validates JWT tokens Cognito-issued tokens verified before Lambda processing 2. Infrastructure Security\nServerless architecture reduces attack surface No OS or server patching required Lambda functions communicate via private AWS networks Only necessary endpoints exposed through API Gateway 3. Data Protection\nKMS encrypts data at rest DynamoDB and S3 encrypted Data unusable without decryption keys TLS/SSL (HTTPS) encrypts data in transit All frontend-backend communication secured 4. Detection \u0026amp; Monitoring\nCloudWatch Logs captures execution details Lambda and API Gateway activity logged Real-time monitoring and anomaly detection enabled AWS Config tracks configuration changes Ensures resource compliance with security objectives 5. Incident Management\nCloudWatch Alarms trigger automated alerts via SES Failed login threshold breaches Lambda resource anomalies Security Hub provides consolidated security view Unified compliance findings across AWS environment Simplifies incident identification and response AWS CloudTrail and AWS Config will be configured for continuous monitoring of activities and compliance status of resources. The customer will share their regulatory control validation requirements as inputs for the partner to ensure all security objectives are met.\nACTIVITIES AND DELIVERABLES 3.1 Activities and Deliverables NOTE: Some Project Phases overlap each other.\nProject Phase Timeline Activities Deliverables/Milestones Total man-day Phase 1: Foundation \u0026amp; Scoring Model Week 1-8 â€¢ Personal infrastructure architecture research â€¢ Data generation for local model training â€¢ Scoring model build â€¢ Finalized personal architecture diagram â€¢ Ready dataset for Local Model training â€¢ Scoring Model MVP (Minimum Viable Product) 80 Phase 2: Project Setup \u0026amp; Dashboard Week 9-10 â€¢ Project Setup with basic functions: IAM Role, CRUD function, Static web â€¢ Web UI Demo â€¢ Implement Dashboard â€¢ Fix Model â€¢ Basic IAM Roles configured â€¢ Operational CRUD functions â€¢ Static website deployed (S3/CloudFront) â€¢ Web UI Demo completed â€¢ Dashboard displaying data implemented 40 Phase 3: AI Agent \u0026amp; Absence Mgmt Week 11 â€¢ Building Bedrock Agent â€¢ Implement Absent managing â€¢ Bedrock Agent built â€¢ Operational Absence tracking workflow implemented 15 Phase 4: Integration, Testing \u0026amp; Handover Week 12 â€¢ Implement Chatbot into App â€¢ Testing and set up Monitoring â€¢ Chatbot integrated into the application â€¢ Functional, Performance, and Security Testing completed â€¢ Monitoring (CloudWatch) configured and operational â€¢ Project Completion Report \u0026amp; Post-implementation support plan delivered 15 3.2 Out Of Scope 1. AI Enhancements\nAI-Powered Insights:\nWhen sufficient data is available, develop AI models capable of: Chatbot accesses database directly and retrieves prompt window -\u0026gt; Price a lot of tokens and high lock, future can be optimized by other ways Identifying performance patterns across teams and departments Predicting HR risks (e.g., turnover likelihood, burnout indicators) Recommending personalized development plans Detecting anomalies in performance data Suggesting optimal team compositions Machine Learning Features:\nPredictive analytics for workforce planning Sentiment analysis from employee feedback Automated skill gap analysis Performance trend forecasting 2. Public API Development\nAPI Ecosystem:\nBuild a comprehensive API set allowing other internal business systems to automatically push performance data into InsightHR. Integration Targets:\nProject management tools (Jira, Asana, Monday.com) CRM systems (Salesforce, HubSpot) Time tracking software (Toggl, Harvest) Communication platforms (Slack, Microsoft Teams) Code repositories (GitHub, GitLab, Bitbucket) Benefits:\nTransform InsightHR into a central HR data processing hub Create a synchronized and comprehensive management ecosystem Eliminate manual data entry Real-time performance tracking 3. Advanced Features\nMobile Applications:\niOS and Android native apps Push notifications Offline capabilities Mobile-optimized dashboards DynamoDB back up Advanced Analytics:\nPredictive modeling Benchmarking across industries Custom report builder Data export and API for third-party tools Collaboration Features:\nPeer review systems 360-degree feedback Goal setting and tracking Performance improvement plans Compliance \u0026amp; Governance:\nAudit trails Compliance reporting Data retention policies Advanced access controls 3.3 Path To Production This document outlines the current production architecture and operational status for the InsightHR platform deployment. The platform is fully live in the ap-southeast-1 (Singapore) region.\n1. Platform Architecture and Access\nPublic URL: https://d7gdgmhloq3vn.cloudfront.net AWS Region: ap-southeast-1 (Singapore) Frontend: React application hosted on S3 (insighthr-web-app-sg) with CloudFront HTTPS distribution. Backend: 8 Lambda function groups accessed via API Gateway REST API. Database: DynamoDB tables configured with On-Demand capacity. 6 tables for each team Employee information table History score table Absent table Account managing tables Authentication: Cognito User Pool. AI/Chatbot: Amazon Bedrock (Claude 3 Haiku) integration for conversation history and knowledge base to enhance models. 2. Live Production Features\nThe following core features have been successfully deployed and are operational:\nAuthentication: Full support for email/password login, password reset workflows. User Management: Complete CRUD functionality, including bulk import and role-based access. Employee Management: Full support for 300+ employees and bulk operations. Performance Score Management: Management of 900+ quarterly scores and calendar-based viewing. Attendance Management: Processing of 9,300+ records, including check-in/check-out kiosk functionality and auto-absence marking. Performance Dashboard: Live charts, trend analysis, live clock, and CSV export capabilities. AI Chatbot: Bedrock integration with conversation history enabled. 3. Deployment and Verification Process\nThe standard, repeatable deployment workflow ensures rapid and verifiable updates to the production site:\nBuild: npm run build creates the optimized production asset bundle. Test: npm run preview validates the built bundle locally prior to deployment. Deploy: aws s3 sync dist/ s3://insighthr-web-app-sg --region ap-southeast-1 pushes assets to the S3 bucket. Invalidate: aws cloudfront create-invalidation --distribution-id E3MHW5VALWTOCI --paths \u0026quot;/*\u0026quot; clears the CloudFront CDN cache. Verify: Full feature testing is performed on the live public URL. 4. Remaining Production Enhancements\nThe platform is in the final phases of enhancement before full stabilization, with key items planned or in progress:\nPage Integration (In Progress)\nConsolidate all administrative page navigation. Verify all features are accessible from the main menu. Test role-based routing across all pages. Fix any integration bugs. Polish and Final Deployment (Planned)\nImplement comprehensive error handling and input validation. Refine responsive design for full mobile compatibility. Conduct dedicated Security testing (penetration testing, vulnerability scanning). Execute Load testing for scalability validation. Develop user documentation and training materials. Perform final production hardening procedures. Monitoring and Scalability Strategy\nActive Monitoring: CloudWatch Logs are enabled for all Lambda functions and API Gateway endpoints, along with CloudWatch Metrics for performance tracking. Planned Alarms: CloudWatch Alarms and SNS notifications are planned for critical error rates and latency. Scalability: Achieved via Serverless Architecture (DynamoDB On-Demand, Lambda, CloudFront CDN). Disaster Recovery: DynamoDB Point-in-Time Recovery and S3 Versioning are planned to be enabled for critical data/assets. Lambda code is stored in version control for rapid redeployment. EXPECTED AWS COST BREAKDOWN BY SERVICES AWS Service Monthly Estimated Cost (USD) Amazon Bedrock $21.61 AWS Lambda $3.75 Amazon Simple Email Service (SES) $2.25 Amazon DynamoDB $1.52 Amazon Simple Storage Service $0.46 Amazon CloudWatch $0.80 Amazon API Gateway $0.06 Amazon CloudFront $0.00 Amazon Cognito $0.00 Amazon EventBridge $0.00 Amazon IAM $1.60 Amazon KMS $1.03 TOTAL MONTHLY COST $33.14 TOTAL YEARLY COST $397.79 TEAM Name Task Role Email / Contact Info BÃ¹i Táº¥n PhÃ¡t Dashboard, Manage Employee, Support, Content check Leader btfat3103@gmail.com Nguyá»…n Ngá»c Long CRUD, Config Network / API Gateway, Test function, Slide Member nguyenngoclong216@gmail.com Äáº·ng Nguyá»…n Minh Duy Database, CloudWatch / CloudLogs, Paper, Slide Member dangnguyenminhduy11b08@gmail.com Äá»— ÄÄƒng Khoa Log In/ Registration / Forget Password, UI / UX - Static Web, Paper Member khoado7577@gmail.com Nguyá»…n Huá»³nh ThiÃªn Quang Auto Scoring, AI Assistant, Slide Member quangkootenhatvutru@gmail.com RESOURCES \u0026amp; COST ESTIMATES Resource Responsibility Rate (USD) / Hour Full-Stack Developers [2] React frontend, Python Lambda backend, API integration $66 Cloud Engineers [3] AWS infrastructure setup, deployment automation, monitoring $66 Other (Please specify) Estimated platform consumption (Lambda, DynamoDB, Bedrock). Paper and present material $0.01 NOTE: Project Phase durations overlap each other.\nProject Phase Duration Man-Days Other (Please specify) Estimated Cost Phase 1: Foundation \u0026amp; Scoring Model 8 Weeks 80 - $42,246.40 (80 x $528.08) Phase 2: Project Setup \u0026amp; Dashboard 2 Weeks 40 - $21,123.20 (40 x $528.08) Phase 3: AI Agent \u0026amp; Absence Mgmt 1 Week 15 - $7,921.20 (15 x $528.08) Phase 4: Integration, Testing \u0026amp; Handover 1 Week 15 - $7,921.20 (15 x $528.08) Total Hours 12 Weeks 150 Man-Days $79,212.00 Cost Contribution distribution between Partner, Customer, AWS.\nParty Contribution (USD) % Contribution of Total Customer 0 0 Partner 0 0 AWS 200 100 ACCEPTANCE 1. Project Acceptance Criteria\nThe InsightHR platform will be considered complete and accepted when the following criteria are met.\n2. Completed Deliverables\nAll major features implemented and deployed to production User and employee management with bulk operations Performance score management with calendar view Attendance system with auto-absence marking Interactive dashboard with live clock AI chatbot with Bedrock integration 3. Key Metrics Achieved\n300+ user accounts 300 employee records across 5 departments 900+ performance scores tracked 9,300+ attendance records AWS monthly cost: ~$33.14 System uptime: 99.9%+ Zero critical security vulnerabilities 4. Acceptance Status\nCurrent Status: Application deployed in cloudfront Production URL: https://d2z6tht6rq32uy.cloudfront.net 5. Next Steps\nMinor bug fixing and feature updates Conduct user acceptance testing Provide knowledge transfer and training "
},
{
	"uri": "https://asapphat.github.io/tanphat.worklog.github.io/3-blogstranslated/3.2-blog2/",
	"title": "Blog 2: Build next-gen AI agents",
	"tags": [],
	"description": "",
	"content": "AWS Public Sector Blog Build next-gen AI agents: AWS empowers partners with AWS Partner Transformation Programâ€™s new agentic AI module for public sector by Jasmine Thakkar and Mohan CV on 15 SEP 2025 in Announcements, Artificial Intelligence, Partner solutions, Public Sector, Public Sector Partners\nThe artificial intelligence (AI) revolution has entered an exhilarating new phase, and Amazon Web Services (AWS) is empowering partners to lead this transformation. Today, weâ€™re excited to announce the launch of our new agentic AI module within the AWS Partner Transformation Program (PTP), designed to accelerate partnersâ€™ capabilities in building autonomous AI solutions. This strategic initiative comes at a crucial time, as number of enterprise applications incorporating agentic AI continues to surge, presenting unprecedented opportunities for innovation in the public sector and beyond.\nHowever, as with any transformative technology, many organizations find themselves asking the same question: How do we move from big ideas to practical execution using agentic AI? Thatâ€™s why the new PTP module features two carefully crafted pathways tailored to meet partners where they are in their agentic AI journey. The Foundational Path is designed for partners beginning their agentic AI journey, providing strategic insights and guided use case development that culminates in a functional proof-of-concept. For partners ready to move into production, the Solution Development Path offers accelerated technical implementation leveraging powerful AWS services including Amazon Bedrock AgentCore, Strands Agents, and other technologies leading to production-ready minimum viable products (MVPs) that deliver immediate value to customers. AWS Partners can also publish completed solutions to the AI Agent \u0026amp; Tools solution page in AWS Marketplace.\nâ€œAWS Partners are continuing to drive rapid AI adoption in public sector. With this new PTP module, weâ€™re not just sharing technology, weâ€™re providing a blueprint to accelerate AWS Partner offerings in the agentic AI space,â€ said Mike Cannady, director of partner core for public sector at AWS.\nPartners participating in the program gain access to an extensive suite of resources, including exclusive workshop content, implementation support from AWS experts, sandbox credits for development, and comprehensive guidance for AWS Marketplace listing.\nFigure 1. AWS PTP Agentic AI Module for building and scaling intelligent agent solutions\nAt the heart of this initiative is the comprehensive portfolio of AWS agentic AI services, such as Amazon Bedrock AgentCore, Amazon Q, AWS Transform, Strands Agents, and Amazon Nova Act. These powerful tools enable partners to build sophisticated AI agents that can understand, decide, and act autonomously while maintaining the highest standards of security and complianceâ€”crucial requirements for public sector implementations.\nâ€œBuilding off the excitement of the AWS Summit New York City, we are making AWS the best place for the worldâ€™s most useful agents. PTP brings technical enablement to empower our partners to create agentic experiences that fundamentally transform how the public sector serves its citizens,â€ shared Alex Martinez, director of public sector, partner solutions architecture at AWS.\nThe potential applications are transformative: from AI agents that optimize government operations and enhance citizen services to systems that revolutionize healthcare delivery and educational experiences. As organizations worldwide seek to harness these capabilities, AWS Partners are uniquely positioned to lead this next wave of innovation, backed by the most comprehensive and secure cloud platform for AI development.\nBuilding on the AWS commitment to partner success, the new PTP module delivers a robust framework of structured approaches, best practices, governance, and guardrailsâ€”enabling partners to accelerate development cycles, minimize risks, and unlock the full potential of agentic AI solutions.\nPartners interested in joining this new module can contact their AWS Partner account manager or partner development manager to learn more about the PTP program and begin their agentic AI journey. The future of AI is agent-based, secure, and built on AWSâ€”and our partners are at the forefront of this exciting transformation.\nTAGS: announcement, Artificial Intelligence, AWS Public Sector, AWS Public Sector Partners\nJasmine Thakkar Jasmine is a global lead for the AWS Partner Transformation Program (PTP), where she helps AWS Partners accelerate their cloud journey and drive meaningful transformation for citizens through public sector innovation. She leads a comprehensive program that equips partners with strategic guidance, technical expertise, and best practices enabling them to build successful cloud practices while driving growth and delivering better outcomes for customers.\nMohan CV Mohan is a principal architect at AWS, passionate about helping partners and customers leverage emerging technologies like agentic AI to transform their organizations and deliver innovative solutions that address complex business challenges. He brings extensive experience in leading large-scale digital transformations, specializing in data and AI.\n"
},
{
	"uri": "https://asapphat.github.io/tanphat.worklog.github.io/1-worklog/1.2-week2/",
	"title": "Week 2 Worklog",
	"tags": [],
	"description": "",
	"content": "Week Duration: September 15 - 21, 2025\nWeek 2 Objectives Set up development environment with Linux Fedora for AWS services Attend AWS Cloud Day 2025 and learn about new AWS technologies Establish team workflow and project management processes Apply AI-DLC (AI Driven Development Lifecycle) methodology with Kiro IDE Explore AWS S3 buckets and basic AWS services Tasks Carried Out This Week Day Task Start Date Completion Date Reference Material Monday - Lab 4 Introduction EC2: 15/09/2025 15/09/2025 https://000004.awsstudygroup.com/ Tuesday - Lab 10 Set up Hybrid DNS with Route 53 Resolver: 16/09/2025 16/09/2025 https://000010.awsstudygroup.com/ Wednesday - Lab 19 Setting up VPC Peering: 17/09/2025 17/09/2025 https://000019.awsstudygroup.com/ Thursday - AWS CloudDays 18/09/2025 18/09/2025 Saturday/Sunday - Search for Project Dataset: Datasets: Employee Performance and Productivity Data, Employee Performance for HR Analytics, Employee Performance Dataset, Resume Dataset 20/09/2025 21/09/2025 Kaggle, GitHub Datasets Week 2 Achievements Completed Lab 4 - Introduction to EC2:\nMastered Amazon EC2 deployment procedures Learned EC2 instance configuration and management Gained practical experience with EC2 instance lifecycle Understood EC2 connection methods and security groups Completed Lab 10 - Set up Hybrid DNS with Route 53 Resolver:\nConfigured Route 53 Resolver for hybrid DNS setup Connected on-premises DNS with AWS Route 53 Established DNS resolution across hybrid environment Hands-on practice with DNS configuration and troubleshooting Completed Lab 19 - Setting up VPC Peering:\nSuccessfully configured VPC peering connections Established communication between multiple VPCs Configured routing and network ACLs for peered VPCs Mastered inter-VPC connectivity best practices Attended AWS CloudDays Event:\nParticipated in AWS CloudDays 2025 conference Networking with AWS professionals and other learners Learned about latest AWS technologies and innovations Gained industry insights and best practices from speakers Connected with fellow AWS learners and practitioners Project Dataset Identification:\nResearched and evaluated multiple HR analytics datasets from Kaggle and GitHub Identified suitable Employee Performance and Productivity Data Explored Employee Performance for HR Analytics dataset Selected relevant Employee Performance Dataset for project use Reviewed Resume Dataset for project supplementary data Finalized comprehensive dataset selection for team project Technical Skills Development:\nAdvanced EC2 deployment and management DNS configuration in hybrid environments VPC network architecture and peering Data exploration and evaluation techniques Project planning and requirements analysis Preparation for Week 3:\nCompleted foundation AWS labs and networking exercises Identified project datasets and established data direction Ready for storage services and data management phase Key AWS Services Understanding:\nEC2 for computing and instance management Route 53 for DNS management VPC for network architecture and peering Data sources and integration patterns Challenges Faced Minimal difficulties encountered during Week 2 Successfully resolved Fedora Linux installation and configuration issues through troubleshooting Key Learnings New Technologies:\nAmazon Nova Act capabilities and use cases AI-DLC (AI Driven Development Lifecycle) methodology and its application in modern software development Kiro AI Agent features and autonomous workflow capabilities AWS Services:\nS3 bucket management and best practices Team collaboration on AWS free tier resources AWS service planning for proposals and workshops Development Skills:\nLinux Fedora environment management Hugo static site generator Chromium extension development AI-driven development workflows Next Week Goals Continue exploring AWS services in depth Apply AI-DLC methodology to more complex projects Prepare detailed proposal for workshop implementation Deepen understanding of VPC and IAM roles "
},
{
	"uri": "https://asapphat.github.io/tanphat.worklog.github.io/4-eventparticipated/4.2-event2/",
	"title": "Event 2",
	"tags": [],
	"description": "",
	"content": "AI-Driven Development Life Cycle Location: AWS Event Hall, L26 Bitexco Tower, HCMC\nDate: Friday, October 3, 2025\nLearning Report: \u0026ldquo;AI-Driven Development Life Cycle\u0026rdquo; Event Objectives Understand the big picture of AI-Driven Software Development Lifecycle (AI-Driven SDLC) Directly experience and see real demonstrations of Amazon Q Developer development assistant Deep dive into the Kiro tool and its potential integration into application development processes Update knowledge of the latest programming support tools to optimize work efficiency Speaker List Toan Huynh Speaker on AI-Driven SDLC \u0026amp; Amazon Q My Nguyen Speaker on Kiro Demo Key Content Highlights AI-Driven Software Development Lifecycle (AI-Driven SDLC) Paradigm Shift: Transition from traditional development processes to deep AI integration at every stage of software development lifecycle Automation: Automate repetitive tasks in the development cycle, including code writing, testing, and deployment Efficiency Enhancement: Optimize time and resources through support from intelligent AI assistants, allowing developers to focus on complex business problems Real-world Demonstration of Amazon Q Developer Programming Assistant: Demonstrate code suggestion capabilities, explain complex code logic, and automatically generate unit tests Troubleshooting: Guide using Amazon Q for debugging and proposing technical solutions to issues in real-time Integration: How to integrate Amazon Q into Integrated Development Environments (IDE) and developers\u0026rsquo; daily workflows Kiro Tool Demonstration Tool Capabilities: Introduction to core features of Kiro (related to Kiro IDE/Agent) Use Cases: Live demonstration of how Kiro is used to solve specific programming problems in practice Developer Experience: Significantly improve developer experience through intuitive interface and intelligent features of Kiro What I Learned Tooling Landscape Amazon Q Developer is not just a simple chat tool but a comprehensive assistant supporting every phase of SDLC Kiro brings new approaches and innovation in supporting development environments (IDE/Agent) Productivity Optimization Applying AI-Driven SDLC helps minimize manual work, allowing developers to spend more time solving core business logic Developers need to proactively learn and master new AI tools to maintain competitive capabilities in technology trends Application to Work Integrate Amazon Q: Pilot Amazon Q Developer in current projects to leverage code review support and test generation capabilities Research Kiro: Spend time studying Kiro more deeply after the demo to evaluate integration capabilities and application to team processes Improve SDLC: Review current development process, identify bottlenecks, and plan using AI to automate those areas Event Experience The workshop focused deeply on technical demonstrations, providing intuitive and convincing insight into the power of advanced programming support tools.\nPractical Perspective Toan Huynh\u0026rsquo;s presentation helped me clearly envision how an AI-optimized and operated SDLC would look in practice My Nguyen\u0026rsquo;s Kiro demo was very specific and practical, highlighting the potential of emerging tools alongside major platforms like AWS Impact on Work Process I clearly see that AI is fundamentally changing how we build software: faster, more accurate, and with fewer errors The event was concise but substantial, getting straight to strategic tools that the programming community cares about Overall, the afternoon session was a perfect complement to the architecture knowledge learned, helping complete the picture of modern software development Some photos from the event In summary, this event equipped me with the knowledge and tools needed to boost productivity while affirming the indispensable role of AI in the future of software development.\n"
},
{
	"uri": "https://asapphat.github.io/tanphat.worklog.github.io/5-workshop/5-workshop/5.3-architecture/",
	"title": "Project Architecture",
	"tags": [],
	"description": "",
	"content": "InsightHR Architecture Deep Dive This section provides a detailed look at the InsightHR platform architecture, explaining how different AWS services work together to create a scalable, secure, and cost-effective serverless application.\nHigh-Level Architecture â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\râ”‚ User Browser â”‚\râ”‚ (React + TypeScript) â”‚\râ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\râ”‚ HTTPS\râ–¼\râ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\râ”‚ CloudFront CDN â”‚\râ”‚ - Global edge locations â”‚\râ”‚ - SSL/TLS termination â”‚\râ”‚ - Caching static assets â”‚\râ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\râ”‚\râ–¼\râ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\râ”‚ S3 Static Website â”‚\râ”‚ - React SPA hosting â”‚\râ”‚ - Static assets (JS, CSS, images) â”‚\râ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\râ”‚ REST API calls\râ–¼\râ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\râ”‚ API Gateway (REST) â”‚\râ”‚ - Request routing â”‚\râ”‚ - Cognito authorization â”‚\râ”‚ - Request/response transformation â”‚\râ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\râ”‚\râ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\râ–¼ â–¼ â–¼ â–¼\râ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\râ”‚ Lambda â”‚ â”‚ Lambda â”‚ â”‚ Lambda â”‚ â”‚ Lambda â”‚\râ”‚ Auth â”‚ â”‚ Employees â”‚ â”‚ Performance â”‚ â”‚ Chatbot â”‚\râ”‚ â”‚ â”‚ â”‚ â”‚ Scores â”‚ â”‚ â”‚\râ””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\râ”‚ â”‚ â”‚ â”‚\râ–¼ â–¼ â–¼ â–¼\râ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\râ”‚ DynamoDB â”‚\râ”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\râ”‚ â”‚ Users â”‚ â”‚Employees â”‚ â”‚ Scores â”‚ â”‚Attendanceâ”‚ â”‚\râ”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚\râ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\râ”‚\râ–¼\râ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\râ”‚ AWS Bedrock â”‚\râ”‚ (Claude 3 Haiku Model) â”‚\râ”‚ - Natural language processing â”‚\râ”‚ - Context-aware responses â”‚\râ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ Component Breakdown 1. Frontend Layer Amazon S3 + CloudFront\nS3 Bucket: Hosts the React SPA as static files\nConfigured for static website hosting Stores HTML, JavaScript, CSS, and image assets Versioning enabled for rollback capability CloudFront Distribution: Global CDN for fast content delivery\nEdge locations worldwide for low latency SSL/TLS certificate from ACM Custom domain support (insight-hr.io.vn) Caching policies for optimal performance Origin Access Identity (OAI) for S3 security React Application\nSingle Page Application (SPA) architecture Client-side routing with React Router State management with Zustand TypeScript for type safety Tailwind CSS for styling 2. API Layer Amazon API Gateway (REST)\nEndpoints: RESTful API design\n/auth/* - Authentication endpoints /employees/* - Employee management /performance-scores/* - Performance tracking /attendance/* - Attendance management /chatbot/* - AI chatbot queries Features:\nRequest validation CORS configuration Rate limiting and throttling Request/response transformation API keys and usage plans Authorization: Cognito User Pool Authorizer\nJWT token validation Role-based access control Automatic token refresh 3. Compute Layer AWS Lambda Functions\nAuthentication Service (auth-*-handler)\nLogin with Cognito User registration Google OAuth integration Password reset workflow Token management Employee Service (employees-*-handler)\nCRUD operations for employees Department and position filtering Bulk import from CSV Search functionality Performance Scores Service (performance-scores-handler)\nScore calculation and management Quarterly performance tracking KPI aggregation Role-based data access Chatbot Service (chatbot-handler)\nNatural language query processing Context building from DynamoDB Bedrock API integration Response formatting Attendance Service (attendance-handler)\nCheck-in/check-out operations Attendance history tracking Status management Bulk operations Lambda Configuration:\nRuntime: Python 3.11 Memory: 256-512 MB Timeout: 30-60 seconds Environment variables for configuration IAM roles with least privilege 4. Data Layer Amazon DynamoDB\nTables Structure:\ninsighthr-users-dev\nPrimary Key: userId GSI: email-index Purpose: User authentication and profiles Attributes: email, name, role, employeeId, department insighthr-employees-dev\nPrimary Key: employeeId GSI: department-index Purpose: Employee master data Attributes: name, email, department, position, status insighthr-performance-scores-dev\nPrimary Key: employeeId Sort Key: period GSI: department-period-index Purpose: Quarterly performance scores Attributes: overallScore, kpiScores, calculatedAt insighthr-attendance-history-dev\nPrimary Key: employeeId Sort Key: date GSI: date-index, department-date-index Purpose: Check-in/check-out history Attributes: checkInTime, checkOutTime, status DynamoDB Features:\nOn-demand billing mode Point-in-time recovery Encryption at rest Global secondary indexes for efficient queries TTL for automatic data expiration (if needed) 5. Authentication Layer Amazon Cognito User Pools\nUser Management:\nEmail/password authentication Google OAuth integration User attributes (name, role, department) Password policies and MFA support Token Management:\nJWT tokens (ID, Access, Refresh) Token expiration and refresh Custom claims for roles Security Features:\nPassword complexity requirements Account recovery workflows User verification Brute force protection 6. AI/ML Layer Amazon Bedrock (Claude 3 Haiku)\nCapabilities:\nNatural language understanding Context-aware responses Data querying and analysis Conversational interface Integration:\nInvoked from Lambda function Context built from DynamoDB data Role-based data filtering Response formatting and validation Cost Optimization:\nHaiku model for cost-effectiveness Efficient prompt engineering Response caching where applicable 7. Monitoring Layer Amazon CloudWatch\nLogs:\nLambda function logs API Gateway access logs Error tracking and debugging Metrics:\nAPI request counts Lambda invocations and duration DynamoDB read/write capacity Error rates and latency Alarms:\nHigh error rate alerts Performance degradation Cost threshold warnings CloudWatch Synthetics Canaries:\nLogin flow testing Dashboard availability Chatbot functionality Performance score calculations Data Flow Examples User Login Flow 1. User enters credentials in React app\r2. React app calls API Gateway /auth/login\r3. API Gateway routes to auth-login-handler Lambda\r4. Lambda authenticates with Cognito\r5. Cognito returns JWT tokens\r6. Lambda stores user session in DynamoDB\r7. Tokens returned to React app\r8. React app stores tokens in localStorage\r9. Subsequent requests include JWT in Authorization header Employee Query Flow 1. User requests employee list in React app\r2. React app calls API Gateway /employees with filters\r3. API Gateway validates JWT token with Cognito\r4. Request routed to employees-handler Lambda\r5. Lambda queries DynamoDB employees table\r6. Results filtered based on user role\r7. Data returned to React app\r8. React app displays employees in table Chatbot Query Flow 1. User types question in chatbot interface\r2. React app calls API Gateway /chatbot/query\r3. API Gateway validates JWT and routes to chatbot-handler\r4. Lambda retrieves relevant data from DynamoDB\r5. Lambda builds context and calls Bedrock API\r6. Bedrock processes query with Claude 3 Haiku\r7. Response formatted and returned to React app\r8. React app displays answer in chat interface Security Architecture Defense in Depth:\nNetwork Security:\nHTTPS only (enforced by CloudFront) API Gateway with AWS WAF (optional) VPC endpoints for private connectivity (optional) Authentication \u0026amp; Authorization:\nCognito for user authentication JWT tokens for API authorization Role-based access control (RBAC) Least privilege IAM roles Data Security:\nEncryption at rest (DynamoDB, S3) Encryption in transit (TLS 1.2+) Secure credential management No hardcoded secrets Application Security:\nInput validation SQL injection prevention (NoSQL) XSS protection CORS configuration Scalability \u0026amp; Performance Auto-Scaling:\nLambda: Automatic concurrent execution scaling DynamoDB: On-demand capacity mode CloudFront: Global edge network API Gateway: Automatic request handling Performance Optimization:\nCloudFront caching for static assets DynamoDB GSIs for efficient queries Lambda function optimization API response caching High Availability:\nMulti-AZ deployment (automatic) CloudFront global distribution DynamoDB replication Lambda fault tolerance Cost Optimization Strategies:\nOn-demand pricing for variable workloads Lambda free tier utilization CloudFront caching to reduce origin requests DynamoDB query optimization Bedrock Haiku model for cost-effectiveness Estimated Monthly Costs (Development):\nDynamoDB: $0.50 Lambda: Free tier S3 + CloudFront: $1-2 API Gateway: $0.10 Bedrock: $0.0004 per query Total: $2-5/month Next Steps Now that you understand the architecture, let\u0026rsquo;s proceed to Setup AWS Environment to start building the platform.\n"
},
{
	"uri": "https://asapphat.github.io/tanphat.worklog.github.io/3-blogstranslated/",
	"title": "Translated Blogs",
	"tags": [],
	"description": "",
	"content": "This section will list and introduce the blogs you have translated. For example:\nBlog 1 - AWS Weekly Roundup: Strands Agents This blog introduces the AWS Weekly Roundup from September 15, 2025, highlighting major AWS announcements and launches. It features Strands Agents, the open-source AWS SDK for agentic AI, which has reached 1 million downloads and 3,000+ GitHub Stars less than 4 months since launching as a preview in May 2025. The article covers recent AWS announcements including Amazon EC2 M4 and M4 Pro Mac instances, LocalStack integration in VS Code, AWS CDK Refactor, and more. It also highlights upcoming AWS events including the AI Agent Global Hackathon and AWS Gen AI Lofts worldwide.\nBlog 2 - Build next-gen AI agents This blog announces the launch of a new agentic AI module within the AWS Partner Transformation Program (PTP), designed to accelerate partners\u0026rsquo; capabilities in building autonomous AI solutions. The article explores two carefully crafted pathways for partners at different stages of their agentic AI journey: the Foundational Path for partners beginning their journey, and the Solution Development Path for partners ready to move into production. It covers the comprehensive portfolio of AWS agentic AI services, including Amazon Bedrock AgentCore, Amazon Q, AWS Transform, Strands Agents, and Amazon Nova Act, enabling partners to build sophisticated AI agents with highest standards of security and compliance.\nBlog 3 - Strands Agents and the Model-Driven Approach This blog explores the model-driven approach that Strands Agents SDK has adopted to eliminate the fragility of traditional orchestration frameworks. Instead of trying to anticipate and program for every possible scenario, modern large language models self-steer their behavior, making intelligent decisions about tool usage and adapting flexibly to situations. The article explains how this approach emerged from real-world production experience building agents for Kiro, Amazon Q Developer, and AWS Glue. It covers the agent loop concept, which is a natural cycle of reasoning and action that mirrors how intelligent systems think and work, allowing models to reason dynamically and handle edge cases better than traditional state machines.\n"
},
{
	"uri": "https://asapphat.github.io/tanphat.worklog.github.io/1-worklog/1.3-week3/",
	"title": "Week 3 Worklog",
	"tags": [],
	"description": "",
	"content": "Week Duration: September 22 - 28, 2025\nWeek 3 Objectives: Explore and understand VPC (Virtual Private Cloud) fundamentals Learn EC2 service basics and deployment practices Deploy a static website using S3 and EC2 Practice AWS architecture diagramming with draw.io Understand cost optimization strategies in AWS Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Monday - Introduction to S3 22/09/2025 22/09/2025 https://000057.awsstudygroup.com/ Tuesday - File Storage Gateway: 23/09/2025 23/09/2025 https://000024.awsstudygroup.com/ Wednesday - VM Import/Export 24/09/2025 24/09/2025 https://000014.awsstudygroup.com/vi/ Thursday - WorkFlow for Project: 25/09/2025 25/09/2025 Project Templates Friday - Using AI Gen + Check Data Version 1: 26/09/2025 29/09/2025 Project Guidelines Week 3 Achievements: Completed Lab 57 - Introduction to S3:\nMastered S3 bucket creation and configuration Learned S3 object management and storage classes Understood S3 access control and permissions Explored S3 static website hosting capabilities Gained practical experience with S3 lifecycle policies Completed Lab 24 - File Storage Gateway:\nConfigured AWS Storage Gateway for hybrid storage Connected on-premises storage with AWS cloud Learned file interface and caching mechanisms Enabled seamless data transfer between on-premises and cloud Understood storage optimization strategies Completed Lab 14 - VM Import/Export:\nSuccessfully imported virtual machines to AWS Converted on-premises VMs to EC2 instances Learned VM migration best practices Configured VM export procedures Gained experience with infrastructure modernization Designed Project Workflow:\nCreated comprehensive project workflow documentation Planned project implementation phases and milestones Defined clear data processing pipeline and stages Established project governance framework Documented roles and responsibilities AI-Powered Data Generation and Verification:\nGenerated synthetic data using AI generation tools Verified data quality and format compliance Checked data version 1 integrity and consistency Created working dataset for project development Validated data schema and structure Storage Services Understanding:\nS3 for scalable object storage solutions Storage Gateway for hybrid storage architecture VM migration for infrastructure transition Storage optimization and cost management Data protection and availability strategies Project Foundation Established:\nSelected and verified comprehensive project dataset Designed project architecture and workflow Established project team collaboration framework Ready for development and implementation phases Technical Competencies Developed:\nAdvanced AWS storage service integration Project planning and workflow design Data generation and AI tool application Infrastructure migration techniques Hybrid cloud architecture design Preparation for Next Phase:\nCompleted storage services foundation Established data processing workflows Ready for advanced AWS topics and project development Created solid foundation for scalable data solutions "
},
{
	"uri": "https://asapphat.github.io/tanphat.worklog.github.io/3-blogstranslated/3.3-blog3/",
	"title": "Blog 3: Strands Agents and the Model-Driven Approach",
	"tags": [],
	"description": "",
	"content": "AWS Open Source Blog Strands Agents and the Model-Driven Approach by Arron Bailiss on 12 SEP 2025 in Artificial Intelligence, Open Source, Technical How-To Permalink Comments Share\nUntil recently, building AI agents meant wrestling with complex orchestration frameworks. Developers wrote intricate state machines, pre-defined workflows, and extensive error-handling code to shepherd language models through multi-step tasks. We needed to construct complex decision trees to handle cases of â€œwhat if the API call fails?â€ or â€œwhat if the user asks something unexpected?â€ Despite the effort, agents would break when they encountered unforeseen situations.\nThe Strands Agents SDK adopts a model-driven approach that eliminates this fragility entirely. Instead of trying to anticipate and program for every possible scenario, we let modern large language models self-steer their behavior, making intelligent decisions about tool usage and adapting flexibly to whatever happens. This approach is more durable, with better resilience because it allows models to reason about problems dynamically. When an API call fails, the model doesnâ€™t crash â€“ it reasons about alternatives. When a user asks something unexpected, the model doesnâ€™t follow a pre-defined â€œI donâ€™t understandâ€ path â€“ it figures out how to help, using the available tools. The modelâ€™s reasoning capability handles edge cases better than any state machine we could write.\nThe model-driven approach comes from the real-world experience of AWS teams building production agents for Kiro, Amazon Q Developer, and AWS Glue. What we discovered fundamentally changed how we thought about agent architecture: the orchestration frameworks we previously built to guide models were now getting in the way of what modern LLMs could do naturally.\nHowever, using the model as the orchestrator doesnâ€™t mean sacrificing developer control. Strands offers a clean, simple interface that gets you started in minutes, while providing powerful configurability for when you need it. Integrated evaluation tools help you understand and validate the agentâ€™s behavior, ensuring you maintain confidence even as the models make autonomous decisions. This balance of simplicity and flexibility is what makes the model-driven approach practical for both rapid prototyping and production systems. Strands grows with you.\nIn this post, weâ€™ll explore how the model-driven approach emerged from real-world production experience and show you practical patterns for building agents that can think for themselves.\nWhen orchestration becomes a constraint Traditional agent frameworks emerged when language models needed a lot of hand-holding. Developers built complex state machines, pre-defined workflows, and intricate coordination logic because the models couldnâ€™t reliably reason about their next steps. We would write hundreds of lines of code just to handle the flow between â€œgather information,â€ â€œanalyze data,â€ and â€œprovide responseâ€ â€“ and still end up with agents that broke when users asked questions in unexpected ways.\nThe AWS teams I worked with to build Amazon Q Developer experienced this directly. Initially, we used traditional orchestration frameworks, but changes in product requirements and user behavior required significant scientific and engineering effort to calibrate the system to handle new use cases. Machine learning (ML) routers and classifiers would return brittle outputs for guardrails â€“ static text like â€œIâ€™m sorry, I cannot answer that questionâ€ â€“ without offering alternatives or explanations in a way that was interactively helpful and appeared intelligent.\nModern models are sophisticated enough to orchestrate themselves. This was the critical insight that changed everything for us. While structured workflows still have their place â€“ especially for compliance requirements or well-defined business processes â€“ the model-driven approach eliminates the need for rigid orchestration in most scenarios.\nWhen the Q Developer teams transitioned to the model-driven approach, the system could flexibly handle changing product requirements and respond coherently no matter what happened. Instead of brittle guardrail responses, the agent could offer alternatives and use the guardrailâ€™s output as context for the model to understand why it couldnâ€™t answer certain inputs. This allowed for a single personality and tone across multi-agent systems, enabling engineers across Amazon to contribute their domain expertise while maintaining coherence. It also provided the right context to the model at the right time with powerful tools for helpful, intelligent responses.\nWhen you give a model tools, it doesnâ€™t just mechanically execute them. It reasons about the best approach, considers multiple strategies, and adjusts its behavior based on what it discovers. A model analyzing a performance issue might start with system metrics, realize the true bottleneck is in the database, pivot to query analysis, and then propose both immediate fixes and long-term architectural improvements â€“ all without being explicitly programmed for that sequence.\nThe operational benefits were transformative. What previously took months of development now took weeks. Our teams could rapidly leverage the latest Large Language Model (LLM) features like tool calling, expansive and interleaved thinking, multi-agent systems, meta agents, MCP, and A2A â€“ all while keeping a simple developer interface that could be fully customized for deploying AI agents in production.\nThis philosophy extends beyond single agents. Whether youâ€™re building a simple data analysis agent or a complex multi-agent system, the model decides when to collaborate with other agents, when to delegate tasks, when to dive deeper into a problem, and when to provide a final answer.\nThe agent loop: Where intelligence meets action The heart of this philosophy is the agent loop â€“ a natural cycle of reasoning and action that mirrors how intelligent systems think and work. This approach builds on the ReAct pattern (ReAct: Synergizing Reasoning and Acting in Language Models), which demonstrated how language models could generate both reasoning traces and task-specific actions in an interleaved manner, creating a greater synergy between thinking and acting.\nThe model engages in a continuous reasoning process, asking questions like:\n\u0026ldquo;What am I trying to achieve here?\u0026rdquo;\n\u0026ldquo;What information do I need?\u0026rdquo;\n\u0026ldquo;Which tool will be most effective?\u0026rdquo;\n\u0026ldquo;How do these results change my understanding?\u0026rdquo;\n\u0026ldquo;Should I continue exploring or provide an answer?\u0026rdquo;\nThis internal reasoning process is what makes model-driven agents powerful. They aren\u0026rsquo;t just executing pre-defined steps â€“ they are thinking, adapting, and evolving their approach in real-time.\nGuiding intelligence through context While the model self-steers its behavior, that behavior is shaped by the context it receives. In the model-driven approach, you guide the agentâ€™s intelligence not through rigid control structures, but through carefully constructed context that includes system prompts, tool specifications, and conversation history.\nThink of it like hiring a skilled consultant. You don\u0026rsquo;t micromanage their every step â€“ instead, you give them clear objectives, explain what resources are available, and provide relevant background information. The model-driven approach works the same way.\nSystem prompts establish the agentâ€™s role and objectives â€“ they set the foundational identity and high-level goals that guide every decision-making process. Instead of dictating concrete steps, effective system prompts describe what success looks like and provide principles for decision-making. Instead of â€œFirst check the database, then validate input, then process the request,â€ you might write â€œYou efficiently and accurately help users analyze their data, always validating input before processing.â€\nTool specifications define the boundaries of capability and guide usage â€“ they communicate not just which tools are available, but how and when they should be used. Well-designed tool descriptions become part of the modelâ€™s reasoning process. A tool description like â€œUse this tool for complex mathematical calculations that require high precisionâ€ helps the model appropriately choose between a calculator tool and writing Python code.\nConversation history maintains task continuity and evolving context â€“ it provides the specific details that enable agents to maintain coherence across interactions. As conversations get longer, this context management becomes critical to maintaining performance while retaining relevant information. Context management is critical for model-driven agents â€“ it is the difference between an agent that maintains coherent, intelligent conversations and one that loses track of crucial details or hits token limits. Strands provides a simple interface that allows developers to define the agentâ€™s system prompts, tools, and context management behavior without getting lost in the complexity of implementation.\nSliding window approaches retain recent messages and discard older ones: simple and effective. Summarization techniques retain critical information from longer conversations within token limits â€“ better for complex, evolving discussions. The goal is to provide the model with the most relevant context to make good decisions for your specific use case.\nThis represents a major shift from procedural programming to contextual programming. Instead of writing â€œif this, then thatâ€ logic, you are creating the context that helps the model figure out the best approach on its own.\nfrom strands import Agent\nfrom strands_tools import calculator, file_write, python_repl\nSimple: Get started in seconds agent = Agent(\ntools=[calculator, file_write, python_repl],\rsystem_prompt=\u0026quot;You are a helpful assistant that can perform calculations and verify them with code.\u0026quot;\r)\nThe model autonomously decides: calculate first, then verify with code agent(\u0026ldquo;Calculate the compound interest on $10,000 at 5% annually for 10 years\u0026rdquo;)\nThis design philosophy is carried throughout Strands. You can build complex agents without getting lost in configuration, yet every aspect remains customizable when your use case demands it.\nScaling model-driven agents with Strands The model-driven approach scales naturally while maintaining a clean developer experience. A single agent equipped with the right tools can handle complex tasks that would traditionally require multiple specialized components. When you do need multiple agents, the models coordinate themselves through several proven patterns. Strands offers four primary approaches for multi-agent coordination, each suited for different use cases:\nAgents-as-Tools: Hierarchical systems where specialists act as intelligent tools\nSwarms: Autonomous collaboration with self-organizing groups of agents\nGraphs: Structured workflows with defined execution paths\nMeta-Agents: Dynamic agents that can modify their own coordination behavior\nWhether you are building simple hierarchies or complex adaptive systems, Strands keeps the interface clean while providing the configurability needed for production systems.\nAgents-as-Tools: Hierarchical Intelligence When a single agent needs specialized expertise, agents-as-tools turn other agents into intelligent tools. This creates natural hierarchies where an orchestrator agent delegates to specialists, maintaining a clear separation of responsibility while leveraging the model-driven approach at every level. How does the orchestrator agent decide which specialist to consult? The model reasons about the request the same way it reasons about tool selection. When a user asks â€œPlan a business trip to Tokyo and research market opportunities there,â€ the orchestrator recognizes that this requires both travel planning and market research expertise. It might consult the travel advisor first to understand the logistics, and then use those constraints to guide the research analystâ€™s work.\nfrom strands import Agent, tool\n@tool\ndef research_analyst(request: str) -\u0026gt; str:\n\u0026quot;\u0026quot;\u0026quot;Research specialist for market analysis\u0026quot;\u0026quot;\u0026quot;\rresearch_agent = Agent(\rsystem_prompt=\u0026quot;You are a research specialist who gathers and analyzes market information.\u0026quot;,\rtools=[web_search, calculator]\r)\rreturn str(research_agent(request))\r@tool\ndef travel_advisor(request: str) -\u0026gt; str:\n\u0026quot;\u0026quot;\u0026quot;Travel planning specialist for logistics and recommendations\u0026quot;\u0026quot;\u0026quot;\rtravel_agent = Agent(\rsystem_prompt=\u0026quot;You are a travel planning specialist who helps with logistics, bookings, and recommendations.\u0026quot;,\rtools=[flight_search, hotel_search, weather_api]\r)\rreturn str(travel_agent(request))\rOrchestrator decides when to consult specialists and how to synthesize their input executive_assistant = Agent(\nsystem_prompt=\u0026quot;You coordinate with specialists to provide comprehensive assistance.\u0026quot;,\rtools=[research_analyst, travel_advisor]\r)\nThe orchestrator model decides which specialists to consult and how to synthesize their input â€“ no pre-defined workflow is needed. This pattern works well when you need clear boundaries of expertise and want the models to delegate with a leading primary agent.\nSwarms: Autonomous Collaboration Swarms allow agents to collaborate autonomously in a self-organizing way, deciding when to hand off tasks to one another. This works well for creative collaboration and emergent problem-solving, where multiple perspectives add value.\nConsider a market research project where you need to analyze a new product opportunity. A traditional method might follow a rigid sequence: research â†’ analyze â†’ write. But real research is messier â€“ you might discover during analysis that you need supplementary data, or realize while writing that a different analytical framework would be more compelling.\nfrom strands import Agent\nfrom strands.multiagent import Swarm\nresearcher = Agent(name=\u0026ldquo;researcher\u0026rdquo;, system_prompt=\u0026ldquo;You research topics thoroughly\u0026hellip;\u0026rdquo;)\nanalyst = Agent(name=\u0026ldquo;analyst\u0026rdquo;, system_prompt=\u0026ldquo;You analyze data and create insights\u0026hellip;\u0026rdquo;)\nwriter = Agent(name=\u0026ldquo;writer\u0026rdquo;, system_prompt=\u0026ldquo;You write comprehensive reports\u0026hellip;\u0026rdquo;)\nAgents coordinate autonomously through model-driven handoffs market_research_team = Swarm([researcher, analyst, writer])\nIn a swarm, the researcher might start gathering information, then hand off to the analyst when theyâ€™ve found interesting patterns. The analyst might spot gaps and hand back to the researcher for more specific data. The writer might join the conversation early to help shape the research direction based on what will make a compelling story.\nThe key difference from agents-as-tools is shared context. In hierarchies, the orchestrator decides what information is passed between specialists. In swarms, all agents have access to the same conversation history, allowing for more fluid collaboration and high-level instructions. You can tell a swarm â€œresearch this market opportunityâ€ without specifying exactly how the work should be divided â€“ the agents figure it out themselves based on their shared understanding of the evolving task.\nSwarms trade predictable execution for emergent capabilities, making them ideal for complex research tasks, brainstorming sessions, and creative projects where the best approach isn\u0026rsquo;t clear at the outset.\nGraphs: Structured Workflows Graphs provide defined workflows where execution follows pre-determined paths. While the individual agents within a graph use model-driven execution, the graph structure ensures specific sequences and dependencies are maintained.\nThis pattern excels for business processes that require mandatory checkpoints or compliance requirements. For instance, a financial analysis process might require research â†’ risk assessment â†’ regulatory review â†’ final proposal, with each step building on the last and some steps requiring specific sign-offs.\nfrom strands import Agent\nfrom strands.multiagent import GraphBuilder\nbuilder = GraphBuilder()\nbuilder.add_node(researcher, \u0026ldquo;research\u0026rdquo;)\nbuilder.add_node(analyst, \u0026ldquo;analysis\u0026rdquo;)\nbuilder.add_node(writer, \u0026ldquo;report\u0026rdquo;)\nbuilder.add_edge(\u0026ldquo;research\u0026rdquo;, \u0026ldquo;analysis\u0026rdquo;)\nbuilder.add_edge(\u0026ldquo;analysis\u0026rdquo;, \u0026ldquo;report\u0026rdquo;)\ngraph = builder.build()\nAWS teams have used graphs to orchestrate data pipelines, where certain transformations must happen in sequence, and for compliance processes, where audit trails require specific step-by-step documentation. The model-driven approach still applies within each node â€“ the research agent can adjust its methodology based on what it discovers â€“ but the overall flow remains predictable.\nThe trade-off for graphs is their rigidity. If the analyst discovers they need supplementary research, they cannot directly request it â€“ the workflow would need to complete and potentially restart. This makes graphs ideal for well-understood processes but less suited for exploratory or creative work.\nMeta-Agents: Flexible Coordination Meta-agents are single agents equipped with tools that allow them to think about thinking â€“ they can dynamically generate other agents, orchestrate workflows, and engage in recursive problem-solving. They represent the ultimate expression of the model-driven approach: agents that can self-architect their own coordination techniques.\nThink of a meta-agent as a senior architect who can not only solve a problem but also decide how to structure the problem-solving approach. When faced with a complex challenge, it might reason: â€œThis requires both deep research and creative synthesis. I should generate a swarm of specialists and then use a graph to ensure the final deliverable meets quality standards.â€\nfrom strands import Agent\nfrom strands_tools import graph, swarm, use_agent, think, workflow\nmeta_agent = Agent(\nsystem_prompt=\u0026quot;You can dynamically create specialized agents and orchestrate complex workflows.\u0026quot;,\rtools=[graph, swarm, use_agent, think, workflow]\r)\nMeta-agents excel in dynamic systems that evolve over time. The key benefit is their added layer of dynamism â€“ they can generate agents that werenâ€™t pre-defined, evolve new tools for emergent needs, adapt their coordination strategies based on changing conditions, and evolve their problem-solving approach as they learn more about the domain.\nUnlike the other patterns where you pre-define agents and tools, meta-agents can create new specialists and custom-build tools on demand. Analyzing a new technology trend? The meta-agent can generate domain-specialist agents for legal implications, market dynamics, and technical feasibility â€“ agents that didnâ€™t exist until that specific need arose. It can also evolve specialized tools for data harvesting, analytical frameworks, or report formats tailored to that specific domain.\nThis dynamic agent and tool creation allows systems to grow and adapt rather than follow fixed patterns.\nBuilding confidence through evaluation The model-driven approach requires robust evaluation to build confidence that agents are performing as expected, especially as they are making autonomous decisions about tool usage and task coordination.\nSince model-driven agents make dynamic decisions instead of following pre-determined paths, evaluation becomes both more critical and more nuanced. You need to assess not just whether the agent got the right answer, but whether it made good decisions about how to approach the problem.\nThe model-driven approach has changed how we think about evaluation itself. Traditional testing with static datasets becomes insufficient when agents flexibly adjust their behavior. This shift demands new evaluation patterns that match the intelligence of the systems being tested.\nLLM judges provide a powerful method for evaluating agent responses at scale, but they must be carefully calibrated to align with end-user expectations. An LLM judge that prioritizes technical accuracy may overlook nuances that matter to real users, such as tone, helpfulness, or practical applicability.\nLLM-driven simulated users allow for dynamic testing with evolving datasets, generating realistic interaction patterns that static test cases cannot capture. However, these simulated users must also be calibrated to behave like genuine end-users instead of idealized test scripts. The goal is authentic user behavior, not perfect user behavior.\nKey evaluation aspects for model-driven agents include:\nTool choice appropriateness â€“ Did the agent select the right tool for the job?\nReasoning quality â€“ Was the agentâ€™s approach logical?\nAdaptability â€“ How well did the agent handle unexpected situations?\nEfficiency â€“ Did the agent complete the task without unnecessary steps?\nThe non-deterministic nature of model-driven agents means you need statistically meaningful baselines and continuous evaluation to track performance over time. This investment in evaluation allows you to confidently deploy agents that make autonomous decisions.\nThe paradigm shift: From rigid control to dynamic adaptation The model-driven approach represents a fundamental shift in how we think about AI agents. Instead of trying to control every facet of agent behavior through complex orchestration, we provide the right tools, context, and goals, and then let the model dynamically determine the best approach on its own.\nThis shift solves a core problem: pre-defined logic struggles to handle an unpredictable world. Traditional frameworks break when users ask questions in unexpected ways, when APIs return unforeseen responses, or when business requirements change. They fail because they can only handle the situations developers anticipated and programmed.\nModel-driven agents can adapt. When an API call fails, they reason about alternatives. When a user asks something unexpected, they figure out how to help using the available tools. When requirements change, they adjust their approach without code changes. This adaptability comes from leveraging the modelâ€™s reasoning capabilities instead of fighting against them.\nThe approach adopted in Strands allows agents to handle shifting demands and data while reducing the complexity developers need to manage. You can have both: the simplicity to get started fast and the power to configure everything when production demands it.\nThe model-driven approach is not about building better orchestration frameworks. Itâ€™s about realizing that modern LLMs have evolved beyond the need for such rigid guidance. By leveraging the modelâ€™s reasoning to orchestrate, we can build agents that are both more capable and easier to develop. This philosophy is the foundation on which everything else in Strands is built. Visit the Strands Agents website to try it today.\nLooking for more open source content? Be sure to follow AWS Open Source on LinkedIn!\nArron Bailiss Arron Bailiss is a Principal Engineer focused on AI agents at AWS, working at the intersection of artificial intelligence, machine learning, and robotics. He helps shape the future of developer tooling that empowers builders to create complex AI-driven applications.\n"
},
{
	"uri": "https://asapphat.github.io/tanphat.worklog.github.io/4-eventparticipated/4.3-event3/",
	"title": "Event 3",
	"tags": [],
	"description": "",
	"content": "Workshop: Data Science on AWS Location: FPT University HCM Campus\nDate: Thursday, October 16, 2025\nEvent Objectives Explore the entire journey of building a modern Data Science system, from theory to practice Master the end-to-end Data Science workflow on AWS, including storage, processing, and model deployment Gain practical experience with real datasets (IMDb) and applied models (Sentiment Analysis) Analyze trade-offs between Cloud and On-premise infrastructure in terms of cost and performance Speaker List VÄƒn HoÃ ng Kha Cloud Solutions Architect, AWS Community Builder Báº¡ch DoÃ£n VÆ°Æ¡ng Cloud DevOps Engineer, AWS Community Builder Key Content Highlights 1. Role of Cloud in Data Science and Workflow Overview Importance of Cloud: Analyze why modern data science relies on cloud platforms to achieve scalability and high integration, rather than being limited by traditional on-premise infrastructure Data Science Workflow on AWS: Storage: Use Amazon S3 as the core data lake platform ETL/Processing: Use AWS Glue for serverless data integration tasks Modeling: Leverage Amazon SageMaker as the central hub for building, training, and deploying models Overview of AWS\u0026rsquo;s extensive AI/ML ecosystem, including AI services, ML services, and infrastructure 2. Practical Demonstrations Demo 1: Data Processing with AWS Glue: Scenario: Process and clean raw data from the IMDb dataset Technique: Demonstrate effective feature engineering and data preparation methods. The workshop highlights different approaches, from low-code options like SageMaker Canvas to code-first methods using Numpy/Pandas Demo 2: Sentiment Analysis with SageMaker: Scenario: Train and deploy a machine learning model for sentiment analysis from text data Process: Illustrate the \u0026ldquo;Train, Tune, Deploy\u0026rdquo; cycle in SageMaker Studio. The session also covers the \u0026ldquo;Bring Your Own Model (BYOM)\u0026rdquo; concept, showcasing flexibility with frameworks like TensorFlow and PyTorch 3. Strategic Discussion Cloud versus On-Premise: In-depth discussion on cost optimization and performance metrics. Content highlights how cloud elasticity enables testing heavy workloads without large upfront hardware investment for on-premise infrastructure Small Project Guide: Introduction to a project designed after the workshop to reinforce learned skills What I Learned Technical Workflow Unified Workflow: A powerful data science workflow extends beyond just code; it requires seamless integration between storage (S3), cleaning (Glue), and modeling (SageMaker) Tool Selection: Understanding when to use pre-managed services (like Amazon Comprehend or Textract) and when to build custom models on SageMaker is key to achieving efficiency Industry Application Real-world Context: The transition from academic theory to industrial application lies in automation and scalability capabilities Cost Awareness: Successful data projects must balance model accuracy with associated computational costs Application to Work Apply AWS Glue: Propose converting local ETL scripts to AWS Glue for serverless data processing automation on larger datasets Deploy SageMaker: Migrate experimental models from local Jupyter notebooks to SageMaker Studio to standardize training and deployment workflows Implement Project: Execute the small project proposed after the workshop to solidify understanding of IMDb data processing workflow Event Experience The workshop \u0026ldquo;Data Science on AWS\u0026rdquo; served as an important bridge between academic knowledge and business practice.\nDirect Connection: The event linked theoretical knowledge with technologies used by leading global enterprises Practical Perspective: Observing the process of cleaning the IMDb dataset and deploying a Sentiment Analysis model directly on the cloud simplified the complexity of AI on Cloud platforms Expert Guidance: Interacting with AWS Community Builders provided deep insights into the \u0026ldquo;Cloud versus On-premise\u0026rdquo; debate, helping me grasp the strategic value of cloud migration beyond technical features Some photos from the event Overall, the workshop provided a comprehensive Data Science framework, emphasizing the importance of AWS managed tools for achieving flexibility, scalability, and cost optimization.\n"
},
{
	"uri": "https://asapphat.github.io/tanphat.worklog.github.io/5-workshop/5-workshop/5.4-setup-aws/",
	"title": "Setup AWS Environment",
	"tags": [],
	"description": "",
	"content": "Setting Up AWS Environment This section guides you through configuring your AWS environment for the InsightHR platform, including IAM roles, policies, and initial service configuration.\nOverview We\u0026rsquo;ll set up:\nIAM roles for Lambda functions IAM policies for service access AWS region configuration Service quotas verification Bedrock model access Step 1: Configure AWS Region Set your default region to Singapore (ap-southeast-1):\n# Configure AWS CLI aws configure set region ap-southeast-1 # Verify configuration aws configure get region Why Singapore?\nAll required services available Good latency for Southeast Asia Bedrock Claude 3 Haiku available Step 2: Create IAM Role for Lambda Lambda functions need an execution role to access AWS services.\nCreate Trust Policy lambda-trust-policy.json:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;lambda.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34; } ] } Create the Role # Create IAM role aws iam create-role \\ --role-name insighthr-lambda-role \\ --assume-role-policy-document file://lambda-trust-policy.json \\ --description \u0026#34;Execution role for InsightHR Lambda functions\u0026#34; Step 3: Create IAM Policies DynamoDB Access Policy dynamodb-policy.json:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;dynamodb:GetItem\u0026#34;, \u0026#34;dynamodb:PutItem\u0026#34;, \u0026#34;dynamodb:UpdateItem\u0026#34;, \u0026#34;dynamodb:DeleteItem\u0026#34;, \u0026#34;dynamodb:Query\u0026#34;, \u0026#34;dynamodb:Scan\u0026#34;, \u0026#34;dynamodb:BatchGetItem\u0026#34;, \u0026#34;dynamodb:BatchWriteItem\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:dynamodb:ap-southeast-1:*:table/insighthr-*\u0026#34; ] } ] } Create the policy:\naws iam create-policy \\ --policy-name insighthr-dynamodb-policy \\ --policy-document file://dynamodb-policy.json Cognito Access Policy cognito-policy.json:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;cognito-idp:AdminInitiateAuth\u0026#34;, \u0026#34;cognito-idp:AdminCreateUser\u0026#34;, \u0026#34;cognito-idp:AdminSetUserPassword\u0026#34;, \u0026#34;cognito-idp:AdminGetUser\u0026#34;, \u0026#34;cognito-idp:AdminUpdateUserAttributes\u0026#34;, \u0026#34;cognito-idp:ListUsers\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:cognito-idp:ap-southeast-1:*:userpool/*\u0026#34; } ] } Create the policy:\naws iam create-policy \\ --policy-name insighthr-cognito-policy \\ --policy-document file://cognito-policy.json Bedrock Access Policy bedrock-policy.json:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;bedrock:InvokeModel\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:bedrock:ap-southeast-1::foundation-model/anthropic.claude-3-haiku-20240307-v1:0\u0026#34; } ] } Create the policy:\naws iam create-policy \\ --policy-name insighthr-bedrock-policy \\ --policy-document file://bedrock-policy.json Step 4: Attach Policies to Role # Get AWS account ID ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text) # Attach AWS managed policy for Lambda basic execution aws iam attach-role-policy \\ --role-name insighthr-lambda-role \\ --policy-arn arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole # Attach custom policies aws iam attach-role-policy \\ --role-name insighthr-lambda-role \\ --policy-arn arn:aws:iam::${ACCOUNT_ID}:policy/insighthr-dynamodb-policy aws iam attach-role-policy \\ --role-name insighthr-lambda-role \\ --policy-arn arn:aws:iam::${ACCOUNT_ID}:policy/insighthr-cognito-policy aws iam attach-role-policy \\ --role-name insighthr-lambda-role \\ --policy-arn arn:aws:iam::${ACCOUNT_ID}:policy/insighthr-bedrock-policy Step 5: Create IAM Role for API Gateway API Gateway needs a role to write logs to CloudWatch.\napi-gateway-trust-policy.json:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;apigateway.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34; } ] } Create the role:\naws iam create-role \\ --role-name insighthr-apigateway-role \\ --assume-role-policy-document file://api-gateway-trust-policy.json # Attach CloudWatch logs policy aws iam attach-role-policy \\ --role-name insighthr-apigateway-role \\ --policy-arn arn:aws:iam::aws:policy/service-role/AmazonAPIGatewayPushToCloudWatchLogs Step 6: Enable Bedrock Model Access Request access to Claude 3 Haiku model:\nUsing AWS Console:\nNavigate to Amazon Bedrock Click \u0026ldquo;Model access\u0026rdquo; in left sidebar Click \u0026ldquo;Manage model access\u0026rdquo; Find \u0026ldquo;Claude 3 Haiku\u0026rdquo; by Anthropic Check the box Click \u0026ldquo;Request model access\u0026rdquo; Wait for approval (usually instant) Verify Access:\naws bedrock list-foundation-models \\ --region ap-southeast-1 \\ --query \u0026#39;modelSummaries[?contains(modelId, `claude-3-haiku`)].modelId\u0026#39; Step 7: Verify Service Quotas Check service limits for your account:\n# Lambda concurrent executions aws service-quotas get-service-quota \\ --service-code lambda \\ --quota-code L-B99A9384 \\ --region ap-southeast-1 # DynamoDB tables aws service-quotas get-service-quota \\ --service-code dynamodb \\ --quota-code L-F98FE922 \\ --region ap-southeast-1 # API Gateway requests per second aws service-quotas get-service-quota \\ --service-code apigateway \\ --quota-code L-8A5B8E43 \\ --region ap-southeast-1 Step 8: Create S3 Bucket for Deployment Artifacts # Create bucket for Lambda deployment packages aws s3 mb s3://insighthr-deployment-artifacts-${ACCOUNT_ID} \\ --region ap-southeast-1 # Enable versioning aws s3api put-bucket-versioning \\ --bucket insighthr-deployment-artifacts-${ACCOUNT_ID} \\ --versioning-configuration Status=Enabled Step 9: Set Up CloudWatch Log Groups Pre-create log groups for Lambda functions:\n# Create log groups LOG_GROUPS=( \u0026#34;/aws/lambda/insighthr-auth-login-handler\u0026#34; \u0026#34;/aws/lambda/insighthr-auth-register-handler\u0026#34; \u0026#34;/aws/lambda/insighthr-auth-google-handler\u0026#34; \u0026#34;/aws/lambda/insighthr-employees-handler\u0026#34; \u0026#34;/aws/lambda/insighthr-performance-scores-handler\u0026#34; \u0026#34;/aws/lambda/insighthr-chatbot-handler\u0026#34; \u0026#34;/aws/lambda/insighthr-attendance-handler\u0026#34; ) for log_group in \u0026#34;${LOG_GROUPS[@]}\u0026#34;; do aws logs create-log-group \\ --log-group-name $log_group \\ --region ap-southeast-1 # Set retention to 7 days aws logs put-retention-policy \\ --log-group-name $log_group \\ --retention-in-days 7 \\ --region ap-southeast-1 done Step 10: Configure Environment Variables Create a configuration file for environment variables:\nconfig.env:\n# AWS Configuration export AWS_REGION=ap-southeast-1 export AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text) # DynamoDB Tables export USERS_TABLE=insighthr-users-dev export EMPLOYEES_TABLE=insighthr-employees-dev export PERFORMANCE_SCORES_TABLE=insighthr-performance-scores-dev export ATTENDANCE_HISTORY_TABLE=insighthr-attendance-history-dev # Cognito (will be set after Cognito setup) export USER_POOL_ID= export CLIENT_ID= # Bedrock export BEDROCK_MODEL_ID=anthropic.claude-3-haiku-20240307-v1:0 export BEDROCK_REGION=ap-southeast-1 # Lambda Role ARN export LAMBDA_ROLE_ARN=arn:aws:iam::${AWS_ACCOUNT_ID}:role/insighthr-lambda-role Load the configuration:\nsource config.env Verification Checklist Verify your AWS environment is ready:\n# Check IAM role exists aws iam get-role --role-name insighthr-lambda-role # Check policies are attached aws iam list-attached-role-policies --role-name insighthr-lambda-role # Check Bedrock access aws bedrock list-foundation-models \\ --region ap-southeast-1 \\ --query \u0026#39;modelSummaries[?contains(modelId, `claude`)].modelId\u0026#39; # Check S3 bucket exists aws s3 ls s3://insighthr-deployment-artifacts-${ACCOUNT_ID} # Check CloudWatch log groups aws logs describe-log-groups \\ --log-group-name-prefix \u0026#34;/aws/lambda/insighthr\u0026#34; \\ --region ap-southeast-1 IAM Setup Summary Resource Purpose Policies Attached insighthr-lambda-role Lambda execution AWSLambdaBasicExecutionRole, DynamoDB, Cognito, Bedrock insighthr-apigateway-role API Gateway logging AmazonAPIGatewayPushToCloudWatchLogs insighthr-dynamodb-policy DynamoDB access Custom policy for table operations insighthr-cognito-policy Cognito access Custom policy for user management insighthr-bedrock-policy Bedrock access Custom policy for model invocation Security Best Practices âœ… Least Privilege: Policies grant only necessary permissions âœ… Resource Restrictions: Policies limited to insighthr-* resources âœ… Separate Roles: Different roles for different services âœ… CloudWatch Logging: All Lambda functions log to CloudWatch âœ… Versioning: S3 bucket versioning enabled for artifacts\nTroubleshooting IAM Role Creation Fails:\nCheck IAM permissions Verify trust policy syntax Ensure role name is unique Policy Attachment Fails:\nVerify policy ARN is correct Check role exists Ensure you have iam:AttachRolePolicy permission Bedrock Access Denied:\nRequest model access in Bedrock console Wait for approval (usually instant for Haiku) Verify region supports Bedrock Service Quota Issues:\nRequest quota increase in Service Quotas console Use different region if needed Contact AWS support for urgent increases Next Steps With the AWS environment configured, proceed to Database Setup to create DynamoDB tables.\n"
},
{
	"uri": "https://asapphat.github.io/tanphat.worklog.github.io/1-worklog/1.4-week4/",
	"title": "Week 4 Worklog",
	"tags": [],
	"description": "",
	"content": "Week Duration: September 29 - October 5, 2025\nWeek 4 Objectives: Master AWS security services and identity management Learn encryption, secrets management, and compliance Understand threat detection and protection mechanisms Practice implementing security best practices Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Monday - Identity federation with AWS Single Sign-On - IAM Permission Boundaries - Access control with IAM Policies and Conditions 09/29/2025 09/29/2025 AWS IAM Documentation Tuesday - Private access to S3 with VPC Endpoints - Application protection with AWS WAF 09/30/2025 09/30/2025 https://000111.awsstudygroup.com/vi/ https://000026.awsstudygroup.com/vi/ Wednesday - Encryption with AWS Key Management Service (KMS) - Data protection with Amazon Macie - Credential management with AWS Secrets Manager 10/01/2025 10/01/2025 https://000033.awsstudygroup.com/vi/ https://000090.awsstudygroup.com/vi/ https://000096.awsstudygroup.com/vi/ Thursday - Security administration with AWS Firewall Manager - Threat detection with AWS GuardDuty 10/02/2025 10/02/2025 https://000097.awsstudygroup.com/vi/ https://000098.awsstudygroup.com/vi/ Friday - Cross-domain authentication with Amazon Cognito - S3 security best practices - Security integration and testing 10/03/2025 10/03/2025 https://000141.awsstudygroup.com/vi/ AWS Identity Services Saturday - Advanced security configurations - Compliance and audit review - Week 4 security summary 10/04/2025 10/04/2025 AWS Well-Architected Framework Week 4 Achievements: Mastered AWS Identity and Access Management (IAM):\nImplemented Identity Federation with AWS Single Sign-On (SSO) Configured IAM Permission Boundaries for fine-grained access control Designed advanced IAM Policies with conditions Applied principle of least privilege across all services Implemented Data Encryption and Protection:\nManaged encryption keys with AWS Key Management Service (KMS) Configured data encryption at rest and in transit Implemented AWS Secrets Manager for secure credential storage Automated secret rotation and access logging Configured Advanced Security Services:\nDeployed AWS WAF for application layer protection Set up AWS Firewall Manager for centralized security administration Enabled VPC Endpoints for private S3 access Configured Security Groups and Network ACLs Implemented Threat Detection and Response:\nActivated AWS GuardDuty for intelligent threat detection Configured Amazon Macie for sensitive data discovery and protection Enabled AWS Security Hub for centralized security monitoring Set up automated alerts and response procedures Managed System Compliance and Patching:\nUsed EC2 Image Builder for automated AMI creation Configured automated system patching processes Implemented compliance monitoring through Security Hub Conducted audit trail reviews Implemented User Authentication and Authorization:\nConfigured Amazon Cognito for cross-domain authentication Set up Multi-Factor Authentication (MFA) enforcement Created and managed user pools and identity pools Implemented role-based access control (RBAC) Applied Security Best Practices:\nImplemented S3 security best practices and bucket policies Configured encryption for all data stores Set up CloudTrail logging for audit purposes Applied defense-in-depth security strategies Integrated AWS CLI for Security Management:\nUsed AWS CLI for programmatic IAM operations Managed credentials and authentication profiles Performed security configurations via command line Automated security operations with scripts Comprehensive Security Architecture:\nDesigned defense-in-depth security strategy Integrated multiple AWS security services Documented security compliance requirements Prepared for secure AWS infrastructure "
},
{
	"uri": "https://asapphat.github.io/tanphat.worklog.github.io/4-eventparticipated/4.4-event4/",
	"title": "Event 4",
	"tags": [],
	"description": "",
	"content": "AWS Mastery #1: AI/ML \u0026amp; GenAI Workshop Location: AWS Event Hall, L26 Bitexco Tower, HCMC\nDate: Saturday, November 15, 2025\nLearning Report: \u0026ldquo;AWS AI/ML \u0026amp; GenAI Workshop\u0026rdquo; Event Objectives Provide a comprehensive overview of Artificial Intelligence/Machine Learning (AI/ML) market trends and landscape in Vietnam Provide practical guidance for building complete end-to-end ML models on Amazon SageMaker platform Deep dive into Generative AI with Amazon Bedrock, including Foundation Models, Agents, and Guardrails Equip necessary skills for Prompt Engineering and building RAG (Retrieval-Augmented Generation) applications Speaker List AWS Expert Team Key Content Highlights Welcome \u0026amp; Overview Market Overview: Update on the comprehensive landscape of artificial intelligence and machine learning (AI/ML) in the Vietnamese market context Networking: Conduct ice-breaker activities and networking to create a comfortable atmosphere for the workshop AWS AI/ML Service Overview (SageMaker) Comprehensive ML Platform: Learn the standard workflow in Amazon SageMaker, from data preparation, labeling to training and model tuning MLOps Integration: Guide on integrating machine learning operations (MLOps) to automate model deployment and management processes SageMaker Studio Demo: Directly experience the interface and functions of SageMaker Studio through practical demonstrations Generative AI with Amazon Bedrock Foundation Models Selection: Compare and provide guidance for selecting appropriate foundation models like Claude, Llama, Titan based on specific requirements Prompt Engineering Techniques: Methods for optimizing input commands, including Chain-of-Thought and Few-shot learning RAG Architecture: Analyze \u0026ldquo;Retrieval-Augmented Generation\u0026rdquo; architecture and how to integrate enterprise Knowledge Base to enhance accuracy and authenticity of AI responses Advanced Features: Guide on using Bedrock Agents for complex, multi-step workflows and Guardrails to establish safeguards ensuring safe and appropriate content Live Demo: Build a complete GenAI Chatbot at the workshop using Amazon Bedrock What I Learned Platform Capabilities SageMaker is a powerful tool designed to standardize and efficiently manage workflows for traditional Machine Learning tasks (prediction) Bedrock provides the fastest approach to GenAI through API, eliminating the need for managing complex infrastructure Strategic Implementation RAG and Agents are two cutting-edge technologies helping transform GenAI applications from simple chat functions to solving complex business problems Guardrails are essential components ensuring AI operates within safe boundaries, complying with company rules and policies Application to Work Deploy MLOps: Apply standardized processes learned on SageMaker to manage the lifecycle of machine learning models in current projects Build RAG: Experiment with integrating internal documents into Bedrock Knowledge Base to develop specialized information retrieval assistants Optimize Prompts: Apply Chain-of-Thought techniques and other methods to significantly improve quality and depth of responses from existing chatbots Evaluate Models: Use provided criteria to select the optimal foundation model (Claude vs. Llama) for cost and performance on each specific use case Event Experience The workshop was a balanced and efficient combination of traditional Machine Learning and modern Generative AI, providing a solid and comprehensive foundation of knowledge.\nPractice \u0026amp; Demonstrations The SageMaker Studio walkthrough helped me clearly visualize a professional, standardized working environment for Data Scientists Demo building Chatbot with Bedrock was an important highlight, demonstrating that creating a complete GenAI application has become faster and more accessible than ever Market Information The introduction to the AI market in Vietnam helped me position my understanding and grasp the potential opportunities of enterprises in the broader technology trends Some photos from the event In summary, this event equipped me with a comprehensive \u0026ldquo;toolkit\u0026rdquo;: from SageMaker for predictive models to Bedrock for generative models, ready for upcoming large-scale AI project deployments.\n"
},
{
	"uri": "https://asapphat.github.io/tanphat.worklog.github.io/4-eventparticipated/",
	"title": "Events Attended",
	"tags": [],
	"description": "",
	"content": "Throughout the internship period, I participated in 7 significant events, each providing valuable learning experiences, new knowledge, and memorable moments.\nEvent 1 Event Name: Vietnam Cloud Day 2025: GenAI and Data Track\nDate \u0026amp; Time: Friday, September 26, 2025\nLocation: AWS Event Hall, L26 Bitexco Tower, HCMC\nRole: Participant\nBrief Description: Workshop covering Agentic AI, Unified Data Foundation, GenAI roadmap, AI-Driven SDLC, Security, and AI Agents for business applications.\nEvent 2 Event Name: AI-Driven Development Life Cycle\nDate \u0026amp; Time: Friday, October 3, 2025\nLocation: AWS Event Hall, L26 Bitexco Tower, HCMC\nRole: Participant\nBrief Description: Practical workshop on AI-driven SDLC paradigm shift, Amazon Q Developer demonstrations, and Kiro tool capabilities for development optimization.\nEvent 3 Event Name: Workshop: Data Science on AWS\nDate \u0026amp; Time: Thursday, October 16, 2025\nLocation: FPT University HCM Campus\nRole: Participant\nBrief Description: End-to-end Data Science workflow on AWS, including data processing with AWS Glue, sentiment analysis with SageMaker, and cost-performance trade-off analysis.\nEvent 4 Event Name: AWS Mastery #1: AI/ML \u0026amp; GenAI Workshop\nDate \u0026amp; Time: Saturday, November 15, 2025\nLocation: AWS Event Hall, L26 Bitexco Tower, HCMC\nRole: Participant\nBrief Description: Comprehensive overview of AI/ML market trends, SageMaker platform for ML models, Generative AI with Amazon Bedrock, Prompt Engineering, and RAG architecture.\nEvent 5 Event Name: AWS Mastery 2: AWS DevOps \u0026amp; Modern Operations\nDate \u0026amp; Time: Monday, November 17, 2025\nLocation: AWS Event Hall, L26 Bitexco Tower, HCMC\nRole: Participant\nBrief Description: DevOps mindset development, complete CI/CD workflow with AWS Developer Tools, Infrastructure as Code with CloudFormation and CDK, and comprehensive observability setup.\nEvent 6 Event Name: AWS Security Specialty Workshop\nDate \u0026amp; Time: Saturday, November 29, 2025\nLocation: AWS Event Hall, L26 Bitexco Tower, HCMC\nRole: Participant\nBrief Description: Deep dive into 5 Core Security Pillars (IAM, Detection, Infrastructure, Data Protection, Incident Response), Zero Trust mindset, and automation-first security strategies.\nEvent 7 Event Name: CloudThinker: Agentic AI \u0026amp; Orchestration on AWS\nDate \u0026amp; Time: Friday, December 5, 2025\nLocation: AWS Event Hall, L26 Bitexco Tower, HCMC\nRole: Participant\nBrief Description: Advanced technical workshop on AWS Bedrock Agent Core, Agentic Workflows, multi-agent orchestration, context optimization, and hands-on CloudThinker Hack prototype building.\nThese events provided a comprehensive learning journey from foundational cloud architecture to advanced AI/ML applications, significantly enhancing both technical expertise and professional development throughout the internship program.\n"
},
{
	"uri": "https://asapphat.github.io/tanphat.worklog.github.io/5-workshop/5-workshop/5.5-database-setup/",
	"title": "Database Setup (DynamoDB)",
	"tags": [],
	"description": "",
	"content": "DynamoDB Database Setup In this section, you\u0026rsquo;ll create and configure the DynamoDB tables required for the InsightHR platform.\nOverview InsightHR uses Amazon DynamoDB, a fully managed NoSQL database service that provides fast and predictable performance with seamless scalability. We\u0026rsquo;ll create 4 main tables:\nUsers Table - Authentication and user profiles Employees Table - Employee master data Performance Scores Table - Quarterly performance tracking Attendance History Table - Check-in/check-out records Table Design Principles DynamoDB Best Practices:\nUse partition keys for even data distribution Add sort keys for range queries Create GSIs for alternate query patterns Denormalize data for read performance Use on-demand billing for variable workloads Step 1: Create Users Table Table Configuration:\nTable Name: insighthr-users-dev Partition Key: userId (String) Billing Mode: On-demand Using AWS Console:\nNavigate to DynamoDB in AWS Console Click \u0026ldquo;Create table\u0026rdquo; Enter table name: insighthr-users-dev Partition key: userId (String) Table settings: Use default settings Billing mode: On-demand Click \u0026ldquo;Create table\u0026rdquo; Add Global Secondary Index:\nSelect the table Go to \u0026ldquo;Indexes\u0026rdquo; tab Click \u0026ldquo;Create index\u0026rdquo; Index name: email-index Partition key: email (String) Projected attributes: All Click \u0026ldquo;Create index\u0026rdquo; Using AWS CLI:\n# Create table aws dynamodb create-table \\ --table-name insighthr-users-dev \\ --attribute-definitions \\ AttributeName=userId,AttributeType=S \\ AttributeName=email,AttributeType=S \\ --key-schema \\ AttributeName=userId,KeyType=HASH \\ --billing-mode PAY_PER_REQUEST \\ --region ap-southeast-1 # Create GSI aws dynamodb update-table \\ --table-name insighthr-users-dev \\ --attribute-definitions \\ AttributeName=email,AttributeType=S \\ --global-secondary-index-updates \\ \u0026#34;[{\\\u0026#34;Create\\\u0026#34;:{\\\u0026#34;IndexName\\\u0026#34;:\\\u0026#34;email-index\\\u0026#34;,\\\u0026#34;KeySchema\\\u0026#34;:[{\\\u0026#34;AttributeName\\\u0026#34;:\\\u0026#34;email\\\u0026#34;,\\\u0026#34;KeyType\\\u0026#34;:\\\u0026#34;HASH\\\u0026#34;}],\\\u0026#34;Projection\\\u0026#34;:{\\\u0026#34;ProjectionType\\\u0026#34;:\\\u0026#34;ALL\\\u0026#34;}}}]\u0026#34; \\ --region ap-southeast-1 Sample Data Structure:\n{ \u0026#34;userId\u0026#34;: \u0026#34;user-123\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;admin@example.com\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Admin User\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;Admin\u0026#34;, \u0026#34;employeeId\u0026#34;: \u0026#34;DEV-001\u0026#34;, \u0026#34;department\u0026#34;: \u0026#34;DEV\u0026#34;, \u0026#34;isActive\u0026#34;: true, \u0026#34;createdAt\u0026#34;: \u0026#34;2025-12-09T00:00:00Z\u0026#34;, \u0026#34;updatedAt\u0026#34;: \u0026#34;2025-12-09T00:00:00Z\u0026#34; } Step 2: Create Employees Table Table Configuration:\nTable Name: insighthr-employees-dev Partition Key: employeeId (String) Billing Mode: On-demand Using AWS CLI:\n# Create table aws dynamodb create-table \\ --table-name insighthr-employees-dev \\ --attribute-definitions \\ AttributeName=employeeId,AttributeType=S \\ AttributeName=department,AttributeType=S \\ --key-schema \\ AttributeName=employeeId,KeyType=HASH \\ --billing-mode PAY_PER_REQUEST \\ --region ap-southeast-1 # Create GSI for department queries aws dynamodb update-table \\ --table-name insighthr-employees-dev \\ --attribute-definitions \\ AttributeName=department,AttributeType=S \\ --global-secondary-index-updates \\ \u0026#34;[{\\\u0026#34;Create\\\u0026#34;:{\\\u0026#34;IndexName\\\u0026#34;:\\\u0026#34;department-index\\\u0026#34;,\\\u0026#34;KeySchema\\\u0026#34;:[{\\\u0026#34;AttributeName\\\u0026#34;:\\\u0026#34;department\\\u0026#34;,\\\u0026#34;KeyType\\\u0026#34;:\\\u0026#34;HASH\\\u0026#34;}],\\\u0026#34;Projection\\\u0026#34;:{\\\u0026#34;ProjectionType\\\u0026#34;:\\\u0026#34;ALL\\\u0026#34;}}}]\u0026#34; \\ --region ap-southeast-1 Sample Data Structure:\n{ \u0026#34;employeeId\u0026#34;: \u0026#34;DEV-001\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;john.doe@example.com\u0026#34;, \u0026#34;department\u0026#34;: \u0026#34;DEV\u0026#34;, \u0026#34;position\u0026#34;: \u0026#34;Senior\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;active\u0026#34;, \u0026#34;hireDate\u0026#34;: \u0026#34;2024-01-01\u0026#34;, \u0026#34;managerId\u0026#34;: \u0026#34;DEV-000\u0026#34;, \u0026#34;createdAt\u0026#34;: \u0026#34;2025-12-09T00:00:00Z\u0026#34;, \u0026#34;updatedAt\u0026#34;: \u0026#34;2025-12-09T00:00:00Z\u0026#34; } Step 3: Create Performance Scores Table Table Configuration:\nTable Name: insighthr-performance-scores-dev Partition Key: employeeId (String) Sort Key: period (String) Billing Mode: On-demand Using AWS CLI:\n# Create table aws dynamodb create-table \\ --table-name insighthr-performance-scores-dev \\ --attribute-definitions \\ AttributeName=employeeId,AttributeType=S \\ AttributeName=period,AttributeType=S \\ AttributeName=department,AttributeType=S \\ --key-schema \\ AttributeName=employeeId,KeyType=HASH \\ AttributeName=period,KeyType=RANGE \\ --billing-mode PAY_PER_REQUEST \\ --region ap-southeast-1 # Create GSI for department-period queries aws dynamodb update-table \\ --table-name insighthr-performance-scores-dev \\ --attribute-definitions \\ AttributeName=department,AttributeType=S \\ AttributeName=period,AttributeType=S \\ --global-secondary-index-updates \\ \u0026#34;[{\\\u0026#34;Create\\\u0026#34;:{\\\u0026#34;IndexName\\\u0026#34;:\\\u0026#34;department-period-index\\\u0026#34;,\\\u0026#34;KeySchema\\\u0026#34;:[{\\\u0026#34;AttributeName\\\u0026#34;:\\\u0026#34;department\\\u0026#34;,\\\u0026#34;KeyType\\\u0026#34;:\\\u0026#34;HASH\\\u0026#34;},{\\\u0026#34;AttributeName\\\u0026#34;:\\\u0026#34;period\\\u0026#34;,\\\u0026#34;KeyType\\\u0026#34;:\\\u0026#34;RANGE\\\u0026#34;}],\\\u0026#34;Projection\\\u0026#34;:{\\\u0026#34;ProjectionType\\\u0026#34;:\\\u0026#34;ALL\\\u0026#34;}}}]\u0026#34; \\ --region ap-southeast-1 Sample Data Structure:\n{ \u0026#34;scoreId\u0026#34;: \u0026#34;score-123\u0026#34;, \u0026#34;employeeId\u0026#34;: \u0026#34;DEV-001\u0026#34;, \u0026#34;period\u0026#34;: \u0026#34;2025-Q1\u0026#34;, \u0026#34;employeeName\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;department\u0026#34;: \u0026#34;DEV\u0026#34;, \u0026#34;position\u0026#34;: \u0026#34;Senior\u0026#34;, \u0026#34;overallScore\u0026#34;: 85.5, \u0026#34;kpiScores\u0026#34;: { \u0026#34;KPI\u0026#34;: 85.0, \u0026#34;completed_task\u0026#34;: 88.0, \u0026#34;feedback_360\u0026#34;: 83.5 }, \u0026#34;calculatedAt\u0026#34;: \u0026#34;2025-12-09T00:00:00Z\u0026#34;, \u0026#34;createdAt\u0026#34;: \u0026#34;2025-12-09T00:00:00Z\u0026#34;, \u0026#34;updatedAt\u0026#34;: \u0026#34;2025-12-09T00:00:00Z\u0026#34; } Step 4: Create Attendance History Table Table Configuration:\nTable Name: insighthr-attendance-history-dev Partition Key: employeeId (String) Sort Key: date (String) Billing Mode: On-demand Using AWS CLI:\n# Create table aws dynamodb create-table \\ --table-name insighthr-attendance-history-dev \\ --attribute-definitions \\ AttributeName=employeeId,AttributeType=S \\ AttributeName=date,AttributeType=S \\ AttributeName=department,AttributeType=S \\ --key-schema \\ AttributeName=employeeId,KeyType=HASH \\ AttributeName=date,KeyType=RANGE \\ --billing-mode PAY_PER_REQUEST \\ --region ap-southeast-1 # Create GSI for date queries aws dynamodb update-table \\ --table-name insighthr-attendance-history-dev \\ --attribute-definitions \\ AttributeName=date,AttributeType=S \\ --global-secondary-index-updates \\ \u0026#34;[{\\\u0026#34;Create\\\u0026#34;:{\\\u0026#34;IndexName\\\u0026#34;:\\\u0026#34;date-index\\\u0026#34;,\\\u0026#34;KeySchema\\\u0026#34;:[{\\\u0026#34;AttributeName\\\u0026#34;:\\\u0026#34;date\\\u0026#34;,\\\u0026#34;KeyType\\\u0026#34;:\\\u0026#34;HASH\\\u0026#34;}],\\\u0026#34;Projection\\\u0026#34;:{\\\u0026#34;ProjectionType\\\u0026#34;:\\\u0026#34;ALL\\\u0026#34;}}}]\u0026#34; \\ --region ap-southeast-1 # Create GSI for department-date queries aws dynamodb update-table \\ --table-name insighthr-attendance-history-dev \\ --attribute-definitions \\ AttributeName=department,AttributeType=S \\ AttributeName=date,AttributeType=S \\ --global-secondary-index-updates \\ \u0026#34;[{\\\u0026#34;Create\\\u0026#34;:{\\\u0026#34;IndexName\\\u0026#34;:\\\u0026#34;department-date-index\\\u0026#34;,\\\u0026#34;KeySchema\\\u0026#34;:[{\\\u0026#34;AttributeName\\\u0026#34;:\\\u0026#34;department\\\u0026#34;,\\\u0026#34;KeyType\\\u0026#34;:\\\u0026#34;HASH\\\u0026#34;},{\\\u0026#34;AttributeName\\\u0026#34;:\\\u0026#34;date\\\u0026#34;,\\\u0026#34;KeyType\\\u0026#34;:\\\u0026#34;RANGE\\\u0026#34;}],\\\u0026#34;Projection\\\u0026#34;:{\\\u0026#34;ProjectionType\\\u0026#34;:\\\u0026#34;ALL\\\u0026#34;}}}]\u0026#34; \\ --region ap-southeast-1 Sample Data Structure:\n{ \u0026#34;employeeId\u0026#34;: \u0026#34;DEV-001\u0026#34;, \u0026#34;date\u0026#34;: \u0026#34;2025-12-09\u0026#34;, \u0026#34;employeeName\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;department\u0026#34;: \u0026#34;DEV\u0026#34;, \u0026#34;checkInTime\u0026#34;: \u0026#34;09:00:00\u0026#34;, \u0026#34;checkOutTime\u0026#34;: \u0026#34;18:00:00\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;present\u0026#34;, \u0026#34;notes\u0026#34;: \u0026#34;On time\u0026#34;, \u0026#34;createdAt\u0026#34;: \u0026#34;2025-12-09T09:00:00Z\u0026#34;, \u0026#34;updatedAt\u0026#34;: \u0026#34;2025-12-09T18:00:00Z\u0026#34; } Step 5: Verify Table Creation Using AWS Console:\nNavigate to DynamoDB Check that all 4 tables are listed Verify each table status is \u0026ldquo;Active\u0026rdquo; Check GSIs are created and active Using AWS CLI:\n# List all tables aws dynamodb list-tables --region ap-southeast-1 # Describe specific table aws dynamodb describe-table \\ --table-name insighthr-users-dev \\ --region ap-southeast-1 Step 6: Load Sample Data (Optional) For testing purposes, you can load sample data into the tables.\nCreate sample data file (sample-users.json):\n{ \u0026#34;insighthr-users-dev\u0026#34;: [ { \u0026#34;PutRequest\u0026#34;: { \u0026#34;Item\u0026#34;: { \u0026#34;userId\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;user-001\u0026#34;}, \u0026#34;email\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;admin@example.com\u0026#34;}, \u0026#34;name\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;Admin User\u0026#34;}, \u0026#34;role\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;Admin\u0026#34;}, \u0026#34;employeeId\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;DEV-001\u0026#34;}, \u0026#34;department\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;DEV\u0026#34;}, \u0026#34;isActive\u0026#34;: {\u0026#34;BOOL\u0026#34;: true}, \u0026#34;createdAt\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;2025-12-09T00:00:00Z\u0026#34;}, \u0026#34;updatedAt\u0026#34;: {\u0026#34;S\u0026#34;: \u0026#34;2025-12-09T00:00:00Z\u0026#34;} } } } ] } Load data:\naws dynamodb batch-write-item \\ --request-items file://sample-users.json \\ --region ap-southeast-1 Database Configuration Summary Table Partition Key Sort Key GSIs Purpose insighthr-users-dev userId - email-index User authentication insighthr-employees-dev employeeId - department-index Employee data insighthr-performance-scores-dev employeeId period department-period-index Performance tracking insighthr-attendance-history-dev employeeId date date-index, department-date-index Attendance records Best Practices Implemented âœ… Partition Key Design: Even distribution of data âœ… Sort Keys: Enable range queries for time-series data âœ… GSIs: Support alternate query patterns âœ… On-Demand Billing: Cost-effective for variable workloads âœ… Naming Convention: Consistent table naming with environment suffix\nTroubleshooting Table Creation Fails:\nCheck IAM permissions for DynamoDB Verify region is correct Ensure table name doesn\u0026rsquo;t already exist GSI Creation Fails:\nWait for table to be ACTIVE before creating GSI Check attribute definitions match Verify IAM permissions High Costs:\nUse on-demand billing for development Monitor read/write capacity units Optimize query patterns Next Steps With the database tables created, proceed to Authentication Service to set up Cognito and authentication Lambda functions.\n"
},
{
	"uri": "https://asapphat.github.io/tanphat.worklog.github.io/5-workshop/5-workshop/",
	"title": "Workshop",
	"tags": [],
	"description": "",
	"content": "InsightHR - Serverless HR Automation Platform Workshop Overview InsightHR is a modern, fully serverless HR automation platform built on AWS that demonstrates best practices for cloud-native application development. This workshop guides you through building and deploying a complete production-ready application using AWS services.\nWhat You\u0026rsquo;ll Build A comprehensive HR management system featuring:\nEmployee Management: Full CRUD operations with advanced filtering Performance Tracking: Quarterly performance scores and KPI management Attendance System: Real-time check-in/check-out with history tracking AI Chatbot: Natural language queries powered by AWS Bedrock (Claude 3) Dashboard Analytics: Interactive performance visualization Role-Based Access Control: Admin, Manager, and Employee roles Authentication: Email/password and Google OAuth via AWS Cognito Architecture Highlights âœ… 100% Serverless - No EC2 instances to manage âœ… Scalable - Auto-scales with demand âœ… Cost-Effective - Pay only for what you use âœ… Secure - Built-in security with Cognito and IAM âœ… Modern Stack - React + TypeScript + Python âœ… Production-Ready - CloudWatch monitoring and custom domain AWS Services Used Frontend: S3 + CloudFront + Route53 Backend: Lambda + API Gateway + DynamoDB Authentication: Cognito User Pools AI/ML: Amazon Bedrock (Claude 3 Haiku) Monitoring: CloudWatch + Synthetics Canaries Security: IAM + ACM (SSL Certificates) Workshop Content Workshop Overview Prerequisites Project Architecture Setup AWS Environment Database Setup (DynamoDB) Authentication Service Backend Services Frontend Development Deployment Testing \u0026amp; Monitoring Cleanup Learning Outcomes By completing this workshop, you will learn:\nHow to design and implement serverless architectures Best practices for AWS Lambda and API Gateway DynamoDB data modeling and optimization AWS Cognito authentication flows Integration with AWS Bedrock for AI capabilities CloudFront CDN configuration Infrastructure as Code principles Production deployment strategies Cost optimization techniques Prerequisites AWS Account with appropriate permissions Basic knowledge of JavaScript/TypeScript and Python Familiarity with React framework Understanding of REST APIs AWS CLI installed and configured Estimated Time Full Workshop: 4-6 hours Core Features Only: 2-3 hours Cost Estimate Running this workshop will incur minimal AWS costs:\nDynamoDB: ~$0.50/month (on-demand pricing) Lambda: Free tier covers most usage S3 + CloudFront: ~$1-2/month API Gateway: ~$0.10/month Bedrock: ~$0.0004 per query Total: ~$2-5/month for development Remember to clean up resources after completing the workshop to avoid ongoing charges.\nSupport For questions or issues during the workshop:\nCheck the troubleshooting sections in each module Review AWS documentation links provided Refer to the GitHub repository for code samples Let\u0026rsquo;s get started! ğŸš€\n"
},
{
	"uri": "https://asapphat.github.io/tanphat.worklog.github.io/1-worklog/1.5-week5/",
	"title": "Week 5 Worklog",
	"tags": [],
	"description": "",
	"content": "Week Duration: October 6 - 12, 2025\nWeek 5 Objectives: Master AWS data protection and backup services Learn network integration and management Understand messaging services (SQS, SNS) Practice backup and recovery strategies Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Monday - AWS Backup fundamentals and strategies - Backup planning and policies - Disaster recovery planning 10/06/2025 10/06/2025 https://000013.awsstudygroup.com/vi/ Tuesday - VPC Peering configuration and management - Cross-VPC communication - Network integration patterns 10/07/2025 10/07/2025 AWS Documentation Wednesday - AWS Transit Gateway for centralized network management - Multi-VPC connectivity - Routing and routing policies 10/08/2025 10/08/2025 https://000020.awsstudygroup.com/vi/ Thursday - Amazon SQS (Simple Queue Service) for message queuing - Amazon SNS (Simple Notification Service) for pub/sub - Message routing and filtering 10/09/2025 10/09/2025 https://000077.awsstudygroup.com/vi/ Friday - Hands-on Practice: + Create backup plans + Set up VPC Peering connections + Configure SQS and SNS + Test messaging workflows 10/10/2025 10/10/2025 AWS Documentation Saturday - Advanced messaging patterns - Backup and recovery testing - Week 5 summary 10/11/2025 10/11/2025 https://cloudjourney.awsstudygroup.com/ Week 5 Achievements: Mastered AWS Backup and Disaster Recovery:\nCreated comprehensive backup plans for multiple AWS resources Configured backup policies aligned with compliance requirements Implemented disaster recovery (DR) strategies Practiced backup restore and recovery procedures Validated Recovery Time Objective (RTO) and Recovery Point Objective (RPO) Set up backup retention policies and lifecycle rules Implemented VPC Peering Connections:\nSuccessfully configured VPC peering between multiple VPCs Established cross-VPC communication and routing Applied network integration patterns and best practices Managed route tables and network ACLs for peered VPCs Understood VPC Peering limits and use cases Tested connectivity between peered VPCs Mastered AWS Transit Gateway:\nDeployed AWS Transit Gateway for centralized network management Established multi-VPC connectivity through Transit Gateway Configured advanced routing policies and route propagation Connected on-premises networks with AWS using Transit Gateway Understood advantages of Transit Gateway over VPC Peering Implemented network segmentation and traffic isolation Configured Amazon Messaging Services:\nSet up Amazon SQS (Simple Queue Service) for message queuing Configured Amazon SNS (Simple Notification Service) for pub/sub patterns Implemented message routing and filtering Practiced event-driven messaging workflows Integrated SQS and SNS with other AWS services Set up Dead Letter Queues (DLQ) for error handling Hands-on Lab Practice:\nCreated and tested comprehensive backup and recovery plans Successfully configured VPC Peering connections Set up SQS queues and SNS topics for messaging Tested end-to-end messaging workflows Configured Security Groups with precise ingress/egress rules Integrated EC2 instances with VPC networking Validated cross-VPC communication Advanced Network Architecture:\nDesigned hybrid network connectivity patterns Built highly available and resilient network architecture Implemented disaster recovery with backup and failover strategies Applied network best practices for production environments Understood network scalability and performance optimization CloudWatch Monitoring and Optimization:\nConfigured CloudWatch metrics for resource monitoring Set up alarms for backup and recovery events Implemented cost tracking and optimization strategies Applied performance tuning best practices Monitored messaging service performance and throughput "
},
{
	"uri": "https://asapphat.github.io/tanphat.worklog.github.io/4-eventparticipated/4.5-event5/",
	"title": "Event 5",
	"tags": [],
	"description": "",
	"content": "AWS Mastery 2: AWS DevOps \u0026amp; Modern Operations Location: AWS Event Hall, L26 Bitexco Tower, HCMC\nDate: Monday, November 17, 2025\nLearning Report: \u0026ldquo;AWS DevOps \u0026amp; Modern Operations\u0026rdquo; Event Objectives Build DevOps mindset and master performance measurement through DORA metrics Establish complete CI/CD workflow, automating with AWS Developer Tools suite Modernize infrastructure management through code (Infrastructure as Code - IaC) with CloudFormation and CDK Deploy containerized applications (Docker) on services like ECS, EKS and App Runner Build comprehensive Observability systems for distributed applications Speaker List AWS Expert Team (Deep specialists in DevOps, Container and Observability) Key Content Highlights DevOps Mindset \u0026amp; CI/CD DORA Metrics: Emphasize importance of performance metrics like Deployment Frequency, Lead Time for Changes, and Mean Time to Recover (MTTR) Git Strategy: Compare source code management strategies like GitFlow and Trunk-based development Pipeline Automation: Illustrate complete CI/CD workflow: from CodeCommit (code repository), CodeBuild (build/test), to CodeDeploy (deployment) orchestrated by CodePipeline Deployment Strategies: Introduce safe deployment techniques to minimize risk: Blue/Green, Canary, and Rolling updates Infrastructure as Code (IaC) CloudFormation: Guide managing infrastructure through templates, including Stacks concept and drift detection mechanisms AWS CDK: Use popular programming languages to define infrastructure, leveraging reusable \u0026ldquo;Constructs\u0026rdquo; and design patterns IaC Selection: Discuss criteria for choosing between CloudFormation and CDK depending on project requirements and scale Container Services Spectrum of Compute: From image management with ECR to orchestration options: ECS (simple, deep AWS integration), EKS (open Kubernetes standard), and App Runner (maximum simplification for web apps/API) Microservices Deployment: Compare and demonstrate deploying microservices across different platforms Monitoring \u0026amp; Observability Comprehensive Observability: Combine CloudWatch (for Metrics, Logs, Alarms) and X-Ray (for Distributed Tracing) for deep and comprehensive system visibility Best Practices: Set up visual monitoring dashboards and efficient on-call processes What I Learned Automation First Priority CI/CD is a cultural foundation that minimizes human error and accelerates product release velocity IaC is a mandatory standard for all modern infrastructure, ensuring Dev/Test/Prod environments remain consistent and reproducible Operational Excellence Observability is more critical than simple Monitoring, especially in Microservices architecture, as it enables rapid root cause analysis through tracing Choosing appropriate deployment strategies (like Blue/Green) is key to achieving near-zero downtime Application to Work Restructure Pipeline: Convert current manual build processes to AWS CodePipeline integrating automated testing steps Apply CDK: Start using AWS CDK to define infrastructure for new projects, moving away from direct Console operations Containerize: Package applications in Docker and experiment deploying them to AWS App Runner for smaller services Set Up Tracing: Integrate AWS X-Ray into applications to monitor latency and communication flows between microservices Event Experience The event systematically organized knowledge seamlessly, progressing from mindset formation to tool introduction and operational procedures.\nIntegration Process The \u0026ldquo;Full CI/CD pipeline walkthrough\u0026rdquo; demo was impressive, helping me clearly visualize the entire journey of source code from developer machine to Production environment I clearly understood the differences and specific use cases of ECS versus EKS, increasing confidence when proposing containerization solutions to the company High Practicality Lessons about Deployment strategies (like Feature flags, Canary) have high real-world application value, addressing the team\u0026rsquo;s \u0026ldquo;deployment anxiety\u0026rdquo; The career roadmap guidance at the end provided clear DevOps skill development roadmap Some photos from the event In summary, the workshop clarified the tight and inseparable connection between Code, Infrastructure as Code, and Observability in modern operations.\n"
},
{
	"uri": "https://asapphat.github.io/tanphat.worklog.github.io/5-workshop/5-workshop/5.6-authentication/",
	"title": "Authentication Service",
	"tags": [],
	"description": "",
	"content": "Authentication Service Setup This section covers setting up AWS Cognito for user authentication and implementing authentication Lambda functions.\nOverview The authentication system includes:\nAWS Cognito User Pool - User management and authentication Login Handler - Email/password authentication Registration Handler - New user signup Google OAuth Handler - Social login integration Password Reset - Password recovery workflow Step 1: Create Cognito User Pool Using AWS Console Navigate to Amazon Cognito\nClick \u0026ldquo;Create user pool\u0026rdquo;\nConfigure sign-in experience:\nSign-in options: Email User name requirements: Allow email addresses Click \u0026ldquo;Next\u0026rdquo; Configure security requirements:\nPassword policy: Cognito defaults Multi-factor authentication: Optional User account recovery: Email only Click \u0026ldquo;Next\u0026rdquo; Configure sign-up experience:\nSelf-registration: Enabled Required attributes: name, email Click \u0026ldquo;Next\u0026rdquo; Configure message delivery:\nEmail provider: Send email with Cognito FROM email address: no-reply@verificationemail.com Click \u0026ldquo;Next\u0026rdquo; Integrate your app:\nUser pool name: insighthr-user-pool App client name: insighthr-web-client Client secret: Don\u0026rsquo;t generate Authentication flows: ALLOW_USER_PASSWORD_AUTH, ALLOW_REFRESH_TOKEN_AUTH Click \u0026ldquo;Next\u0026rdquo; Review and create\nUsing AWS CLI # Create user pool aws cognito-idp create-user-pool \\ --pool-name insighthr-user-pool \\ --policies \u0026#39;{ \u0026#34;PasswordPolicy\u0026#34;: { \u0026#34;MinimumLength\u0026#34;: 8, \u0026#34;RequireUppercase\u0026#34;: true, \u0026#34;RequireLowercase\u0026#34;: true, \u0026#34;RequireNumbers\u0026#34;: true, \u0026#34;RequireSymbols\u0026#34;: false } }\u0026#39; \\ --auto-verified-attributes email \\ --username-attributes email \\ --schema \u0026#39;[ { \u0026#34;Name\u0026#34;: \u0026#34;email\u0026#34;, \u0026#34;AttributeDataType\u0026#34;: \u0026#34;String\u0026#34;, \u0026#34;Required\u0026#34;: true, \u0026#34;Mutable\u0026#34;: true }, { \u0026#34;Name\u0026#34;: \u0026#34;name\u0026#34;, \u0026#34;AttributeDataType\u0026#34;: \u0026#34;String\u0026#34;, \u0026#34;Required\u0026#34;: true, \u0026#34;Mutable\u0026#34;: true } ]\u0026#39; \\ --region ap-southeast-1 # Get user pool ID USER_POOL_ID=$(aws cognito-idp list-user-pools \\ --max-results 10 \\ --query \u0026#34;UserPools[?Name==\u0026#39;insighthr-user-pool\u0026#39;].Id\u0026#34; \\ --output text \\ --region ap-southeast-1) echo \u0026#34;User Pool ID: $USER_POOL_ID\u0026#34; Create App Client # Create app client aws cognito-idp create-user-pool-client \\ --user-pool-id $USER_POOL_ID \\ --client-name insighthr-web-client \\ --no-generate-secret \\ --explicit-auth-flows ALLOW_USER_PASSWORD_AUTH ALLOW_REFRESH_TOKEN_AUTH ALLOW_USER_SRP_AUTH \\ --region ap-southeast-1 # Get client ID CLIENT_ID=$(aws cognito-idp list-user-pool-clients \\ --user-pool-id $USER_POOL_ID \\ --query \u0026#34;UserPoolClients[?ClientName==\u0026#39;insighthr-web-client\u0026#39;].ClientId\u0026#34; \\ --output text \\ --region ap-southeast-1) echo \u0026#34;Client ID: $CLIENT_ID\u0026#34; Step 2: Configure Cognito for Development For development, disable email verification requirement:\n# Update user pool to auto-verify emails aws cognito-idp update-user-pool \\ --user-pool-id $USER_POOL_ID \\ --auto-verified-attributes email \\ --region ap-southeast-1 Step 3: Create Login Lambda Function Create Function Code auth_login_handler.py:\nimport json import boto3 import os from botocore.exceptions import ClientError cognito_client = boto3.client(\u0026#39;cognito-idp\u0026#39;) dynamodb = boto3.resource(\u0026#39;dynamodb\u0026#39;) USER_POOL_ID = os.environ[\u0026#39;USER_POOL_ID\u0026#39;] CLIENT_ID = os.environ[\u0026#39;CLIENT_ID\u0026#39;] USERS_TABLE = os.environ[\u0026#39;DYNAMODB_USERS_TABLE\u0026#39;] def lambda_handler(event, context): try: body = json.loads(event[\u0026#39;body\u0026#39;]) email = body[\u0026#39;email\u0026#39;] password = body[\u0026#39;password\u0026#39;] # Authenticate with Cognito response = cognito_client.admin_initiate_auth( UserPoolId=USER_POOL_ID, ClientId=CLIENT_ID, AuthFlow=\u0026#39;ADMIN_NO_SRP_AUTH\u0026#39;, AuthParameters={ \u0026#39;USERNAME\u0026#39;: email, \u0026#39;PASSWORD\u0026#39;: password } ) # Get user attributes user_response = cognito_client.admin_get_user( UserPoolId=USER_POOL_ID, Username=email ) # Extract user info user_attributes = {attr[\u0026#39;Name\u0026#39;]: attr[\u0026#39;Value\u0026#39;] for attr in user_response[\u0026#39;UserAttributes\u0026#39;]} # Get user from DynamoDB table = dynamodb.Table(USERS_TABLE) db_response = table.get_item( Key={\u0026#39;userId\u0026#39;: user_attributes[\u0026#39;sub\u0026#39;]} ) user_data = db_response.get(\u0026#39;Item\u0026#39;, {}) return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;headers\u0026#39;: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, \u0026#39;body\u0026#39;: json.dumps({ \u0026#39;tokens\u0026#39;: { \u0026#39;accessToken\u0026#39;: response[\u0026#39;AuthenticationResult\u0026#39;][\u0026#39;AccessToken\u0026#39;], \u0026#39;idToken\u0026#39;: response[\u0026#39;AuthenticationResult\u0026#39;][\u0026#39;IdToken\u0026#39;], \u0026#39;refreshToken\u0026#39;: response[\u0026#39;AuthenticationResult\u0026#39;][\u0026#39;RefreshToken\u0026#39;] }, \u0026#39;user\u0026#39;: { \u0026#39;userId\u0026#39;: user_attributes[\u0026#39;sub\u0026#39;], \u0026#39;email\u0026#39;: user_attributes[\u0026#39;email\u0026#39;], \u0026#39;name\u0026#39;: user_attributes.get(\u0026#39;name\u0026#39;, \u0026#39;\u0026#39;), \u0026#39;role\u0026#39;: user_data.get(\u0026#39;role\u0026#39;, \u0026#39;Employee\u0026#39;), \u0026#39;department\u0026#39;: user_data.get(\u0026#39;department\u0026#39;, \u0026#39;\u0026#39;), \u0026#39;employeeId\u0026#39;: user_data.get(\u0026#39;employeeId\u0026#39;, \u0026#39;\u0026#39;) } }) } except ClientError as e: error_code = e.response[\u0026#39;Error\u0026#39;][\u0026#39;Code\u0026#39;] if error_code == \u0026#39;NotAuthorizedException\u0026#39;: return { \u0026#39;statusCode\u0026#39;: 401, \u0026#39;headers\u0026#39;: {\u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39;}, \u0026#39;body\u0026#39;: json.dumps({\u0026#39;error\u0026#39;: \u0026#39;Invalid credentials\u0026#39;}) } elif error_code == \u0026#39;UserNotFoundException\u0026#39;: return { \u0026#39;statusCode\u0026#39;: 404, \u0026#39;headers\u0026#39;: {\u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39;}, \u0026#39;body\u0026#39;: json.dumps({\u0026#39;error\u0026#39;: \u0026#39;User not found\u0026#39;}) } else: return { \u0026#39;statusCode\u0026#39;: 500, \u0026#39;headers\u0026#39;: {\u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39;}, \u0026#39;body\u0026#39;: json.dumps({\u0026#39;error\u0026#39;: str(e)}) } except Exception as e: return { \u0026#39;statusCode\u0026#39;: 500, \u0026#39;headers\u0026#39;: {\u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39;}, \u0026#39;body\u0026#39;: json.dumps({\u0026#39;error\u0026#39;: str(e)}) } Create requirements.txt boto3\u0026gt;=1.26.0 Package and Deploy # Create deployment package mkdir -p lambda/auth/package cd lambda/auth pip install -r requirements.txt -t package/ cd package zip -r ../auth-login-handler.zip . cd .. zip -g auth-login-handler.zip auth_login_handler.py # Deploy Lambda function aws lambda create-function \\ --function-name insighthr-auth-login-handler \\ --runtime python3.11 \\ --role arn:aws:iam::${ACCOUNT_ID}:role/insighthr-lambda-role \\ --handler auth_login_handler.lambda_handler \\ --zip-file fileb://auth-login-handler.zip \\ --timeout 30 \\ --memory-size 256 \\ --environment Variables=\u0026#34;{ USER_POOL_ID=${USER_POOL_ID}, CLIENT_ID=${CLIENT_ID}, DYNAMODB_USERS_TABLE=insighthr-users-dev }\u0026#34; \\ --region ap-southeast-1 Step 4: Create Registration Lambda Function auth_register_handler.py:\nimport json import boto3 import os import uuid from datetime import datetime from botocore.exceptions import ClientError cognito_client = boto3.client(\u0026#39;cognito-idp\u0026#39;) dynamodb = boto3.resource(\u0026#39;dynamodb\u0026#39;) USER_POOL_ID = os.environ[\u0026#39;USER_POOL_ID\u0026#39;] CLIENT_ID = os.environ[\u0026#39;CLIENT_ID\u0026#39;] USERS_TABLE = os.environ[\u0026#39;DYNAMODB_USERS_TABLE\u0026#39;] def lambda_handler(event, context): try: body = json.loads(event[\u0026#39;body\u0026#39;]) email = body[\u0026#39;email\u0026#39;] password = body[\u0026#39;password\u0026#39;] name = body[\u0026#39;name\u0026#39;] role = body.get(\u0026#39;role\u0026#39;, \u0026#39;Employee\u0026#39;) department = body.get(\u0026#39;department\u0026#39;, \u0026#39;\u0026#39;) employee_id = body.get(\u0026#39;employeeId\u0026#39;, \u0026#39;\u0026#39;) # Create user in Cognito cognito_response = cognito_client.sign_up( ClientId=CLIENT_ID, Username=email, Password=password, UserAttributes=[ {\u0026#39;Name\u0026#39;: \u0026#39;email\u0026#39;, \u0026#39;Value\u0026#39;: email}, {\u0026#39;Name\u0026#39;: \u0026#39;name\u0026#39;, \u0026#39;Value\u0026#39;: name} ] ) user_id = cognito_response[\u0026#39;UserSub\u0026#39;] # Auto-confirm user for development cognito_client.admin_confirm_sign_up( UserPoolId=USER_POOL_ID, Username=email ) # Store user in DynamoDB table = dynamodb.Table(USERS_TABLE) timestamp = datetime.utcnow().isoformat() + \u0026#39;Z\u0026#39; table.put_item( Item={ \u0026#39;userId\u0026#39;: user_id, \u0026#39;email\u0026#39;: email, \u0026#39;name\u0026#39;: name, \u0026#39;role\u0026#39;: role, \u0026#39;department\u0026#39;: department, \u0026#39;employeeId\u0026#39;: employee_id, \u0026#39;isActive\u0026#39;: True, \u0026#39;createdAt\u0026#39;: timestamp, \u0026#39;updatedAt\u0026#39;: timestamp } ) return { \u0026#39;statusCode\u0026#39;: 201, \u0026#39;headers\u0026#39;: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, \u0026#39;body\u0026#39;: json.dumps({ \u0026#39;message\u0026#39;: \u0026#39;User registered successfully\u0026#39;, \u0026#39;userId\u0026#39;: user_id }) } except ClientError as e: error_code = e.response[\u0026#39;Error\u0026#39;][\u0026#39;Code\u0026#39;] if error_code == \u0026#39;UsernameExistsException\u0026#39;: return { \u0026#39;statusCode\u0026#39;: 409, \u0026#39;headers\u0026#39;: {\u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39;}, \u0026#39;body\u0026#39;: json.dumps({\u0026#39;error\u0026#39;: \u0026#39;User already exists\u0026#39;}) } else: return { \u0026#39;statusCode\u0026#39;: 500, \u0026#39;headers\u0026#39;: {\u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39;}, \u0026#39;body\u0026#39;: json.dumps({\u0026#39;error\u0026#39;: str(e)}) } except Exception as e: return { \u0026#39;statusCode\u0026#39;: 500, \u0026#39;headers\u0026#39;: {\u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39;}, \u0026#39;body\u0026#39;: json.dumps({\u0026#39;error\u0026#39;: str(e)}) } Deploy the registration handler:\nzip -g auth-register-handler.zip auth_register_handler.py aws lambda create-function \\ --function-name insighthr-auth-register-handler \\ --runtime python3.11 \\ --role arn:aws:iam::${ACCOUNT_ID}:role/insighthr-lambda-role \\ --handler auth_register_handler.lambda_handler \\ --zip-file fileb://auth-register-handler.zip \\ --timeout 30 \\ --memory-size 256 \\ --environment Variables=\u0026#34;{ USER_POOL_ID=${USER_POOL_ID}, CLIENT_ID=${CLIENT_ID}, DYNAMODB_USERS_TABLE=insighthr-users-dev }\u0026#34; \\ --region ap-southeast-1 Step 5: Create API Gateway Endpoints Create REST API # Create API API_ID=$(aws apigateway create-rest-api \\ --name \u0026#34;InsightHR API\u0026#34; \\ --description \u0026#34;InsightHR Backend API\u0026#34; \\ --region ap-southeast-1 \\ --query \u0026#39;id\u0026#39; \\ --output text) echo \u0026#34;API ID: $API_ID\u0026#34; Create Cognito Authorizer # Create authorizer AUTHORIZER_ID=$(aws apigateway create-authorizer \\ --rest-api-id $API_ID \\ --name insighthr-cognito-authorizer \\ --type COGNITO_USER_POOLS \\ --provider-arns arn:aws:cognito-idp:ap-southeast-1:${ACCOUNT_ID}:userpool/${USER_POOL_ID} \\ --identity-source method.request.header.Authorization \\ --region ap-southeast-1 \\ --query \u0026#39;id\u0026#39; \\ --output text) echo \u0026#34;Authorizer ID: $AUTHORIZER_ID\u0026#34; Create /auth Resource # Get root resource ID ROOT_ID=$(aws apigateway get-resources \\ --rest-api-id $API_ID \\ --query \u0026#34;items[?path==\u0026#39;/\u0026#39;].id\u0026#34; \\ --output text \\ --region ap-southeast-1) # Create /auth resource AUTH_ID=$(aws apigateway create-resource \\ --rest-api-id $API_ID \\ --parent-id $ROOT_ID \\ --path-part auth \\ --query \u0026#39;id\u0026#39; \\ --output text \\ --region ap-southeast-1) # Create /auth/login resource LOGIN_ID=$(aws apigateway create-resource \\ --rest-api-id $API_ID \\ --parent-id $AUTH_ID \\ --path-part login \\ --query \u0026#39;id\u0026#39; \\ --output text \\ --region ap-southeast-1) # Create POST method for /auth/login aws apigateway put-method \\ --rest-api-id $API_ID \\ --resource-id $LOGIN_ID \\ --http-method POST \\ --authorization-type NONE \\ --region ap-southeast-1 # Integrate with Lambda aws apigateway put-integration \\ --rest-api-id $API_ID \\ --resource-id $LOGIN_ID \\ --http-method POST \\ --type AWS_PROXY \\ --integration-http-method POST \\ --uri arn:aws:apigateway:ap-southeast-1:lambda:path/2015-03-31/functions/arn:aws:lambda:ap-southeast-1:${ACCOUNT_ID}:function:insighthr-auth-login-handler/invocations \\ --region ap-southeast-1 # Grant API Gateway permission to invoke Lambda aws lambda add-permission \\ --function-name insighthr-auth-login-handler \\ --statement-id apigateway-invoke \\ --action lambda:InvokeFunction \\ --principal apigateway.amazonaws.com \\ --source-arn \u0026#34;arn:aws:execute-api:ap-southeast-1:${ACCOUNT_ID}:${API_ID}/*/*\u0026#34; \\ --region ap-southeast-1 Step 6: Enable CORS # Enable CORS for /auth/login aws apigateway put-method \\ --rest-api-id $API_ID \\ --resource-id $LOGIN_ID \\ --http-method OPTIONS \\ --authorization-type NONE \\ --region ap-southeast-1 aws apigateway put-integration \\ --rest-api-id $API_ID \\ --resource-id $LOGIN_ID \\ --http-method OPTIONS \\ --type MOCK \\ --request-templates \u0026#39;{\u0026#34;application/json\u0026#34;: \u0026#34;{\\\u0026#34;statusCode\\\u0026#34;: 200}\u0026#34;}\u0026#39; \\ --region ap-southeast-1 aws apigateway put-method-response \\ --rest-api-id $API_ID \\ --resource-id $LOGIN_ID \\ --http-method OPTIONS \\ --status-code 200 \\ --response-parameters \u0026#39;{ \u0026#34;method.response.header.Access-Control-Allow-Headers\u0026#34;: true, \u0026#34;method.response.header.Access-Control-Allow-Methods\u0026#34;: true, \u0026#34;method.response.header.Access-Control-Allow-Origin\u0026#34;: true }\u0026#39; \\ --region ap-southeast-1 aws apigateway put-integration-response \\ --rest-api-id $API_ID \\ --resource-id $LOGIN_ID \\ --http-method OPTIONS \\ --status-code 200 \\ --response-parameters \u0026#39;{ \u0026#34;method.response.header.Access-Control-Allow-Headers\u0026#34;: \u0026#34;\u0026#39;\u0026#34;\u0026#39;\u0026#34;\u0026#39;Content-Type,X-Amz-Date,Authorization,X-Api-Key,X-Amz-Security-Token\u0026#39;\u0026#34;\u0026#39;\u0026#34;\u0026#39;\u0026#34;, \u0026#34;method.response.header.Access-Control-Allow-Methods\u0026#34;: \u0026#34;\u0026#39;\u0026#34;\u0026#39;\u0026#34;\u0026#39;POST,OPTIONS\u0026#39;\u0026#34;\u0026#39;\u0026#34;\u0026#39;\u0026#34;, \u0026#34;method.response.header.Access-Control-Allow-Origin\u0026#34;: \u0026#34;\u0026#39;\u0026#34;\u0026#39;\u0026#34;\u0026#39;*\u0026#39;\u0026#34;\u0026#39;\u0026#34;\u0026#39;\u0026#34; }\u0026#39; \\ --region ap-southeast-1 Step 7: Deploy API # Create deployment aws apigateway create-deployment \\ --rest-api-id $API_ID \\ --stage-name dev \\ --description \u0026#34;Initial deployment\u0026#34; \\ --region ap-southeast-1 # Get API endpoint echo \u0026#34;API Endpoint: https://${API_ID}.execute-api.ap-southeast-1.amazonaws.com/dev\u0026#34; Step 8: Test Authentication Test Registration curl -X POST \\ https://${API_ID}.execute-api.ap-southeast-1.amazonaws.com/dev/auth/register \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;email\u0026#34;: \u0026#34;test@example.com\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;Test123!\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Test User\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;Employee\u0026#34;, \u0026#34;department\u0026#34;: \u0026#34;DEV\u0026#34; }\u0026#39; Test Login curl -X POST \\ https://${API_ID}.execute-api.ap-southeast-1.amazonaws.com/dev/auth/login \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;email\u0026#34;: \u0026#34;test@example.com\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;Test123!\u0026#34; }\u0026#39; Authentication Flow Summary 1. User submits credentials â†’ Frontend\r2. Frontend calls /auth/login â†’ API Gateway\r3. API Gateway routes to Lambda â†’ auth-login-handler\r4. Lambda authenticates with Cognito â†’ Cognito User Pool\r5. Cognito returns JWT tokens â†’ Lambda\r6. Lambda retrieves user data â†’ DynamoDB\r7. Lambda returns tokens + user data â†’ Frontend\r8. Frontend stores tokens â†’ localStorage\r9. Subsequent requests include JWT â†’ Authorization header Security Best Practices âœ… Password Policy: Strong password requirements âœ… JWT Tokens: Short-lived access tokens âœ… Refresh Tokens: Long-lived for token renewal âœ… HTTPS Only: All communication encrypted âœ… CORS: Properly configured for frontend domain âœ… No Secrets: Client doesn\u0026rsquo;t use client secret\nTroubleshooting Cognito Authentication Fails:\nVerify user pool ID and client ID Check password meets requirements Ensure user is confirmed Lambda Permission Denied:\nCheck IAM role has Cognito permissions Verify Lambda execution role attached CORS Errors:\nEnable CORS on API Gateway Check Access-Control-Allow-Origin header Verify OPTIONS method configured Next Steps With authentication configured, proceed to Backend Services to implement employee, performance, and chatbot APIs.\n"
},
{
	"uri": "https://asapphat.github.io/tanphat.worklog.github.io/1-worklog/1.6-week6/",
	"title": "Week 6 Worklog",
	"tags": [],
	"description": "",
	"content": "Week Duration: October 13 - 19, 2025\nWeek 6 Objectives: Master RDS database deployment and management Learn application deployment with Auto Scaling Understand monitoring with CloudWatch Practice DynamoDB for NoSQL workloads Learn networking and content delivery Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Monday - Create VPC, EC2 security group, RDS security group - Set up DB subnet group - Launch EC2 instance for RDS - Create RDS Database instance - Deploy application - Back up and restore databases 10/13/2025 10/13/2025 https://000005.awsstudygroup.com/vi/ Tuesday - Set up VPC for Auto Scaling - Create EC2 Instance and database with Amazon RDS - Create AMI from EC2 and Launch template - Create target group and Load balancer - Create Auto Scaling Group - Test manual, schedule, and dynamic scaling 10/14/2025 10/14/2025 https://000006.awsstudygroup.com/vi/ Wednesday - CloudWatch fundamentals and metrics - Amazon DynamoDB configuration and management 10/15/2025 10/15/2025 https://000008.awsstudygroup.com/vi/ https://000060.awsstudygroup.com/vi/ Thursday - Workshop Data Science on AWS: + AWS AI/ML Stack + Amazon Comprehend, Translate, Textract + Amazon Polly, Transcribe, Rekognition 10/16/2025 10/16/2025 Workshop Link Friday - AWS Networking and Content Delivery fundamentals - CloudFront, Route 53, and CDN best practices 10/17/2025 10/17/2025 https://000092.awsstudygroup.com/vi/ Saturday - Review Secure Architectures (IAM, MFA, SCP, Encryption, Security Groups, NACLs) - Review GuardDuty, Shield, WAF, Secrets Manager - Cost optimization review 10/18/2025 10/18/2025 https://cloudjourney.awsstudygroup.com/ Week 6 Achievements: Mastered Amazon RDS Database Deployment:\nCreated VPC, security groups, and DB subnet groups Successfully launched RDS database instances Configured database backups and automated restore procedures Deployed applications with secure database connectivity Implemented Multi-AZ deployments for high availability Understood RDS parameter groups and option groups Implemented Auto Scaling Solutions:\nCreated custom AMIs from EC2 instances Configured launch templates with specific configurations Set up Application Load Balancers with target groups Implemented manual, scheduled, and dynamic scaling policies Tested scaling solutions under load Monitored scaling activities and performance metrics Configured scaling group lifecycle hooks Mastered CloudWatch Monitoring and Observability:\nConfigured CloudWatch metrics for all AWS resources Created custom dashboards for real-time monitoring Set up alarms and notifications for critical events Implemented log aggregation and analysis Understood metric math and custom metrics Created detailed monitoring strategies Explored DynamoDB NoSQL Database:\nConfigured DynamoDB tables and indexes Understood partition keys and sort keys Practiced read/write capacity management Explored DynamoDB Streams for event processing Configured TTL (Time-to-Live) for data expiration Learned DynamoDB best practices Learned AWS AI/ML Services:\nExplored AWS AI/ML Stack architecture Used Amazon Comprehend for text analysis and entity recognition Practiced Amazon Translate for language translation Experimented with Amazon Textract for document processing Configured Amazon Polly for text-to-speech conversion Used Amazon Transcribe for speech-to-text transcription Explored Amazon Rekognition for image and video analysis Mastered AWS Networking and Content Delivery:\nConfigured CloudFront CDN for global content distribution Set up CloudFront distributions with S3 origins Used Route 53 for DNS management and routing policies Implemented geo-routing and failover strategies Understood caching behavior and invalidation Applied networking best practices Reviewed and Strengthened Security Architecture:\nReinforced IAM, MFA, and Service Control Policies (SCPs) Reviewed encryption strategies (KMS, TLS/ACM) Studied AWS GuardDuty for threat detection Understood AWS Shield for DDoS protection Reviewed AWS WAF for application protection Practiced with AWS Secrets Manager Understood VPN connections and hybrid networking Practical Troubleshooting:\nResolved connectivity problems between instances Fixed routing conflicts in VPCs Corrected security group misconfigurations Validated end-to-end application connectivity "
},
{
	"uri": "https://asapphat.github.io/tanphat.worklog.github.io/6-self-evaluation/",
	"title": "Self-Assessment",
	"tags": [],
	"description": "",
	"content": "\râš ï¸ Note: The information below is for reference purposes only. Please do not copy it verbatim into your report, including this warning.\nDuring my internship at AWS from September 8, 2025 to December 19, 2025, I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in developing the InsightHR-1 application, an HR support tool, through which I improved my skills in programming, AI application, AWS services, and back-end coding.\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality â˜ âœ… â˜ 2 Ability to learn Ability to absorb new knowledge and learn quickly â˜ â˜ âœ… 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions âœ… â˜ â˜ 4 Sense of responsibility Completing tasks on time and ensuring quality âœ… â˜ â˜ 5 Discipline Adhering to schedules, rules, and work processes â˜ âœ… â˜ 6 Progressive mindset Willingness to receive feedback and improve oneself âœ… â˜ â˜ 7 Communication Presenting ideas and reporting work clearly âœ… â˜ â˜ 8 Teamwork Working effectively with colleagues and participating in teams âœ… â˜ â˜ 9 Professional conduct Respecting colleagues, partners, and the work environment âœ… â˜ â˜ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity â˜ âœ… â˜ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team âœ… â˜ â˜ 12 Overall General evaluation of the entire internship period âœ… â˜ â˜ Needs Improvement Learning methodology: Adopt a more effective learning approach to improve knowledge retention and test scores Knowledge deepening: Continue enhancing technical knowledge to strengthen understanding of key concepts Time management: Develop a clearer schedule to avoid missing important tasks and deadlines Task focus: Thoroughly resolve one problem completely before moving to the next task to ensure quality and depth "
},
{
	"uri": "https://asapphat.github.io/tanphat.worklog.github.io/4-eventparticipated/4.6-event6/",
	"title": "Event 6",
	"tags": [],
	"description": "",
	"content": "AWS Security Specialty Workshop Location: AWS Event Hall, L26 Bitexco Tower, HCMC\nDate: Saturday, November 29, 2025\nLearning Report: \u0026ldquo;AWS Security Specialty Workshop\u0026rdquo; Event Objectives Deeply understand the role of Security Pillar in AWS Well-Architected Framework Master the 5 Core Security Pillars: Identity and Access Management (IAM), Detection, Infrastructure Security, Data Protection, and Incident Response Update on leading cybersecurity threats in Cloud environments in the Vietnamese market Practice skills like IAM privilege reviews and building incident response playbooks (IR Playbook) Speaker List AWS Security Expert Team (Deep expertise in security architecture and compliance) Key Content Highlights Foundation \u0026amp; Identity (Pillar 1) Core Principles: Strictly apply Least Privilege, Zero Trust (default deny), and Defense in Depth principles Modern IAM: Transition from IAM Users (with long-term credentials) to IAM Roles and AWS Identity Center (SSO) for centralized access management Access Control: Use Service Control Policies (SCP) and Permission Boundaries to limit privilege scope in multi-account environments Small Practical Demo: Demonstrate how to validate IAM Policies and simulate access to detect security configuration errors Detection \u0026amp; Infrastructure (Pillar 2 \u0026amp; 3) Continuous Monitoring: Enable services like CloudTrail (at organization level), GuardDuty, and Security Hub for continuous security monitoring and assessment Logging Strategy: Require logging at every system layer: VPC Flow Logs (network), ALB logs (application), and S3 logs (storage) Network Security: Implement network segmentation with VPC, combining Security Groups and NACLs. Protect perimeter with WAF, Shield, and Network Firewall Data Protection \u0026amp; Incident Response (Pillar 4 \u0026amp; 5) Encryption: Implement data encryption in-transit and at-rest on services like S3, EBS, RDS, using KMS (Key Management Service) Secrets Management: Eliminate hard-coded credentials by using Secrets Manager and Parameter Store, combining automatic rotation mechanisms Incident Response Automation: Build response playbooks for common incidents (like exposed access keys, malware) and automate resource isolation using Lambda/Step Functions What I Learned Zero Trust Mindset Identity as the Perimeter: In Cloud environments, Identity has become the most important protective barrier, replacing traditional IP addresses Always follow the principle: Never trust by default, always authenticate every request, and grant only minimum necessary permissions Automation is Core Manual security cannot keep pace with Cloud deployment speed. Must apply methods like Detection-as-Code and Auto-remediation to minimize risk from human error and delays Application to Work Audit IAM: Review all IAM Users, delete old access keys, and migrate applications to IAM Roles for enhanced security Enable GuardDuty: Activate GuardDuty service across all regions and accounts to detect abnormal access and activities early Deploy Secrets Manager: Replace configuration files containing database passwords with API calls to Secrets Manager Build IR Playbook: Write detailed incident handling procedures for \u0026ldquo;IAM Key Exposure\u0026rdquo; scenario and conduct drills with technical team Event Experience The workshop provided deep technical details, comprehensively covering essential security aspects modern Cloud engineers must master.\nComprehensive Framework Organizing content by 5 Security Pillars helped me systematize previously scattered security knowledge into a standard, applicable framework The discussion on Leading threats in Vietnam is highly practical, helping the team identify specific risks suited to local context Applied Practice Demos on Access Analyzer and Validate IAM Policy tools are very useful, directly addressing daily permission debugging challenges The Incident Response section clarified that \u0026ldquo;detection\u0026rdquo; is only part of the solution; fast automatic \u0026ldquo;response\u0026rdquo; is what determines system safety Some photos from the event Overall, the event clearly confirmed that security is not a blocker but an enabler allowing enterprises to operate faster and safer.\n"
},
{
	"uri": "https://asapphat.github.io/tanphat.worklog.github.io/5-workshop/5-workshop/5.7-backend-services/",
	"title": "Backend Services",
	"tags": [],
	"description": "",
	"content": "Backend Services Implementation This section covers implementing the core backend Lambda functions for employee management, performance tracking, attendance, and AI chatbot.\nOverview We\u0026rsquo;ll implement four main services:\nEmployee Service - CRUD operations for employee records Performance Scores Service - Quarterly performance tracking Attendance Service - Check-in/check-out management Chatbot Service - AI-powered natural language queries Employee Service The employee service handles all employee-related operations with department and position filtering.\nKey Features:\nCRUD operations (Create, Read, Update, Delete) Department filtering (DEV, QA, DAT, SEC, AI) Position filtering (Junior, Mid, Senior, Lead, Manager) Status filtering (active, inactive) Search by name or employee ID Bulk import from CSV API Endpoints:\nGET /employees - List all employees\rGET /employees/{id} - Get employee by ID\rPOST /employees - Create new employee\rPUT /employees/{id} - Update employee\rDELETE /employees/{id} - Delete employee\rPOST /employees/bulk - Bulk import from CSV Performance Scores Service Manages quarterly performance scores with automatic calculation.\nKey Features:\nAutomatic score calculation (average of KPI, completed_task, feedback_360) Department and period filtering Role-based data access (Admin: all, Manager: department, Employee: own) Denormalized employee data for performance Bulk import from CSV API Endpoints:\nGET /performance-scores - List all scores\rGET /performance-scores/{id}/{period} - Get specific score\rPOST /performance-scores - Create score\rPUT /performance-scores/{id}/{period} - Update score\rDELETE /performance-scores/{id}/{period} - Delete score\rPOST /performance-scores/bulk - Bulk import Attendance Service Tracks employee attendance with check-in/check-out functionality.\nKey Features:\nReal-time check-in/check-out Date range filtering Department filtering Status tracking (present, absent, late, half-day) Bulk operations API Endpoints:\nGET /attendance - List attendance records\rGET /attendance/{id}/{date} - Get specific record\rPOST /attendance/check-in - Check in\rPOST /attendance/check-out - Check out\rPUT /attendance/{id}/{date} - Update record\rDELETE /attendance/{id}/{date} - Delete record Chatbot Service AI-powered chatbot using AWS Bedrock (Claude 3 Haiku).\nKey Features:\nAWS Bedrock integration (Claude 3 Haiku) Natural language query processing Context building from DynamoDB Role-based data filtering Cost-effective ($0.0004 per query) Supported Queries:\nEmployee information (\u0026ldquo;Who is DEV-001?\u0026rdquo;) Performance queries (\u0026ldquo;What\u0026rsquo;s the average score for Q1 2025?\u0026rdquo;) Department statistics (\u0026ldquo;Compare DEV and QA performance\u0026rdquo;) Trend analysis (\u0026ldquo;Performance trends over quarters\u0026rdquo;) API Endpoint:\nPOST /chatbot/query Implementation Pattern All services follow this pattern:\nRequest Validation - Validate input parameters Authorization - Check user permissions based on role DynamoDB Operations - Query or update data Response Formatting - Return standardized JSON response Error Handling - Catch and return appropriate errors Deployment Deploy all backend services using the Lambda deployment scripts provided in the workshop materials. Each service is packaged with its dependencies and deployed to AWS Lambda with appropriate environment variables.\nTesting Test each service using the provided test scripts or curl commands. Verify:\nCRUD operations work correctly Filtering and search functionality Role-based access control Error handling Performance and response times Next Steps With backend services implemented, proceed to Frontend Development to build the React application.\n"
},
{
	"uri": "https://asapphat.github.io/tanphat.worklog.github.io/1-worklog/1.7-week7/",
	"title": "Week 7 Worklog",
	"tags": [],
	"description": "",
	"content": "Week Duration: October 20 - 26, 2025\nWeek 7 Objectives: Master serverless architecture with Lambda, S3, DynamoDB Learn front-end code for API Gateway Practice serverless deployment Review and improve proposal document Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Monday - Optimizing EC2 Costs with Lambda week 10/20/2025 10/20/2025 https://000022.awsstudygroup.com/ Tuesday - Serverless Lambda, S3, DynamoDB: Learn serverless architecture fundamentals - Serverless Front-End: Learn to write front-end code for calling API Gateway 10/21/2025 10/21/2025 https://000078.awsstudygroup.com/vi/ https://000079.awsstudygroup.com/vi/ Wednesday - Review Midterm Concepts: Study resilient architecture patterns and design principles 10/22/2025 10/22/2025 AWS Documentation Thursday - Continue Reviewing Resilient Architecture - Draw AWS Architecture Diagrams for project design 10/23/2025 10/23/2025 Workshop Materials Friday - Amazon Bedrock Tutorials: + How to use Bedrock + Lex + Lambda + Bedrock integration - AWS CLI Workshop: S3, SNS, IAM, VPC, EC2 operations 10/24/2025 10/24/2025 https://000011.awsstudygroup.com/vi/ Saturday - Serverless framework and deployment strategies - Review and finalize proposal document - Week 7 summary 10/25/2025 10/25/2025 https://cloudjourney.awsstudygroup.com/ Week 7 Achievements: Mastered Serverless Architecture Design:\nUnderstood event-driven architecture patterns and best practices Learned AWS Lambda function creation, configuration, and deployment Mastered Lambda execution models, triggers, and permissions Designed serverless application workflows using multiple AWS services Understood scaling behavior and concurrency in serverless applications Practiced composing Lambda functions into complete systems Learned about Lambda layers and code reusability strategies Deployed Lambda, S3, and DynamoDB Solutions:\nCreated Lambda functions for compute operations Configured S3 triggers to invoke Lambda functions Set up DynamoDB tables for serverless data storage Integrated Lambda with DynamoDB for read/write operations Implemented event source mappings for asynchronous processing Practiced error handling and retry logic in serverless applications Tested end-to-end serverless workflows Developed Front-End Code for API Gateway:\nCreated HTML/JavaScript client applications Implemented API calls to API Gateway endpoints Handled asynchronous request/response patterns Implemented error handling and validation on client side Practiced CORS configuration for web applications Tested front-end integration with backend APIs Deployed static front-end to S3 with CloudFront CDN Reviewed and Mastered Midterm Concepts:\nStudied resilient architecture design principles and patterns Reviewed multi-AZ deployment strategies Mastered auto-scaling and load balancing concepts Understood disaster recovery and business continuity planning Reviewed backup and restoration procedures Studied fault-tolerance mechanisms across AWS services Practiced designing highly available systems Designed AWS Architecture Diagrams:\nCreated comprehensive architecture diagrams using draw.io Visualized multi-service serverless solutions Documented component relationships and data flows Applied AWS architecture best practices in diagram design Created deployment and network architecture diagrams Practiced architecture review and documentation Used official AWS icon set for professional diagrams Explored Amazon Bedrock for Generative AI:\nLearned Amazon Bedrock foundational concepts and capabilities Understood base models available in Bedrock (Claude, Llama, etc.) Practiced prompt engineering for effective AI interactions Integrated Bedrock with Lambda functions for serverless AI Explored Bedrock knowledge bases for retrieval-augmented generation (RAG) Experimented with different prompt templates and model parameters Understood API limits and pricing considerations Integrated Lex, Lambda, and Bedrock for Chatbots:\nConfigured Amazon Lex conversational AI bot Set up intents, slots, and fulfillment logic Integrated Lambda functions for backend processing Connected Bedrock for natural language understanding Implemented multi-turn conversation flows Tested bot interactions through various channels Practiced deployment and monitoring of chatbot solutions Mastered AWS CLI for Infrastructure Operations:\nExecuted S3 operations programmatically (create, list, upload, download) Managed SNS topics and subscriptions via CLI Configured IAM roles, policies, and permissions through CLI Managed VPC resources including subnets, security groups, and route tables Created and managed EC2 instances programmatically Practiced batch operations and scripting for automation Understood CLI output formatting and filtering options Optimized EC2 Costs with Serverless Alternatives:\nAnalyzed EC2 usage patterns and identified optimization opportunities Evaluated Lambda as cost-effective alternative for specific workloads Compared pricing models between traditional and serverless approaches Implemented cost-saving measures by migrating workloads to Lambda Practiced reserved capacity and spot instance considerations Learned about AWS Compute Optimizer recommendations "
},
{
	"uri": "https://asapphat.github.io/tanphat.worklog.github.io/7-feedback/",
	"title": "Sharing and Feedback",
	"tags": [],
	"description": "",
	"content": " Here, I freely share my personal opinions about my experience participating in the First Cloud Journey program, to help the FCJ team improve any shortcomings based on the following aspects:\nOverall Evaluation 1. Working Environment\nThe working environment is very friendly and open. FCJ members are always willing to help whenever I encounter difficulties, even outside working hours. The workspace is tidy and comfortable, helping me focus better.\n2. Support from Mentor / Team Admin\nThe mentor and team admin are always ready to answer questions and provide suggestions for my inquiries. The information and suggestions provided are very detailed and aligned with my concerns. People not only guide me but also encourage me to explore other approaches.\n3. Relevance of Work to Academic Major\nMost of the work assigned to me is quite new compared to what I\u0026rsquo;ve learned so far, so I have the opportunity to learn additional knowledge that I had not explored before. The knowledge provided helps complete the project and improve the knowledge I learned at school.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I learned many new skills such as using project management tools, teamwork skills, and professional communication in a corporate environment. The mentor also shared valuable real-world experiences that helped me better plan my career path.\n5. Company Culture \u0026amp; Team Spirit\nThe admin and mentors are always cheerful and willing to chat with me about various topics from general matters to technical subjects. I\u0026rsquo;m also exposed to work stories and knowledge through workshops. When I made mistakes at first, I was reminded of shortcomings or misconceptions in my knowledge, helping me gain experience.\n6. Internship Policies / Benefits\nThe company prioritizes the comfort and productivity of its interns by providing a generous and well-conditioned working area that meets high operational standards. This commitment to an excellent physical environment is matched by a dedication to flexibility in professional development. Specifically, the internship program features a highly adaptable working schedule, giving interns the autonomy to balance their professional experience with other commitments. This combination of superior workspace and flexible hours ensures a supportive and positive experience.\nAdditional Questions What I found most satisfying:\nI really enjoyed the working environment at FCJ and how people treat each other, creating a friendly atmosphere. The work is professionally challenging, and there are great opportunities to work on projects and enhance my knowledge.\nWhat should be improved:\nIssues regarding time management and documentation need to be more unified and clearly defined.\nWould I recommend to friends:\nDefinitely yes. This is an excellent environment and opportunity for engineering students because it\u0026rsquo;s a community to learn general knowledge and cloud computing in particularâ€”it\u0026rsquo;s excellent.\nSuggestions \u0026amp; Expectations Future expectations:\nIf there\u0026rsquo;s an opportunity, I would like to continue accompanying this program in the future.\n"
},
{
	"uri": "https://asapphat.github.io/tanphat.worklog.github.io/4-eventparticipated/4.7-event7/",
	"title": "Event 7",
	"tags": [],
	"description": "",
	"content": "CloudThinker: Agentic AI \u0026amp; Orchestration on AWS Location: AWS Event Hall, L26 Bitexco Tower, HCMC\nDate: Friday, December 5, 2025\nLearning Report: \u0026ldquo;CloudThinker: Agentic AI \u0026amp; Orchestration on AWS\u0026rdquo; Event Objectives Deep dive into AWS Bedrock Agent Core and its core capabilities Explore real-world use cases for building Agentic Workflows Master advanced concepts like Agentic Orchestration and Context Optimization at deep technical level (L300) Gain hands-on experience through CloudThinker Hack workshop Speaker List Nguyá»…n Gia HÆ°ng Head of Solutions Architect, AWS KiÃªn Nguyá»…n Solutions Architect, AWS Viá»‡t Pháº¡m Founder \u0026amp; CEO, CloudThinker Tháº¯ng TÃ´n Co-founder \u0026amp; COO, CloudThinker Henry BÃ¹i Head of Engineering, CloudThinker Kha VÄƒn Workshop Facilitator Key Content Highlights 1. AWS Foundation and Agentic AI Opening: Nguyá»…n Gia HÆ°ng opens the event, emphasizing the increasingly critical role of Agentic AI in today\u0026rsquo;s cloud technology landscape Bedrock Agent Core: KiÃªn Nguyá»…n provides technical overview of AWS Bedrock Agent Core, explaining how this service simplifies creating Agents capable of planning and executing tasks through API calls 2. Real-World Applications Building Agentic Workflows: Viá»‡t Pháº¡m demonstrates a specific use case, illustrating the design and deployment of an agentic workflow from start to finish on AWS platform CloudThinker Introduction: Tháº¯ng TÃ´n introduces CloudThinker ecosystem and the company\u0026rsquo;s vision for AI-integrated cloud solutions 3. Deep Dive (Level 300) Agent Orchestration \u0026amp; Context Optimization: This is the deep technical section of the morning. Henry BÃ¹i discusses advanced strategies for orchestrating multi-agent systems and context optimization in Amazon Bedrock, ensuring high accuracy in complex interactions 4. Hands-on Practice CloudThinker Hack: Facilitated by Kha VÄƒn, this 60-minute hands-on session allows participants to directly apply learned knowledge to build an agent prototype using CloudThinker framework and AWS services Key Takeaways Evolution of AI From Chat to Action: The technology field is witnessing a strong shift from chatbots that only respond passively to proactive Agents capable of performing complex orchestration and calling APIs to complete tasks Context is Key: As workflows become increasingly complex, standard context windows are insufficient. Applying \u0026ldquo;Context Optimization\u0026rdquo; strategies is essential for reducing operational costs while increasing Agent accuracy Architecture Orchestration Model: Managing a system with multiple Agents requires a strong Orchestration Layer to allocate and decide which Agent handles which part of user requests Application to Work Prototype Agent: Use AWS Bedrock Agent to build simple internal tools capable of connecting to existing company APIs (for example: checking server status or leave statistics) Research Context Models: Deep dive into context optimization techniques shared by Henry BÃ¹i to apply to current RAG systems for improved performance Join Hackathons: Encourage technical team to participate in similar real hackathons for the fastest and most intuitive updates on AWS new features Event Experience The event was highly technical and deeply focused on a single topic.\nTechnical Depth: The L300 session on Orchestration was particularly valuable, helping me understand how to scale AI applications beyond simple demos Interactivity: The \u0026ldquo;CloudThinker Hack\u0026rdquo; section helped me immediately reinforce theoretical knowledge with practice, making this one of the most effective learning sessions Some photos from the event Overall, this event provided a clear architectural framework for developing Agentic AI Systems capable of complex orchestration and execution on AWS cloud platform.\n"
},
{
	"uri": "https://asapphat.github.io/tanphat.worklog.github.io/5-workshop/5-workshop/5.8-frontend/",
	"title": "Frontend Development",
	"tags": [],
	"description": "",
	"content": "Frontend Development This section covers building the React frontend application for InsightHR.\nOverview The frontend is a Single Page Application (SPA) built with:\nReact 18 with TypeScript Vite 7.2 for fast builds Tailwind CSS 3.4 for styling (Frutiger Aero theme) Zustand 5.0 for state management React Hook Form + Zod for form validation Recharts 3.4 for data visualization React Router v7 for routing Project Structure insighthr-web/\râ”œâ”€â”€ src/\râ”‚ â”œâ”€â”€ components/\râ”‚ â”‚ â”œâ”€â”€ auth/ # Login, Register, ProtectedRoute\râ”‚ â”‚ â”œâ”€â”€ admin/ # User/Employee/Score Management\râ”‚ â”‚ â”œâ”€â”€ dashboard/ # Charts, Tables, Filters\râ”‚ â”‚ â”œâ”€â”€ attendance/ # Check-in/out, History\râ”‚ â”‚ â”œâ”€â”€ chatbot/ # AI Chat Interface\râ”‚ â”‚ â”œâ”€â”€ profile/ # User Profile\râ”‚ â”‚ â”œâ”€â”€ common/ # Reusable UI components\râ”‚ â”‚ â””â”€â”€ layout/ # Header, Sidebar, Footer\râ”‚ â”œâ”€â”€ pages/ # Page components\râ”‚ â”œâ”€â”€ services/ # API service layer\râ”‚ â”œâ”€â”€ store/ # Zustand state stores\râ”‚ â”œâ”€â”€ types/ # TypeScript type definitions\râ”‚ â”œâ”€â”€ utils/ # Utility functions\râ”‚ â””â”€â”€ styles/ # Global styles and theme\râ”œâ”€â”€ public/ # Static assets\râ””â”€â”€ package.json # Dependencies Key Components Authentication Components LoginForm: Email/password and Google OAuth login RegisterForm: New user registration ProtectedRoute: Route guard for authenticated users ChangePasswordModal: Password change dialog Admin Panel Components UserManagement: User CRUD with role assignment EmployeeManagement: Employee CRUD with filters PerformanceScoreManagement: Score management with bulk import PasswordRequestsPanel: Password reset request handling Dashboard Components PerformanceDashboard: Main analytics dashboard BarChart: Department performance comparison LineChart: Trend analysis over time PieChart: Score distribution DataTable: Sortable, filterable data grid FilterPanel: Date range and department filters ExportButton: CSV/Excel export Chatbot Components MessageList: Conversation history display MessageInput: User input with send button TypingIndicator: Loading animation ChatbotInstructions: Usage guide Attendance Components CheckInCheckOut: Quick check-in/out buttons AttendanceManagement: Full attendance interface AttendanceCalendarView: Calendar visualization AttendanceRecordsList: History table State Management Using Zustand for global state:\n// Auth Store interface AuthState { user: User | null; tokens: Tokens | null; isAuthenticated: boolean; login: (email: string, password: string) =\u0026gt; Promise\u0026lt;void\u0026gt;; logout: () =\u0026gt; void; } // Employee Store interface EmployeeState { employees: Employee[]; loading: boolean; error: string | null; fetchEmployees: (filters?: Filters) =\u0026gt; Promise\u0026lt;void\u0026gt;; createEmployee: (data: EmployeeInput) =\u0026gt; Promise\u0026lt;void\u0026gt;; } API Integration All API calls go through a centralized service layer with axios interceptors for authentication:\nconst api = axios.create({ baseURL: import.meta.env.VITE_API_BASE_URL, headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39; } }); // Add auth token to all requests api.interceptors.request.use((config) =\u0026gt; { const token = localStorage.getItem(\u0026#39;idToken\u0026#39;); if (token) { config.headers.Authorization = `Bearer ${token}`; } return config; }); Routing React Router v7 handles navigation:\nconst router = createBrowserRouter([ { path: \u0026#39;/login\u0026#39;, element: \u0026lt;LoginPage /\u0026gt; }, { path: \u0026#39;/register\u0026#39;, element: \u0026lt;RegisterForm /\u0026gt; }, { path: \u0026#39;/\u0026#39;, element: \u0026lt;ProtectedRoute\u0026gt;\u0026lt;Layout /\u0026gt;\u0026lt;/ProtectedRoute\u0026gt;, children: [ { path: \u0026#39;/\u0026#39;, element: \u0026lt;DashboardPage /\u0026gt; }, { path: \u0026#39;/admin\u0026#39;, element: \u0026lt;AdminPage /\u0026gt; }, { path: \u0026#39;/employees\u0026#39;, element: \u0026lt;EmployeesPage /\u0026gt; }, { path: \u0026#39;/performance-scores\u0026#39;, element: \u0026lt;PerformanceScoresPage /\u0026gt; }, { path: \u0026#39;/attendance\u0026#39;, element: \u0026lt;AttendancePage /\u0026gt; }, { path: \u0026#39;/chatbot\u0026#39;, element: \u0026lt;ChatbotPage /\u0026gt; }, { path: \u0026#39;/profile\u0026#39;, element: \u0026lt;ProfilePage /\u0026gt; } ] } ]); Environment Configuration Create .env file:\nVITE_API_BASE_URL=https://your-api-id.execute-api.ap-southeast-1.amazonaws.com/dev VITE_GOOGLE_CLIENT_ID=your-google-client-id VITE_AWS_REGION=ap-southeast-1 VITE_COGNITO_USER_POOL_ID=your-user-pool-id VITE_COGNITO_CLIENT_ID=your-client-id Development # Install dependencies npm install # Start development server npm run dev # Build for production npm run build # Preview production build npm run preview Styling Tailwind CSS with custom Frutiger Aero theme provides a modern, clean interface with:\nGradient backgrounds Glassmorphism effects Smooth animations Responsive design Accessible components Testing The frontend includes:\nComponent unit tests Integration tests for user flows E2E tests with Playwright Accessibility testing Next Steps With the frontend built, proceed to Deployment to deploy the complete application to AWS.\n"
},
{
	"uri": "https://asapphat.github.io/tanphat.worklog.github.io/1-worklog/1.8-week8/",
	"title": "Week 8 Worklog",
	"tags": [],
	"description": "",
	"content": "Week Duration: October 27 - November 1, 2025\nWeek 8 Objectives: Review and consolidate midterm concepts for AWS midterm test Identify knowledge gaps Prepare mentally for advanced topics Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Monday-Friday Review Midterm Test: Review and assess midterm AWS concepts 10/27/2025 10/31/2025 Course Materials Week 8 Achievements: Comprehensive Review of AWS Foundations (Weeks 1-3):\nReviewed AWS account setup, identity management, and organizational practices Consolidated knowledge of EC2, networking, and VPC fundamentals Reviewed storage services: S3, File Storage Gateway, and VM Import/Export Practiced configuring security groups and network access controls Reinforced understanding of IAM policies and permissions Reviewed best practices for multi-tier application architecture Assessed foundational knowledge retention Deep Review of Security and Data Protection (Weeks 4-5):\nReviewed IAM Identity Center, permission boundaries, and service control policies Consolidated encryption knowledge: KMS, TLS, ACM certificate management Reviewed AWS GuardDuty, Shield, WAF, and threat detection mechanisms Assessed data protection strategies using Macie and compliance monitoring Reviewed backup and disaster recovery procedures Assessed understanding of VPC Peering, Transit Gateway, and network resilience Practiced security architecture design patterns Advanced Services Consolidation (Weeks 6-7):\nReviewed RDS database deployment and Multi-AZ configuration Consolidated Auto Scaling concepts and scaling policies Reviewed CloudWatch monitoring, metrics, and alerting Assessed DynamoDB knowledge including indexes and capacity management Reviewed AI/ML services: Comprehend, Translate, Textract, Polly, Transcribe, Rekognition Reviewed CloudFront and Route 53 for content delivery and DNS management Reviewed serverless architecture with Lambda, S3, and DynamoDB Reviewed API Gateway and front-end development patterns Consolidated Amazon Bedrock and Lex for chatbot applications Assessment of High-Availability and Scaling Concepts:\nReviewed load balancing strategies and Application/Network Load Balancers Assessed understanding of multi-AZ and multi-region deployments Reviewed auto-scaling triggers, policies, and scaling lifecycle Reviewed database resilience and failover mechanisms Assessed knowledge of stateless vs. stateful application design Practiced designing fault-tolerant systems Reviewed cost optimization strategies for scaling solutions Identified Knowledge Gaps and Improvement Areas:\nDocumented concepts needing deeper understanding Identified areas requiring additional hands-on practice Planned study strategies for challenging topics Flagged advanced scenarios for deeper exploration Recognized relationships between different AWS services Noted questions for clarification during advanced weeks Mental and Knowledge Preparation for Advanced Topics:\nAssessed readiness for advanced monitoring and observability Reviewed prerequisites for Lambda-based project development Consolidated API design and integration patterns Assessed readiness for containerization and deployment Reviewed advanced IAM and security architecture concepts Prepared for project implementation phase Mentally prepared for increased complexity of remaining topics "
},
{
	"uri": "https://asapphat.github.io/tanphat.worklog.github.io/5-workshop/5-workshop/5.9-deployment/",
	"title": "Deployment",
	"tags": [],
	"description": "",
	"content": "Deploying InsightHR to Production This section covers deploying the complete InsightHR application to AWS, including frontend deployment to S3/CloudFront and backend Lambda functions.\nDeployment Overview The deployment process consists of:\nFrontend Deployment: Build and deploy React app to S3 CloudFront Configuration: Set up CDN for global distribution Lambda Deployment: Deploy all backend functions API Gateway Configuration: Set up REST API endpoints Environment Configuration: Configure environment variables DNS Setup (Optional): Configure custom domain Prerequisites Before deploying, ensure you have:\nâœ… All DynamoDB tables created âœ… Cognito User Pool configured âœ… IAM roles and policies set up âœ… AWS CLI configured âœ… Node.js and npm installed âœ… Python 3.11+ installed Step 1: Frontend Deployment Build the React Application Navigate to the frontend directory and build the production bundle:\ncd insighthr-web # Install dependencies npm install # Build production bundle npm run build This creates an optimized production build in the dist/ directory.\nBuild Output:\ndist/\râ”œâ”€â”€ index.html\râ”œâ”€â”€ assets/\râ”‚ â”œâ”€â”€ index-[hash].js\râ”‚ â”œâ”€â”€ index-[hash].css\râ”‚ â””â”€â”€ [other assets]\râ””â”€â”€ [other files] Create S3 Bucket Create an S3 bucket for hosting the static website:\n# Create bucket aws s3 mb s3://insighthr-web-app-sg --region ap-southeast-1 # Enable static website hosting aws s3 website s3://insighthr-web-app-sg \\ --index-document index.html \\ --error-document index.html Configure Bucket Policy Create a bucket policy to allow CloudFront access:\nbucket-policy.json:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;PublicReadGetObject\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:GetObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::insighthr-web-app-sg/*\u0026#34; } ] } Apply the policy:\naws s3api put-bucket-policy \\ --bucket insighthr-web-app-sg \\ --policy file://bucket-policy.json Upload Files to S3 # Sync dist folder to S3 aws s3 sync dist/ s3://insighthr-web-app-sg \\ --region ap-southeast-1 \\ --delete # Verify upload aws s3 ls s3://insighthr-web-app-sg --recursive Step 2: CloudFront Configuration Create CloudFront Distribution Using AWS Console:\nNavigate to CloudFront\nClick \u0026ldquo;Create Distribution\u0026rdquo;\nConfigure origin:\nOrigin domain: insighthr-web-app-sg.s3.ap-southeast-1.amazonaws.com Origin path: (leave empty) Name: S3-insighthr-web-app Default cache behavior:\nViewer protocol policy: Redirect HTTP to HTTPS Allowed HTTP methods: GET, HEAD, OPTIONS Cache policy: CachingOptimized Settings:\nPrice class: Use all edge locations Alternate domain names (CNAMEs): insight-hr.io.vn, www.insight-hr.io.vn Custom SSL certificate: Request or import certificate Default root object: index.html Click \u0026ldquo;Create Distribution\u0026rdquo;\nUsing AWS CLI:\n# Create distribution configuration cat \u0026gt; cloudfront-config.json \u0026lt;\u0026lt; EOF { \u0026#34;CallerReference\u0026#34;: \u0026#34;insighthr-$(date +%s)\u0026#34;, \u0026#34;Comment\u0026#34;: \u0026#34;InsightHR CloudFront Distribution\u0026#34;, \u0026#34;DefaultRootObject\u0026#34;: \u0026#34;index.html\u0026#34;, \u0026#34;Origins\u0026#34;: { \u0026#34;Quantity\u0026#34;: 1, \u0026#34;Items\u0026#34;: [ { \u0026#34;Id\u0026#34;: \u0026#34;S3-insighthr-web-app\u0026#34;, \u0026#34;DomainName\u0026#34;: \u0026#34;insighthr-web-app-sg.s3.ap-southeast-1.amazonaws.com\u0026#34;, \u0026#34;S3OriginConfig\u0026#34;: { \u0026#34;OriginAccessIdentity\u0026#34;: \u0026#34;\u0026#34; } } ] }, \u0026#34;DefaultCacheBehavior\u0026#34;: { \u0026#34;TargetOriginId\u0026#34;: \u0026#34;S3-insighthr-web-app\u0026#34;, \u0026#34;ViewerProtocolPolicy\u0026#34;: \u0026#34;redirect-to-https\u0026#34;, \u0026#34;AllowedMethods\u0026#34;: { \u0026#34;Quantity\u0026#34;: 3, \u0026#34;Items\u0026#34;: [\u0026#34;GET\u0026#34;, \u0026#34;HEAD\u0026#34;, \u0026#34;OPTIONS\u0026#34;] }, \u0026#34;ForwardedValues\u0026#34;: { \u0026#34;QueryString\u0026#34;: false, \u0026#34;Cookies\u0026#34;: {\u0026#34;Forward\u0026#34;: \u0026#34;none\u0026#34;} }, \u0026#34;MinTTL\u0026#34;: 0, \u0026#34;DefaultTTL\u0026#34;: 86400, \u0026#34;MaxTTL\u0026#34;: 31536000 }, \u0026#34;Enabled\u0026#34;: true } EOF # Create distribution aws cloudfront create-distribution \\ --distribution-config file://cloudfront-config.json Configure Custom Error Pages Set up error pages for SPA routing:\nGo to CloudFront distribution\nNavigate to \u0026ldquo;Error Pages\u0026rdquo; tab\nCreate custom error response:\nHTTP error code: 403 Customize error response: Yes Response page path: /index.html HTTP response code: 200 Repeat for error code 404\nStep 3: Lambda Deployment Package Lambda Functions For each Lambda function, create a deployment package:\nExample: Auth Login Handler\ncd lambda/auth # Create deployment package mkdir -p package pip install -r requirements.txt -t package/ cd package zip -r ../auth-login-handler.zip . cd .. zip -g auth-login-handler.zip auth_login_handler.py # Deploy to Lambda aws lambda create-function \\ --function-name insighthr-auth-login-handler \\ --runtime python3.11 \\ --role arn:aws:iam::ACCOUNT_ID:role/insighthr-lambda-role \\ --handler auth_login_handler.lambda_handler \\ --zip-file fileb://auth-login-handler.zip \\ --timeout 30 \\ --memory-size 256 \\ --environment Variables=\u0026#34;{ USER_POOL_ID=ap-southeast-1_rzDtdAhvp, CLIENT_ID=6suhk5huhe40o6iuqgsnmuucj5, DYNAMODB_USERS_TABLE=insighthr-users-dev }\u0026#34; \\ --region ap-southeast-1 Deploy All Lambda Functions Create a deployment script:\ndeploy-all-lambdas.sh:\n#!/bin/bash FUNCTIONS=( \u0026#34;auth-login-handler\u0026#34; \u0026#34;auth-register-handler\u0026#34; \u0026#34;auth-google-handler\u0026#34; \u0026#34;employees-handler\u0026#34; \u0026#34;employees-bulk-handler\u0026#34; \u0026#34;performance-scores-handler\u0026#34; \u0026#34;chatbot-handler\u0026#34; \u0026#34;attendance-handler\u0026#34; ) for func in \u0026#34;${FUNCTIONS[@]}\u0026#34;; do echo \u0026#34;Deploying $func...\u0026#34; # Package and deploy logic here done Step 4: API Gateway Configuration Create REST API # Create API aws apigateway create-rest-api \\ --name \u0026#34;InsightHR API\u0026#34; \\ --description \u0026#34;InsightHR Backend API\u0026#34; \\ --region ap-southeast-1 Create Resources and Methods Example: Create /employees endpoint\n# Get API ID API_ID=$(aws apigateway get-rest-apis \\ --query \u0026#34;items[?name==\u0026#39;InsightHR API\u0026#39;].id\u0026#34; \\ --output text) # Get root resource ID ROOT_ID=$(aws apigateway get-resources \\ --rest-api-id $API_ID \\ --query \u0026#34;items[?path==\u0026#39;/\u0026#39;].id\u0026#34; \\ --output text) # Create /employees resource EMPLOYEES_ID=$(aws apigateway create-resource \\ --rest-api-id $API_ID \\ --parent-id $ROOT_ID \\ --path-part employees \\ --query \u0026#39;id\u0026#39; \\ --output text) # Create GET method aws apigateway put-method \\ --rest-api-id $API_ID \\ --resource-id $EMPLOYEES_ID \\ --http-method GET \\ --authorization-type COGNITO_USER_POOLS \\ --authorizer-id $AUTHORIZER_ID # Integrate with Lambda aws apigateway put-integration \\ --rest-api-id $API_ID \\ --resource-id $EMPLOYEES_ID \\ --http-method GET \\ --type AWS_PROXY \\ --integration-http-method POST \\ --uri arn:aws:apigateway:ap-southeast-1:lambda:path/2015-03-31/functions/arn:aws:lambda:ap-southeast-1:ACCOUNT_ID:function:insighthr-employees-handler/invocations Deploy API # Create deployment aws apigateway create-deployment \\ --rest-api-id $API_ID \\ --stage-name dev \\ --description \u0026#34;Initial deployment\u0026#34; # Get API endpoint echo \u0026#34;API Endpoint: https://$API_ID.execute-api.ap-southeast-1.amazonaws.com/dev\u0026#34; Step 5: Environment Configuration Frontend Environment Variables Create .env.production file:\nVITE_API_BASE_URL=https://lqk4t6qzag.execute-api.ap-southeast-1.amazonaws.com/dev VITE_GOOGLE_CLIENT_ID=your-google-client-id VITE_AWS_REGION=ap-southeast-1 VITE_COGNITO_USER_POOL_ID=ap-southeast-1_rzDtdAhvp VITE_COGNITO_CLIENT_ID=6suhk5huhe40o6iuqgsnmuucj5 Rebuild and redeploy frontend:\nnpm run build aws s3 sync dist/ s3://insighthr-web-app-sg --delete Lambda Environment Variables Update Lambda functions with environment variables:\naws lambda update-function-configuration \\ --function-name insighthr-employees-handler \\ --environment Variables=\u0026#34;{ AWS_REGION=ap-southeast-1, EMPLOYEES_TABLE=insighthr-employees-dev, USERS_TABLE=insighthr-users-dev }\u0026#34; Step 6: CloudFront Cache Invalidation After deploying frontend changes, invalidate CloudFront cache:\n# Get distribution ID DIST_ID=$(aws cloudfront list-distributions \\ --query \u0026#34;DistributionList.Items[?Comment==\u0026#39;InsightHR CloudFront Distribution\u0026#39;].Id\u0026#34; \\ --output text) # Create invalidation aws cloudfront create-invalidation \\ --distribution-id $DIST_ID \\ --paths \u0026#34;/*\u0026#34; Step 7: DNS Configuration (Optional) If using a custom domain:\nRequest SSL Certificate # Request certificate in us-east-1 (required for CloudFront) aws acm request-certificate \\ --domain-name insight-hr.io.vn \\ --subject-alternative-names www.insight-hr.io.vn \\ --validation-method DNS \\ --region us-east-1 Configure Route53 # Create hosted zone aws route53 create-hosted-zone \\ --name insight-hr.io.vn \\ --caller-reference $(date +%s) # Create A record for CloudFront cat \u0026gt; change-batch.json \u0026lt;\u0026lt; EOF { \u0026#34;Changes\u0026#34;: [{ \u0026#34;Action\u0026#34;: \u0026#34;CREATE\u0026#34;, \u0026#34;ResourceRecordSet\u0026#34;: { \u0026#34;Name\u0026#34;: \u0026#34;insight-hr.io.vn\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;AliasTarget\u0026#34;: { \u0026#34;HostedZoneId\u0026#34;: \u0026#34;Z2FDTNDATAQYW2\u0026#34;, \u0026#34;DNSName\u0026#34;: \u0026#34;d2z6tht6rq32uy.cloudfront.net\u0026#34;, \u0026#34;EvaluateTargetHealth\u0026#34;: false } } }] } EOF aws route53 change-resource-record-sets \\ --hosted-zone-id ZONE_ID \\ --change-batch file://change-batch.json Deployment Checklist Before going live, verify:\nAll Lambda functions deployed and tested API Gateway endpoints configured DynamoDB tables populated with initial data Cognito User Pool configured Frontend built and deployed to S3 CloudFront distribution active SSL certificate validated (if using custom domain) DNS records configured (if using custom domain) Environment variables set correctly CORS configured on API Gateway CloudWatch logs enabled IAM roles and permissions verified Testing Deployment Test Frontend # Access CloudFront URL curl -I https://d2z6tht6rq32uy.cloudfront.net # Or custom domain curl -I https://insight-hr.io.vn Test API Endpoints # Test health check curl https://lqk4t6qzag.execute-api.ap-southeast-1.amazonaws.com/dev/health # Test authenticated endpoint (with token) curl -H \u0026#34;Authorization: Bearer YOUR_JWT_TOKEN\u0026#34; \\ https://lqk4t6qzag.execute-api.ap-southeast-1.amazonaws.com/dev/employees Continuous Deployment For automated deployments, consider:\nGitHub Actions: Automate build and deploy on push AWS CodePipeline: Full CI/CD pipeline AWS Amplify: Simplified frontend deployment Troubleshooting Frontend not loading:\nCheck S3 bucket policy Verify CloudFront distribution status Check browser console for errors API errors:\nVerify Lambda function logs in CloudWatch Check API Gateway configuration Verify Cognito authorizer setup CORS issues:\nConfigure CORS on API Gateway Check allowed origins match frontend domain Next Steps With deployment complete, proceed to Testing \u0026amp; Monitoring to set up CloudWatch monitoring and synthetic canaries.\n"
},
{
	"uri": "https://asapphat.github.io/tanphat.worklog.github.io/1-worklog/1.9-week9/",
	"title": "Week 9 Worklog",
	"tags": [],
	"description": "",
	"content": "Week Duration: November 3 - 9, 2025\nWeek 9 Objectives: Learn advanced monitoring with CloudWatch and Grafana Master CloudFront CDN for content delivery Understand resource tagging and IAM access control Practice AWS Systems Manager and operational excellence Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Monday - Advanced Monitoring with CloudWatch and Grafana - Proposal work 11/03/2025 11/03/2025 https://000029.awsstudygroup.com/vi/ Tuesday - CloudFront with S3 - Manage Access to EC2 Services with Resource Tags through IAM Services 11/04/2025 11/04/2025 https://000094.awsstudygroup.com/vi/ https://000048.awsstudygroup.com/vi/ Wednesday - Manage Patches and Run Commands on Multiple Servers with AWS System Manager - Work with Amazon System Manager - Session Manager 11/05/2025 11/05/2025 https://000031.awsstudygroup.com/vi/ https://000058.awsstudygroup.com/vi/ Thursday - Choose the Correct EC2 Size - Monitor Network Infrastructure - Automatically Archive Amazon EBS Snapshots with Amazon Data Lifecycle Manager 11/06/2025 11/06/2025 https://000032.awsstudygroup.com/vi/ https://000074.awsstudygroup.com/vi/ https://000088.awsstudygroup.com/vi/ Friday - Finalize proposal preparation 11/07/2025 11/07/2025 AWS Documentation Saturday - Week 9 summary 11/08/2025 11/08/2025 https://cloudjourney.awsstudygroup.com/ Week 9 Achievements: Mastered Advanced CloudWatch and Grafana Monitoring:\nConfigured custom CloudWatch dashboards with metrics and alarms Implemented CloudWatch Logs Insights for log analysis and querying Set up metric aggregation across multiple services Configured anomaly detection and predictive alarms Integrated Grafana with CloudWatch data sources Created Grafana dashboards for visualization and alerting Practiced advanced monitoring patterns and best practices Configured cross-account monitoring architecture Implemented CloudFront CDN for Content Distribution:\nCreated CloudFront distributions with S3 origins Configured origin access identity (OAI) for secure S3 access Set up multiple origins with failover behavior Implemented cache behaviors and TTL optimization Configured request/response custom headers Implemented geo-restriction and geo-based routing Practiced invalidation strategies and cache busting Optimized CloudFront for performance and cost Mastered Resource Tagging and IAM Access Control:\nImplemented comprehensive resource tagging strategy Created tag-based access control policies in IAM Set up tag compliance and cost allocation tags Used tags for resource organization and management Implemented attribute-based access control (ABAC) Enforced tag-based resource policies Practiced filtering resources by tags in AWS Management Console Reviewed tag governance and enforcement Practiced AWS Systems Manager for Operational Excellence:\nConfigured Systems Manager Agent on EC2 instances Set up Systems Manager Document (SSM Document) library Implemented automated patch management Used Patch Manager for compliance scanning and patching Practiced run commands across multiple instances Used Automation documents for complex operational tasks Set up maintenance windows for scheduled operations Reviewed Systems Manager best practices Mastered AWS Systems Manager Session Manager:\nConfigured Session Manager for secure shell access Set up IAM permissions for Session Manager usage Practiced session logging and session history auditing Implemented KMS encryption for session data Used Session Manager as alternative to SSH/RDP Configured session preferences and connection settings Practiced session sharing and collaboration Understood security benefits of Session Manager Optimized EC2 Instance Sizing and Performance:\nAnalyzed EC2 instance performance metrics Used AWS Compute Optimizer for sizing recommendations Right-sized instances based on utilization patterns Evaluated burstable vs. dedicated performance Reviewed instance family types and use cases Practiced performance baseline establishment Implemented monitoring for capacity planning Calculated cost savings from right-sizing Managed Network Infrastructure and Monitoring:\nConfigured VPC Flow Logs for network traffic analysis Set up CloudWatch Network Insights for connectivity troubleshooting Monitored elastic network interface (ENI) metrics Analyzed network throughput and latency patterns Implemented network topology analysis Practiced identifying network bottlenecks Configured network monitoring alarms Reviewed network performance optimization techniques Automated EBS Snapshot Management with Data Lifecycle Manager:\nConfigured Data Lifecycle Manager (DLM) policies Set up automated snapshot creation schedules Implemented snapshot retention policies Configured cross-region snapshot copies Set up automated snapshot tagging Monitored DLM policy execution Practiced snapshot lifecycle automation Calculated storage costs for snapshot retention Prepared Project Proposal with Technical Details:\nRefined project scope and architecture based on learnings Incorporated advanced monitoring strategies Documented resource tagging scheme Included Systems Manager automation in proposal Updated infrastructure sizing recommendations Added operational excellence practices Prepared implementation timeline and milestones Validated technical feasibility of proposed solution "
},
{
	"uri": "https://asapphat.github.io/tanphat.worklog.github.io/5-workshop/5-workshop/5.10-testing/",
	"title": "Testing &amp; Monitoring",
	"tags": [],
	"description": "",
	"content": "Testing \u0026amp; Monitoring This section covers setting up comprehensive testing and monitoring for the InsightHR platform.\nOverview We\u0026rsquo;ll implement:\nCloudWatch Logs - Centralized logging CloudWatch Metrics - Performance monitoring CloudWatch Alarms - Automated alerting CloudWatch Synthetics - Automated testing X-Ray Tracing - Distributed tracing (optional) CloudWatch Logs All Lambda functions automatically log to CloudWatch Logs.\nView Logs:\n# List log groups aws logs describe-log-groups \\ --log-group-name-prefix \u0026#34;/aws/lambda/insighthr\u0026#34; \\ --region ap-southeast-1 # Tail logs for a function aws logs tail /aws/lambda/insighthr-auth-login-handler \\ --follow \\ --region ap-southeast-1 Log Insights Queries:\nFind errors:\nfields @timestamp, @message | filter @message like /ERROR/ | sort @timestamp desc | limit 20 Analyze response times:\nfields @timestamp, @duration | stats avg(@duration), max(@duration), min(@duration) CloudWatch Metrics Monitor key metrics:\nLambda Metrics:\nInvocations Duration Errors Throttles Concurrent executions DynamoDB Metrics:\nRead/Write capacity units Throttled requests System errors API Gateway Metrics:\nRequest count Latency 4XX/5XX errors View Metrics:\n# Lambda invocations aws cloudwatch get-metric-statistics \\ --namespace AWS/Lambda \\ --metric-name Invocations \\ --dimensions Name=FunctionName,Value=insighthr-auth-login-handler \\ --start-time 2025-12-09T00:00:00Z \\ --end-time 2025-12-09T23:59:59Z \\ --period 3600 \\ --statistics Sum \\ --region ap-southeast-1 CloudWatch Alarms Create alarms for critical metrics:\nHigh Error Rate Alarm:\naws cloudwatch put-metric-alarm \\ --alarm-name insighthr-high-error-rate \\ --alarm-description \u0026#34;Alert when Lambda error rate exceeds 5%\u0026#34; \\ --metric-name Errors \\ --namespace AWS/Lambda \\ --statistic Sum \\ --period 300 \\ --evaluation-periods 2 \\ --threshold 5 \\ --comparison-operator GreaterThanThreshold \\ --dimensions Name=FunctionName,Value=insighthr-auth-login-handler \\ --region ap-southeast-1 High Latency Alarm:\naws cloudwatch put-metric-alarm \\ --alarm-name insighthr-high-latency \\ --alarm-description \u0026#34;Alert when API latency exceeds 2 seconds\u0026#34; \\ --metric-name Latency \\ --namespace AWS/ApiGateway \\ --statistic Average \\ --period 300 \\ --evaluation-periods 2 \\ --threshold 2000 \\ --comparison-operator GreaterThanThreshold \\ --region ap-southeast-1 CloudWatch Synthetics Canaries Automated testing with synthetic canaries:\n1. Login Canary - Tests login flow 2. Dashboard Canary - Tests dashboard loading 3. Chatbot Canary - Tests chatbot queries 4. Autoscoring Canary - Tests performance calculations\nCreate Login Canary:\n// login-canary.js const synthetics = require(\u0026#39;Synthetics\u0026#39;); const log = require(\u0026#39;SyntheticsLogger\u0026#39;); const loginFlow = async function () { const page = await synthetics.getPage(); // Navigate to login page await page.goto(\u0026#39;https://insight-hr.io.vn/login\u0026#39;, { waitUntil: \u0026#39;domcontentloaded\u0026#39;, timeout: 30000 }); // Fill login form await page.type(\u0026#39;#email\u0026#39;, \u0026#39;test@example.com\u0026#39;); await page.type(\u0026#39;#password\u0026#39;, \u0026#39;Test123!\u0026#39;); // Click login button await Promise.all([ page.waitForNavigation(), page.click(\u0026#39;button[type=\u0026#34;submit\u0026#34;]\u0026#39;) ]); // Verify successful login await page.waitForSelector(\u0026#39;.dashboard\u0026#39;, { timeout: 10000 }); log.info(\u0026#39;Login flow completed successfully\u0026#39;); }; exports.handler = async () =\u0026gt; { return await synthetics.executeStep(\u0026#39;LoginFlow\u0026#39;, loginFlow); }; Deploy Canary:\n# Create S3 bucket for canary artifacts aws s3 mb s3://insighthr-canary-artifacts --region ap-southeast-1 # Create canary aws synthetics create-canary \\ --name insighthr-login-canary \\ --artifact-s3-location s3://insighthr-canary-artifacts \\ --execution-role-arn arn:aws:iam::${ACCOUNT_ID}:role/CloudWatchSyntheticsRole \\ --schedule Expression=\u0026#34;rate(5 minutes)\u0026#34; \\ --runtime-version syn-nodejs-puppeteer-6.2 \\ --code file://login-canary.zip \\ --region ap-southeast-1 Dashboard Setup Create a CloudWatch Dashboard:\naws cloudwatch put-dashboard \\ --dashboard-name InsightHR-Dashboard \\ --dashboard-body file://dashboard-config.json \\ --region ap-southeast-1 dashboard-config.json:\n{ \u0026#34;widgets\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;metric\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;metrics\u0026#34;: [ [\u0026#34;AWS/Lambda\u0026#34;, \u0026#34;Invocations\u0026#34;, {\u0026#34;stat\u0026#34;: \u0026#34;Sum\u0026#34;}], [\u0026#34;.\u0026#34;, \u0026#34;Errors\u0026#34;, {\u0026#34;stat\u0026#34;: \u0026#34;Sum\u0026#34;}], [\u0026#34;.\u0026#34;, \u0026#34;Duration\u0026#34;, {\u0026#34;stat\u0026#34;: \u0026#34;Average\u0026#34;}] ], \u0026#34;period\u0026#34;: 300, \u0026#34;stat\u0026#34;: \u0026#34;Average\u0026#34;, \u0026#34;region\u0026#34;: \u0026#34;ap-southeast-1\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;Lambda Metrics\u0026#34; } }, { \u0026#34;type\u0026#34;: \u0026#34;metric\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;metrics\u0026#34;: [ [\u0026#34;AWS/ApiGateway\u0026#34;, \u0026#34;Count\u0026#34;, {\u0026#34;stat\u0026#34;: \u0026#34;Sum\u0026#34;}], [\u0026#34;.\u0026#34;, \u0026#34;4XXError\u0026#34;, {\u0026#34;stat\u0026#34;: \u0026#34;Sum\u0026#34;}], [\u0026#34;.\u0026#34;, \u0026#34;5XXError\u0026#34;, {\u0026#34;stat\u0026#34;: \u0026#34;Sum\u0026#34;}] ], \u0026#34;period\u0026#34;: 300, \u0026#34;stat\u0026#34;: \u0026#34;Sum\u0026#34;, \u0026#34;region\u0026#34;: \u0026#34;ap-southeast-1\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;API Gateway Metrics\u0026#34; } } ] } Testing Strategy Unit Tests:\nTest individual Lambda functions Mock DynamoDB and Cognito calls Verify business logic Integration Tests:\nTest API endpoints end-to-end Verify data flow through services Test authentication and authorization Load Tests:\nUse Artillery or k6 for load testing Test concurrent user scenarios Identify performance bottlenecks Example Load Test:\n# load-test.yml config: target: \u0026#39;https://your-api-id.execute-api.ap-southeast-1.amazonaws.com/dev\u0026#39; phases: - duration: 60 arrivalRate: 10 scenarios: - name: \u0026#34;Login and fetch employees\u0026#34; flow: - post: url: \u0026#34;/auth/login\u0026#34; json: email: \u0026#34;test@example.com\u0026#34; password: \u0026#34;Test123!\u0026#34; - get: url: \u0026#34;/employees\u0026#34; Best Practices âœ… Structured Logging: Use JSON format for logs âœ… Correlation IDs: Track requests across services âœ… Error Tracking: Log errors with context âœ… Performance Monitoring: Track response times âœ… Automated Testing: Run canaries regularly âœ… Alerting: Set up alarms for critical metrics âœ… Cost Monitoring: Track AWS costs\nTroubleshooting High Error Rates:\nCheck CloudWatch Logs for error details Verify IAM permissions Check DynamoDB capacity Review recent code changes High Latency:\nOptimize DynamoDB queries Add caching where appropriate Review Lambda memory allocation Check cold start times Failed Canaries:\nReview canary logs Check application availability Verify test credentials Update canary scripts if UI changed Monitoring Checklist CloudWatch Logs configured for all Lambda functions Key metrics tracked (invocations, errors, duration) Alarms set up for critical thresholds Synthetics canaries deployed and running Dashboard created for visualization Log retention policies configured Cost alerts configured On-call rotation established (for production) Next Steps With monitoring in place, proceed to Cleanup when you\u0026rsquo;re done with the workshop to avoid ongoing charges.\n"
},
{
	"uri": "https://asapphat.github.io/tanphat.worklog.github.io/1-worklog/1.10-week10/",
	"title": "Week 10 Worklog",
	"tags": [],
	"description": "",
	"content": "Week Duration: November 10 - 16, 2025\nWeek 10 Objectives: Learn AWS Lambda with Python programming Create IAM roles and permissions for project Build serverless dashboard functions Practice function development and testing Explore AI/ML on AWS Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Monday - Divide project work among team members - Learn AWS Lambda with Python - Create IAM Roles for project - Update proposal document - Code dashboard functions 11/10/2025 11/10/2025 AWS Documentation Tuesday - Continue learning AWS Lambda with Python - Review and fix proposal document 11/11/2025 11/11/2025 AWS Documentation Wednesday - Code and demo dashboard functions - Collect feedback on functionality 11/12/2025 11/12/2025 Project Repository Thursday - Fix dashboard issues - Brainstorm UI design templates - Collect function feedback 11/13/2025 11/13/2025 Design Templates Friday - AWS Cloud Mastery Series #1: AI/ML Fundamentals 11/14/2025 11/14/2025 Workshop Link Saturday - Fix and deploy demo dashboard function - Week 10 summary 11/16/2025 11/16/2025 https://cloudjourney.awsstudygroup.com/ Week 10 Achievements: Mastered AWS Lambda with Python Programming:\nCreated Lambda functions in Python from scratch Understood Lambda handler function syntax and parameters Worked with Lambda execution environment and runtime Implemented Lambda layers for code reusability Practiced dependency management with pip in Lambda Worked with environment variables and secure secrets Implemented context object and request/response handling Practiced error handling and exception management Created IAM Roles and Policies for Project Security:\nDesigned fine-grained IAM permissions for Lambda functions Created service roles with appropriate permissions Configured trust relationships for service assumptions Implemented least privilege access principles Created policies for specific AWS services (S3, DynamoDB, CloudWatch) Tested IAM permissions and debugged permission issues Documented role structure and permission requirements Reviewed security best practices for IAM Developed Serverless Dashboard Functions:\nCreated Lambda functions to retrieve and aggregate metrics Implemented data transformation logic in Python Connected Lambda to DynamoDB for data storage Integrated CloudWatch metrics collection Implemented correlation and root cause analysis logic Created alerting and notification functions Implemented dashboard data aggregation and formatting Tested function behavior under various scenarios Designed and Implemented Alert Management System:\nDeveloped alert fatigue management strategies Created alert correlation functions Implemented alert deduplication logic Configured alert routing and grouping Created alert threshold management Implemented alert history tracking Designed alert notification templates Practiced alert lifecycle management Divided Project Work and Planned Team Workflow:\nIdentified project components and deliverables Assigned tasks to team members based on skills Created project timeline and milestones Established collaboration workflow and communication Set up code review and testing procedures Documented project requirements and specifications Created version control and branching strategy Established deployment and release procedures Updated Project Proposal with Technical Specifications:\nDocumented architecture design and component interactions Specified Lambda function requirements and interfaces Defined IAM role structures and permissions Detailed data models and storage strategies Specified API endpoints and integration points Documented monitoring and logging requirements Added cost estimation and optimization strategies Included deployment and operations procedures Designed UI/UX Templates and Brainstormed Design Solutions:\nCreated wireframes for dashboard interface Designed responsive layouts for different device sizes Implemented color schemes and branding guidelines Created visualization templates for metrics Designed alert and notification interfaces Implemented user navigation and interaction patterns Practiced usability testing and feedback incorporation Documented UI/UX design decisions and rationale Participated in AWS Cloud Mastery Series: AI/ML Fundamentals Workshop:\nLearned AI/ML fundamentals on AWS Explored various AWS AI/ML services Understood machine learning workflow and best practices Practiced with AI/ML service implementations Networked with other AWS learners and professionals Gathered insights for potential project enhancements Updated knowledge of cutting-edge AWS AI/ML capabilities Deployed and Tested Dashboard Functions:\nPackaged and deployed Lambda functions Configured function triggers and event sources Tested function execution and output formatting Implemented CI/CD pipeline for function deployment Performed load testing and performance optimization Implemented function monitoring and logging Created test cases and automated testing Validated end-to-end dashboard functionality "
},
{
	"uri": "https://asapphat.github.io/tanphat.worklog.github.io/5-workshop/5-workshop/5.11-cleanup/",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "Cleaning Up Resources To avoid ongoing charges, it\u0026rsquo;s important to delete all AWS resources created during this workshop. This section provides step-by-step instructions for cleaning up.\nImportant: Deleting resources is irreversible. Make sure you\u0026rsquo;ve backed up any data you want to keep before proceeding.\nCleanup Overview We\u0026rsquo;ll delete resources in the following order:\nCloudFront Distribution S3 Buckets API Gateway Lambda Functions DynamoDB Tables Cognito User Pool CloudWatch Logs IAM Roles and Policies Route53 (if configured) ACM Certificates (if created) Step 1: Delete CloudFront Distribution CloudFront distributions must be disabled before deletion.\nUsing AWS Console:\nNavigate to CloudFront Select the InsightHR distribution Click \u0026ldquo;Disable\u0026rdquo; Wait for status to change to \u0026ldquo;Disabled\u0026rdquo; (may take 15-20 minutes) Select the distribution again Click \u0026ldquo;Delete\u0026rdquo; Using AWS CLI:\n# Get distribution ID DIST_ID=$(aws cloudfront list-distributions \\ --query \u0026#34;DistributionList.Items[?Comment==\u0026#39;InsightHR CloudFront Distribution\u0026#39;].Id\u0026#34; \\ --output text) # Get ETag ETAG=$(aws cloudfront get-distribution --id $DIST_ID \\ --query \u0026#39;ETag\u0026#39; --output text) # Disable distribution aws cloudfront get-distribution-config --id $DIST_ID \u0026gt; dist-config.json # Edit dist-config.json: Set \u0026#34;Enabled\u0026#34;: false aws cloudfront update-distribution \\ --id $DIST_ID \\ --if-match $ETAG \\ --distribution-config file://dist-config.json # Wait for distribution to be disabled aws cloudfront wait distribution-deployed --id $DIST_ID # Delete distribution ETAG=$(aws cloudfront get-distribution --id $DIST_ID \\ --query \u0026#39;ETag\u0026#39; --output text) aws cloudfront delete-distribution --id $DIST_ID --if-match $ETAG Step 2: Delete S3 Buckets Empty and delete the web app bucket:\n# Empty bucket aws s3 rm s3://insighthr-web-app-sg --recursive # Delete bucket aws s3 rb s3://insighthr-web-app-sg --force Delete canary artifacts bucket (if created):\naws s3 rm s3://insighthr-canary-artifacts --recursive aws s3 rb s3://insighthr-canary-artifacts --force Step 3: Delete API Gateway Using AWS Console:\nNavigate to API Gateway Select \u0026ldquo;InsightHR API\u0026rdquo; Click \u0026ldquo;Actions\u0026rdquo; â†’ \u0026ldquo;Delete API\u0026rdquo; Confirm deletion Using AWS CLI:\n# Get API ID API_ID=$(aws apigateway get-rest-apis \\ --query \u0026#34;items[?name==\u0026#39;InsightHR API\u0026#39;].id\u0026#34; \\ --output text \\ --region ap-southeast-1) # Delete API aws apigateway delete-rest-api \\ --rest-api-id $API_ID \\ --region ap-southeast-1 Step 4: Delete Lambda Functions Delete all Lambda functions:\n# List of functions to delete FUNCTIONS=( \u0026#34;insighthr-auth-login-handler\u0026#34; \u0026#34;insighthr-auth-register-handler\u0026#34; \u0026#34;insighthr-auth-google-handler\u0026#34; \u0026#34;insighthr-employees-handler\u0026#34; \u0026#34;insighthr-employees-bulk-handler\u0026#34; \u0026#34;insighthr-performance-scores-handler\u0026#34; \u0026#34;insighthr-chatbot-handler\u0026#34; \u0026#34;insighthr-attendance-handler\u0026#34; ) # Delete each function for func in \u0026#34;${FUNCTIONS[@]}\u0026#34;; do echo \u0026#34;Deleting $func...\u0026#34; aws lambda delete-function \\ --function-name $func \\ --region ap-southeast-1 done Step 5: Delete DynamoDB Tables Using AWS Console:\nNavigate to DynamoDB Select each table Click \u0026ldquo;Delete\u0026rdquo; Confirm by typing \u0026ldquo;delete\u0026rdquo; Using AWS CLI:\n# List of tables to delete TABLES=( \u0026#34;insighthr-users-dev\u0026#34; \u0026#34;insighthr-employees-dev\u0026#34; \u0026#34;insighthr-performance-scores-dev\u0026#34; \u0026#34;insighthr-attendance-history-dev\u0026#34; \u0026#34;insighthr-password-reset-requests-dev\u0026#34; \u0026#34;insighthr-kpis-dev\u0026#34; \u0026#34;insighthr-formulas-dev\u0026#34; \u0026#34;insighthr-data-tables-dev\u0026#34; \u0026#34;insighthr-notification-rules-dev\u0026#34; ) # Delete each table for table in \u0026#34;${TABLES[@]}\u0026#34;; do echo \u0026#34;Deleting $table...\u0026#34; aws dynamodb delete-table \\ --table-name $table \\ --region ap-southeast-1 done Step 6: Delete Cognito User Pool Using AWS Console:\nNavigate to Cognito Select \u0026ldquo;User Pools\u0026rdquo; Select the InsightHR user pool Click \u0026ldquo;Delete user pool\u0026rdquo; Type the pool name to confirm Using AWS CLI:\n# Get user pool ID USER_POOL_ID=$(aws cognito-idp list-user-pools \\ --max-results 10 \\ --query \u0026#34;UserPools[?Name==\u0026#39;insighthr-user-pool\u0026#39;].Id\u0026#34; \\ --output text \\ --region ap-southeast-1) # Delete user pool aws cognito-idp delete-user-pool \\ --user-pool-id $USER_POOL_ID \\ --region ap-southeast-1 Step 7: Delete CloudWatch Logs Delete log groups:\n# List log groups aws logs describe-log-groups \\ --log-group-name-prefix \u0026#34;/aws/lambda/insighthr\u0026#34; \\ --region ap-southeast-1 # Delete each log group LOG_GROUPS=$(aws logs describe-log-groups \\ --log-group-name-prefix \u0026#34;/aws/lambda/insighthr\u0026#34; \\ --query \u0026#39;logGroups[*].logGroupName\u0026#39; \\ --output text \\ --region ap-southeast-1) for log_group in $LOG_GROUPS; do echo \u0026#34;Deleting $log_group...\u0026#34; aws logs delete-log-group \\ --log-group-name $log_group \\ --region ap-southeast-1 done Delete CloudWatch Synthetics Canaries (if created):\n# List canaries aws synthetics describe-canaries \\ --region ap-southeast-1 # Delete each canary CANARIES=$(aws synthetics describe-canaries \\ --query \u0026#39;Canaries[?starts_with(Name, `insighthr`)].Name\u0026#39; \\ --output text \\ --region ap-southeast-1) for canary in $CANARIES; do echo \u0026#34;Deleting canary $canary...\u0026#34; aws synthetics delete-canary \\ --name $canary \\ --region ap-southeast-1 done Step 8: Delete IAM Roles and Policies Using AWS Console:\nNavigate to IAM Go to \u0026ldquo;Roles\u0026rdquo; Search for \u0026ldquo;insighthr\u0026rdquo; Select each role Detach policies Delete role Using AWS CLI:\n# List roles aws iam list-roles \\ --query \u0026#39;Roles[?starts_with(RoleName, `insighthr`)].RoleName\u0026#39; \\ --output text # For each role, detach policies and delete ROLES=$(aws iam list-roles \\ --query \u0026#39;Roles[?starts_with(RoleName, `insighthr`)].RoleName\u0026#39; \\ --output text) for role in $ROLES; do echo \u0026#34;Processing role $role...\u0026#34; # Detach managed policies POLICIES=$(aws iam list-attached-role-policies \\ --role-name $role \\ --query \u0026#39;AttachedPolicies[*].PolicyArn\u0026#39; \\ --output text) for policy in $POLICIES; do aws iam detach-role-policy \\ --role-name $role \\ --policy-arn $policy done # Delete inline policies INLINE_POLICIES=$(aws iam list-role-policies \\ --role-name $role \\ --query \u0026#39;PolicyNames[*]\u0026#39; \\ --output text) for policy in $INLINE_POLICIES; do aws iam delete-role-policy \\ --role-name $role \\ --policy-name $policy done # Delete role aws iam delete-role --role-name $role done Delete custom policies:\n# List custom policies aws iam list-policies \\ --scope Local \\ --query \u0026#39;Policies[?starts_with(PolicyName, `insighthr`)].Arn\u0026#39; \\ --output text # Delete each policy POLICIES=$(aws iam list-policies \\ --scope Local \\ --query \u0026#39;Policies[?starts_with(PolicyName, `insighthr`)].Arn\u0026#39; \\ --output text) for policy in $POLICIES; do echo \u0026#34;Deleting policy $policy...\u0026#34; # Delete all policy versions except default VERSIONS=$(aws iam list-policy-versions \\ --policy-arn $policy \\ --query \u0026#39;Versions[?!IsDefaultVersion].VersionId\u0026#39; \\ --output text) for version in $VERSIONS; do aws iam delete-policy-version \\ --policy-arn $policy \\ --version-id $version done # Delete policy aws iam delete-policy --policy-arn $policy done Step 9: Delete Route53 Resources (If Configured) Delete DNS records:\n# Get hosted zone ID ZONE_ID=$(aws route53 list-hosted-zones \\ --query \u0026#34;HostedZones[?Name==\u0026#39;insight-hr.io.vn.\u0026#39;].Id\u0026#34; \\ --output text) # List and delete records (except NS and SOA) aws route53 list-resource-record-sets \\ --hosted-zone-id $ZONE_ID # Delete A records for CloudFront cat \u0026gt; delete-records.json \u0026lt;\u0026lt; EOF { \u0026#34;Changes\u0026#34;: [{ \u0026#34;Action\u0026#34;: \u0026#34;DELETE\u0026#34;, \u0026#34;ResourceRecordSet\u0026#34;: { \u0026#34;Name\u0026#34;: \u0026#34;insight-hr.io.vn\u0026#34;, \u0026#34;Type\u0026#34;: \u0026#34;A\u0026#34;, \u0026#34;AliasTarget\u0026#34;: { \u0026#34;HostedZoneId\u0026#34;: \u0026#34;Z2FDTNDATAQYW2\u0026#34;, \u0026#34;DNSName\u0026#34;: \u0026#34;d2z6tht6rq32uy.cloudfront.net\u0026#34;, \u0026#34;EvaluateTargetHealth\u0026#34;: false } } }] } EOF aws route53 change-resource-record-sets \\ --hosted-zone-id $ZONE_ID \\ --change-batch file://delete-records.json # Delete hosted zone aws route53 delete-hosted-zone --id $ZONE_ID Step 10: Delete ACM Certificates (If Created) Using AWS Console:\nNavigate to Certificate Manager (in us-east-1 region) Select the certificate Click \u0026ldquo;Delete\u0026rdquo; Using AWS CLI:\n# List certificates aws acm list-certificates \\ --region us-east-1 # Delete certificate CERT_ARN=$(aws acm list-certificates \\ --query \u0026#34;CertificateSummaryList[?DomainName==\u0026#39;insight-hr.io.vn\u0026#39;].CertificateArn\u0026#34; \\ --output text \\ --region us-east-1) aws acm delete-certificate \\ --certificate-arn $CERT_ARN \\ --region us-east-1 Automated Cleanup Script Create a comprehensive cleanup script:\ncleanup-all.sh:\n#!/bin/bash set -e echo \u0026#34;Starting InsightHR cleanup...\u0026#34; # 1. Disable and delete CloudFront echo \u0026#34;Step 1: CloudFront...\u0026#34; # (Add CloudFront cleanup commands) # 2. Delete S3 buckets echo \u0026#34;Step 2: S3 Buckets...\u0026#34; aws s3 rm s3://insighthr-web-app-sg --recursive aws s3 rb s3://insighthr-web-app-sg --force # 3. Delete API Gateway echo \u0026#34;Step 3: API Gateway...\u0026#34; # (Add API Gateway cleanup commands) # 4. Delete Lambda functions echo \u0026#34;Step 4: Lambda Functions...\u0026#34; # (Add Lambda cleanup commands) # 5. Delete DynamoDB tables echo \u0026#34;Step 5: DynamoDB Tables...\u0026#34; # (Add DynamoDB cleanup commands) # 6. Delete Cognito echo \u0026#34;Step 6: Cognito User Pool...\u0026#34; # (Add Cognito cleanup commands) # 7. Delete CloudWatch logs echo \u0026#34;Step 7: CloudWatch Logs...\u0026#34; # (Add CloudWatch cleanup commands) # 8. Delete IAM roles echo \u0026#34;Step 8: IAM Roles and Policies...\u0026#34; # (Add IAM cleanup commands) echo \u0026#34;Cleanup complete!\u0026#34; Verification After cleanup, verify all resources are deleted:\n# Check S3 buckets aws s3 ls | grep insighthr # Check Lambda functions aws lambda list-functions \\ --query \u0026#39;Functions[?starts_with(FunctionName, `insighthr`)].FunctionName\u0026#39; \\ --region ap-southeast-1 # Check DynamoDB tables aws dynamodb list-tables \\ --query \u0026#39;TableNames[?starts_with(@, `insighthr`)]\u0026#39; \\ --region ap-southeast-1 # Check CloudFront distributions aws cloudfront list-distributions \\ --query \u0026#34;DistributionList.Items[?Comment==\u0026#39;InsightHR CloudFront Distribution\u0026#39;].Id\u0026#34; # Check API Gateway aws apigateway get-rest-apis \\ --query \u0026#34;items[?name==\u0026#39;InsightHR API\u0026#39;].id\u0026#34; \\ --region ap-southeast-1 Cost Verification After cleanup, monitor your AWS billing:\nGo to AWS Billing Dashboard Check \u0026ldquo;Bills\u0026rdquo; for current month Verify no ongoing charges for deleted services Check \u0026ldquo;Cost Explorer\u0026rdquo; for trends Cleanup Checklist CloudFront distribution disabled and deleted S3 buckets emptied and deleted API Gateway deleted All Lambda functions deleted All DynamoDB tables deleted Cognito User Pool deleted CloudWatch log groups deleted CloudWatch Synthetics canaries deleted IAM roles and policies deleted Route53 hosted zone deleted (if created) ACM certificates deleted (if created) Billing dashboard checked for ongoing charges Troubleshooting Cleanup CloudFront won\u0026rsquo;t delete:\nEnsure distribution is fully disabled Wait 15-20 minutes after disabling Check for associated resources S3 bucket won\u0026rsquo;t delete:\nEnsure bucket is completely empty Check for versioned objects Disable versioning before deleting IAM role won\u0026rsquo;t delete:\nDetach all managed policies first Delete all inline policies Check for service-linked roles DynamoDB table deletion fails:\nWait for table to be in ACTIVE state Check for ongoing operations Verify IAM permissions Final Notes Best Practice: Always clean up resources after completing a workshop to avoid unexpected charges. Set up billing alerts to notify you of any ongoing costs.\nCongratulations! You\u0026rsquo;ve successfully completed the InsightHR workshop and cleaned up all resources. You\u0026rsquo;ve learned:\nâœ… Serverless architecture design âœ… AWS Lambda and API Gateway âœ… DynamoDB data modeling âœ… Cognito authentication âœ… AWS Bedrock AI integration âœ… CloudFront CDN deployment âœ… Infrastructure as Code principles âœ… Cost optimization strategies Thank you for participating in this workshop!\n"
},
{
	"uri": "https://asapphat.github.io/tanphat.worklog.github.io/1-worklog/1.11-week11/",
	"title": "Week 11 Worklog",
	"tags": [],
	"description": "",
	"content": "Week Duration: November 17 - 23, 2025\nWeek 11 Objectives: Continue development of Lambda functions in Python Generate demo data for applications Learn Docker containerization Deploy Docker containers to ECR Build chatbot and manage absent employee functions Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Monday - AWS Cloud Mastery Series #1 11/17/2025 11/17/2025 Workshop Link Tuesday - Generate demo data for applications - Update Dashboard Function 11/18/2025 11/18/2025 Project Repository Wednesday - Update Lambda functions - Code manage absent employee function 11/19/2025 11/19/2025 Project Documentation Thursday - Docker tutorial and containerization - Demo and push Docker to ECR 11/20/2025 11/20/2025 https://youtu.be/pg19Z8LL06w?si=QyLqSkxfBPJ1ygbu Friday - Fix Docker deployment - Finalize Docker images and ECR push 11/21/2025 11/21/2025 AWS ECR Documentation Saturday - UI/UX feedback for dashboard - Week 11 summary 11/23/2025 11/23/2025 https://cloudjourney.awsstudygroup.com/ Week 11 Achievements: Continued Lambda Function Development in Python:\nEnhanced Lambda function implementations with additional features Implemented more complex business logic in Python Optimized function performance and resource utilization Added comprehensive error handling and logging Implemented function versioning and aliases Optimized Lambda deployment packages Practiced code organization and reusability Implemented monitoring and alerting for functions Generated Demo Data for Application Testing:\nCreated realistic demo datasets for testing Implemented data generation scripts in Python Generated sample metrics and time-series data Created test scenarios for function validation Seeded DynamoDB with demo data Generated alert scenarios for testing Populated sample user records Ensured data consistency and referential integrity Coded Manage Absent Employee Function:\nDesigned function architecture for employee absence management Implemented employee absence tracking logic Created functions to record absence events Built reporting functions for absence patterns Integrated with DynamoDB for data persistence Implemented notification logic for absent employees Created dashboard widgets for absence metrics Integrated with HR systems for data synchronization Mastered Docker Containerization:\nLearned Docker fundamentals and containerization concepts Created Dockerfiles for Lambda functions Understood Docker image layers and optimization Built Docker images with proper dependencies Practiced multi-stage builds for optimization Implemented Docker best practices for production Created .dockerignore files for efficiency Practiced local Docker testing before deployment Deployed Docker Containers to ECR (Elastic Container Registry):\nSet up ECR repositories for container storage Configured ECR access controls and permissions Tagged Docker images with appropriate versions Pushed Docker images to ECR repositories Set up image lifecycle policies Configured image scanning and vulnerability detection Implemented image retention and cleanup policies Practiced ECR repository management and organization Attended AWS Cloud Mastery Series: DevOps on AWS Workshop:\nLearned DevOps practices and automation Explored CI/CD pipelines on AWS Understood infrastructure as code (IaC) concepts Practiced automated testing and deployment Learned about deployment strategies Explored monitoring and logging best practices Networked with AWS DevOps professionals Gathered insights for project automation Updated Dashboard Functions with Enhanced Features:\nAdded new metrics and visualization capabilities Implemented real-time data updates Enhanced dashboard responsiveness Added filtering and aggregation features Implemented dashboard customization Added drill-down capabilities for detailed analysis Enhanced performance for large datasets Implemented dashboard caching strategies Collected UI/UX Feedback and Implemented Improvements:\nGathered feedback from stakeholders Evaluated user experience improvements Implemented design refinements Optimized user interface layout Improved accessibility features Enhanced visual design and branding Implemented responsive design for mobile devices Documented design decisions based on feedback Prepared for Integration and Testing Phases:\nValidated all components for integration readiness Prepared test scenarios and test cases Set up testing environments Documented integration procedures Created deployment scripts Established rollback procedures Prepared monitoring and alerting for production Conducted pre-deployment validation "
},
{
	"uri": "https://asapphat.github.io/tanphat.worklog.github.io/1-worklog/1.12-week12/",
	"title": "Week 12 Worklog",
	"tags": [],
	"description": "",
	"content": "Week Duration: November 24 - 30, 2025\nWeek 12 Objectives: Test and deploy Amazon Bedrock chatbot Complete chatbot with RAG research Build attendance and absent management functions Finalize all project components Prepare project deployment and presentation Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material Monday - Demo and Test Amazon Bedrock - Implement manage absent employee function 11/24/2025 11/24/2025 AWS Bedrock Documentation Tuesday - Code chatbot function - Fix auto_scoring_function - Create workflow for manage absent 11/25/2025 11/25/2025 Project Repository Wednesday - Configure and deploy chatbot to AWS Lambda - Configure API Gateway integration 11/26/2025 11/26/2025 AWS Deployment Guide Thursday - Fix chatbot query function - RAG research and implementation 11/27/2025 11/27/2025 AWS AI/ML Resources Friday - Finish chatbot - Remake attendance functions: check in/out and absent management 11/28/2025 11/28/2025 Project Documentation Saturday - Final testing and deployment - Project presentation preparation - Week 12 internship summary 11/29/2025 11/29/2025 https://cloudjourney.awsstudygroup.com/ Week 12 Achievements: Tested and Deployed Amazon Bedrock Chatbot:\nUnderstood Bedrock chatbot capabilities and models Created chatbot function architecture in Lambda Integrated Bedrock with Lambda for text generation Tested various conversation scenarios Optimized prompt engineering for better responses Implemented conversation history management Set up API Gateway for chatbot access Deployed chatbot to production environment Implemented Retrieval-Augmented Generation (RAG):\nStudied RAG architecture and implementation patterns Created knowledge base for chatbot Implemented document retrieval and ranking Integrated semantic search with Bedrock Optimized retrieval for relevant responses Implemented response generation with context Tested RAG effectiveness with various queries Documented RAG configuration and procedures Built Attendance Management System:\nDesigned attendance tracking functions Implemented check-in/check-out functionality Created attendance records in DynamoDB Built attendance reporting functions Implemented attendance validation logic Created dashboard for attendance metrics Set up automatic attendance alerts Integrated with employee management system Implemented Absent Employee Management Function:\nCreated absence recording and tracking logic Built absence validation and approval workflows Implemented absence notification system Created absence reports and analytics Set up absence compliance tracking Integrated with calendar and scheduling systems Implemented absence period management Created dashboard for absence metrics Fixed and Optimized Auto-Scoring Function:\nDebugged auto-scoring logic issues Optimized scoring algorithm performance Implemented proper error handling Fixed data input and output formatting Tested scoring with various datasets Improved scoring accuracy and reliability Documented scoring methodology Created automated testing for scoring function Configured API Gateway Integration:\nSet up API Gateway endpoints for chatbot Configured request/response models Implemented authentication and authorization Set up rate limiting and throttling Configured CORS for web applications Implemented API logging and monitoring Created API documentation Tested API endpoints end-to-end Completed Project Finalization and Deployment:\nPerformed comprehensive system testing Fixed and resolved all identified issues Optimized performance and resource usage Set up monitoring and alerting Created runbooks and operational procedures Established disaster recovery procedures Deployed all components to production Verified production functionality Prepared Project Presentation:\nDocumented project architecture and design Created presentation slides with visuals Prepared demonstrations of key features Documented project achievements and metrics Outlined project implementation timeline Highlighted lessons learned and insights Prepared for Q\u0026amp;A sessions Practiced presentation delivery Comprehensive AWS Learning Journey Completion:\nReviewed all core AWS services covered Synthesized learnings across 12 weeks Understood service integration patterns Mastered AWS architecture best practices Built end-to-end serverless solutions Integrated multiple AWS services effectively Prepared for AWS certification exams Reflected on internship growth and achievements Completed 12-Week FCAJ-AWS Internship:\nSuccessfully completed all learning objectives Built production-ready cloud solutions Mastered AWS core services and patterns Developed practical cloud engineering skills Collaborated with team on real project Achieved significant technical growth Prepared for professional cloud roles Documented comprehensive learning journey "
},
{
	"uri": "https://asapphat.github.io/tanphat.worklog.github.io/5-workshop/",
	"title": "5-Workshops",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://asapphat.github.io/tanphat.worklog.github.io/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://asapphat.github.io/tanphat.worklog.github.io/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]